%        File: Diplomarbeit.tex
%     Created: Fri Jan 02 04:00 PM 2015 C
  % Last Change: Sat May 02 08:00 PM 2015 C
%      Author: Eduard Zhu
%       Email: Kewell1203@gmail.com

\documentclass[a4paper, twoside, 11pt]{article}

%------------------------------%
\synctex=1
%----------- preamble ---------%
%----------- packages ---------%
\usepackage[body={15cm, 23cm}, top=4.5cm, left=4cm]{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage[perpage, symbol]{footmisc}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{bbm}

%----------- pagestyle setting ----------%
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\renewcommand{\headrulewidth}{.4pt}
\renewcommand{\footrulewidth}{.4pt}
\fancyhead[RO]{\leftmark}
\fancyhead[LE]{\rightmark}
\fancyfoot[LE, RO]{\large \thepage}

%---------- new commands ---------%
\theoremstyle{definition}
\newtheorem{definition}{\scshape Definition}[section]
\newtheorem{theorem}[definition]{\scshape Theorem}
\newtheorem{lemma}[definition]{\scshape Lemma}
\newtheorem{proposition}[definition]{\scshape Proposition}
\newtheorem{corollary}[definition]{\scshape Corollary}
\newtheorem{example}[definition]{\scshape Example}
\renewcommand{\proofname}{\upshape\bfseries Proof.}
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%---------- definitions of math -----------%
% R, N
\def\RR{$\mathcal{R}$}
\def\NN{$\mathcal{N}$}
\def\AA{$\mathscr{A}$\ }
% complement
\newcommand{\compl}[1]{{#1}^{c}}
% sigma algebra
\def\sa{$\sigma$- Algebra\ } 
% prob. space
\def\bs{$(\Omega, \mathscr{A}, \mathcal{P})$\ } 
\def\bsigma{\mathscr{B}\brkt{\mathbb{R}^{n}}}
\newcommand{\sqbr}[1]{\left[ {#1} \right]}
\newcommand{\brkt}[1]{\left({#1} \right)}

  %---------- global variables setting -----%
  \setlength{\parindent}{0em}
  \setlength{\parskip}{1.5ex plus .5ex minus .5ex}
  \renewcommand{\baselinestretch}{1.3}
  \setfnsymbol{wiley}
  \renewenvironment{abstract}{
	\begin{center}
		  \Large
		  \textbf{Abstract}
		  \hspace{2em}
	\end{center}				
  }{}

  %---------- beginning of document --------%
  \begin{document}

  %---------- title page -----------%
  \input{./title.tex}
  \newpage

  %---------- abstract -------------%
  \thispagestyle{empty}
  \begin{abstract}
  Fractional Brownian motion (fBm) $(U_H(t))_{t\in \mathbb{R}}$ has an integration representation defined by Mandelbrot and Van Ness\cite{mandelbrot}
\begin{eqnarray*}
 	U_H(t) - U_H(s) = \frac{1}{\Gamma(H+\frac{1}{2})}\brkt{\int_{\mathbb{R}} \mathbbm{1}_{\{t\ge u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{s\ge u\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}},
\end{eqnarray*}
where $(B_t)_{t\in\mathbb{R}}$ is two-sides Brownian motion.\\
One of applications of fBm is fractional Ornstein-Uhlenbeck process (fOU) 
\begin{eqnarray*}
  X_t = e^{-at}\brkt{\gamma\int_{-\infty}^t e^{au}\mathop{dU_H(u)}},
\end{eqnarray*}
which is as the stationary solution of the SDE with 
\begin{eqnarray*}
  \mathop{dX_t} = -aX_t\mathop{dt} + \gamma\mathop{dU_H(t)},
\end{eqnarray*}
where $a,\gamma \in \mathbb{R}_+$.\\
This thesis is devoted to the study of fBm, fOU and financial modelings, which are derived from fOU.

  \end{abstract}
\newpage

%---------- contents -------------%
\thispagestyle{empty}
\mbox{}
\newpage
\fancyhead[LO, RE]{}
\fancyfoot[LE, RO]{}
\tableofcontents
\newpage
\thispagestyle{empty}
\mbox{}
\newpage

%---------- 1. introduction -------%
\fancyhead[RO]{\leftmark}
\fancyhead[LE]{\rightmark}
\fancyfoot[LE, RO]{\large \thepage}
\setcounter{section}{0}
\setcounter{page}{1}
\section{Introduction}
The aim of this Diploma thesis is to study fractional Brownian motion (fBm). Unlike ordinary Brownian motion, fBm does not need to have independent increments. fBm was first introduced by Kolmogorov\cite{kolm} as a centered Gaussian process, which has covariance function as follows
\begin{eqnarray*}
  \mathrm{Cov}[U_H(t), U_H(s)] &=& \frac{1}{2} (|t|^{2H} + |s|^{2H} - |t-s|^{2H}),
\end{eqnarray*}
where $H$ is real number in $(0, 1)$. The successive pioneer work can be traced to Mandelbrot and Van Ness\cite{mandelbrot}. Where fBm $(U_H(t))_{t\in \mathbb{R}}$ has an integration representation
\begin{eqnarray*}
 	U_H(t) - U_H(s) = \frac{1}{\Gamma(H+\frac{1}{2})}\brkt{\int_{\mathbb{R}} \mathbbm{1}_{\{t\ge u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{s\ge u\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}},
	\label{sec:int}
\end{eqnarray*}
and has stationary self-similar increments satisfying for $t\in \mathbb{R}, \tau > 0$
%which could fulfil the preceeding covariance property.\\
%The main tasks of this thesis are to study FBM by its integration representation and to discuss its applications in financial mathematics. It is arranged as follows. In Section 2 we shall give some basic concepts of Gaussion processes and Brownian motion. To define the integral of (\ref{sec:int}), we introduce in Section 3 the stable integrals. In terms of (\ref{sec:int}), FBM could be a self-similar process and has stationary increments satisfying
\begin{eqnarray*}
  U_H(t+\tau) - U_H(t) \sim \kappa \tau^{H} 
  \label{sec:int1}
\end{eqnarray*}
with some $\kappa$. According to Theorem of Kolmogorov Chentsov, fBm must be a continuous-time process.\\

One of applications of fBm is fractional Ornstein-Uhlenbeck process (fOU) 
\begin{eqnarray*}
  X_t = e^{-at}\brkt{\gamma\int_{-\infty}^t e^{au}\mathop{dU_H(u)}}
\end{eqnarray*}
which is as the stationary solution of the SDE with 
\begin{eqnarray*}
  \mathop{dX_t} = -aX_t\mathop{dt} + \gamma\mathop{dU_H(t)}
\end{eqnarray*}
where $a,\gamma \in \mathbb{R}_+$. Although fBm is not a semimartingale for $H\neq \frac{1}{2}$ (Liptser \& Shiryayev\cite{liptshir}), Cheridito prove fOU as integral driven by $U_H(t)$ in sense of Riemann-Stieljes exists, i.e., for $a>0$,
\begin{eqnarray*}
  \int_{0}^t e^{au}\mathop{dU_H(u)}
\end{eqnarray*}
is well-defined. In addition, if $H\in(\frac{1}{2}, 1)$, fOU exhibits long memory property. More recently, fOU is more and more commonly applied in stochastic volatility models. In particular, Comte and Renault\cite{comren} assume the log-volatility to follow fOU with $H\in(\frac{1}{2}, 1)$. In contrast, Gatheral et al.\cite{Gatheral} take $H \in (0, \frac{1}{2})$.

The structure of this thesis is as follows. We provide in Section 2 basic concepts of probability theory and stochastic process involving Gaussian process which could be characterized by characteristic function. Brownian motion is discussed at the end of this section as an example. Section 3 focuses on stable integrals, which ensure that fBm can be defined as a Gaussian process. Section 4 presents definition of fBm in sense of stable integral, based on which we show self-similarity, stationary property of increments of fBm, regularity, and nonsemimartingale (except for $H=\frac{1}{2}$). From Section 5 up to the end we turn towards applications of FBM in financial mathematics. In Section 5, we deal with fOU, which exhibits long memory if $H > \frac{1}{2}$. In Section 6 we list out applications of FBM, not only for pricing models of risky asset (fractional Black-Scholes)  but also for stochastic volatility models. We complete the proof of Cheridito to show that if a minimal amount of time between two successive transactions exists, fractional Black-Scholes would be arbitragefree. In fractional stochastic volatility model, we use fOU to model log-volatility. It differs from the choise of $H$. Whilst $H>\frac{1}{2}$ (in FSV) can ensure long memory, on the other hand, if $H < \frac{1}{2}$ the model (RFSV) can generate more desirable volatility smoothness according to empirical data. In order to combine this two characteristics, we define weighted fractional stochatic volatility model which inherits long memory property from FSV and could have a very close result as in RFSV.

\newpage

%---------- 2. section ------------%
\section{Gaussian Processes and Brownian Motion}
\setcounter{equation}{0}
In this section we start off by looking at some general concepts of probability spaces and stochastic processes, in which the Gaussian process is  an important example. Within the framework of Gaussian processes, one could specify a stationary and independent behavior. This therefore leads us to introduction of the Brownian motion.

%---------- 2.1. subsection -------%
\subsection{Probability Spaces and Stochastic Processes }
\begin{definition}
  Let \AA be a collection of subsets of a set $\Omega$. \AA is said to be a \emph{$\sigma$- Algebra} on $\Omega$, if it satisfies the following conditions:
  \begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
	\item $\Omega \in $ \AA.
	\item For any set $F \in \mathscr{A}$, its complement $\compl{F} \in$ \AA.
	\item If there is a series $\{F_n\}_{n\in \mathbb{N}}$ such that $\{F_n\}_{n \in \mathbb{N}} \subseteq \mathscr{A}$, then $\cup_{n \in \mathbb{N}}F_n \in $ \AA.
  \end{enumerate}
\end{definition}

\begin{definition}
  A mapping $\mathcal{P}$ is said to be a \emph{probability measure} from $\mathscr{A}$ to $\bsigma$, if $\mathcal{P}\sqbr{\sum_{n=1}^{\infty} F_n} = \sum_{n=1}^{\infty} \mathcal{P}\sqbr{F_n}$ for any $\{F_n\}_{n \in \mathbb{N}}$ disjoint in $\mathscr{A}$ satisfying $\sum_{n=1}^{\infty}F_n \in \mathscr{A}$. 
\end{definition}

\begin{definition}
  A \emph{probability space} is defined as a triple \bs of a set $\Omega$, a \sa \AA  of $\Omega$ and a measure $\mathcal{P}$ from $\mathscr{A}$ to $\bsigma$.
\end{definition}

The $\sigma$- Algebra generated of all open sets on $\mathbb{R}^{n}$ is called the \emph{Borel $\sigma$- Algebra} which is denoted by $\mathscr{B}\left(\mathbb{R}^{n}\right)$. Let $\mu$ be a probability measure on $\mathbb{R}^{n}$. Indeed, $\brkt{\mathbb{R}^{n}, \mathscr{B}\brkt{\mathbb{R}^{n}}, \mu}$ is a special case that probability space on $\mathbb{R}^{n}$. A function $f$ mapping from $\brkt{\mathcal{D}, \mathscr{D}, \mu}$ into $\brkt{\mathcal{E}, \mathscr{E}, \nu}$ is \emph{measurable}, if its collection of the inverse image of $\mathscr{E}$ is a subset of $\mathscr{D}$. A \emph{random variable} is a $\mathbb{R}^{n}$-valued measurable function on some probability space. Let $\mathcal{P}$ represent a probability measure, recall that in probability theory, for $B \in \bsigma$ we call $\mathcal{P}\sqbr{\left\{X \in B\right\}}$ the \emph{distribution} of $X$. We write also $\mathcal{P}_X \sqbr{\cdot}$ or $\mathcal{P}\sqbr{X}$ for convenience for those notations .

\begin{definition}
  Let $\brkt{\Omega, \mathscr{A}, \mathcal{P}}$ be a probability space. A $n$-dimensional \emph{stochastic process} $\brkt{X_t}_{t}$ is a family of random variable such that $X_t\brkt{\omega} : \Omega \longrightarrow  \mathbb{R}^{n},  \forall t \in T$, where $T$ denotes the set of Index of Time.    
\end{definition}

Without specification, we set $T=\mathbb{R}$. Some basic definitions, which are needed in following sections, are given.
\begin{definition}
  A stochastic process $\brkt{X_t}_{t \in T}$ is said to be \emph{stationary}, if the joint \\distribution 
\[
  \mathcal{P}\sqbr{X_{t_1},\dots,X_{t_n}} = \mathcal{P}\sqbr{X_{t_1+\tau},\dots,X_{t_n+\tau}} 
\]
for $t_1, \dots, t_n$ and $t_1+\tau,\dots,t_n+\tau \in \mathbb{R}$. 
\label{sec:stn}
\end{definition}
\begin{definition}
  Let $(X_t)_t$ be a stochastic process. 
  \begin{equation*}
	\varsigma_X(t,s) := \mathrm{Cov}(X_t, X_s) 
  \end{equation*} is called \emph{autocovariance} between $s, t$ and 
  \begin{equation*}
	\eta_X(t, s) := \frac{\mathrm{Cov[X_t, X_s]}}{\sqrt{\mathrm{Var}[X_t]\mathrm{Var}[X_s]}}
  \end{equation*}
  is called \emph{autocorrelation} betwenn $s, t$.
\end{definition}

\begin{definition}
  A stochastic process $(X_t)_t$ is said to be \emph{weak  stationary} if 
  \begin{eqnarray*}
	\mathrm{E}[X_t] = \mathrm{E}[X_{t+\tau}]
  \end{eqnarray*}
  and 
  \begin{eqnarray*}
	\varsigma_{X}(t, s) = \varsigma_{X}(t-s, 0)
  \end{eqnarray*}
  for $\tau, s \in \mathbb{R}$.
\end{definition}
Remark that, weak stationarity is more general than stationarity. If $(X_t)_t$ is a weak stationary process, for any $t$, we write $\varsigma_X(\tau)$ for $\varsigma_X(t+\tau, t)$. $\eta_X(\tau)$ is used in the same way. 

%\begin{definition}
%  A stochastic process $(X_t)_t$ is said to be \emph{ergodic} if the moving average of $X_t$ over $T$ tend to infinity, in fact,
%  \begin{equation}
%	\frac{1}{T}\int_0^T\,X_t\,\mathop{dt} \longrightarrow \infty\nonumber\\.
%  \end{equation}
%	\label{sec:erg}
%\end{definition}

We use a notation $X \sim Y$ represents $X$ equals $Y$ \emph{in distribution}. 
\begin{definition}
  A stochastic process $(X_t)_{t}$ is said to be \emph{$\alpha$-self similar} if $(X_{ct_1},\dots,X_{ct_k}) \sim (c^\alpha X_{t_1},\dots, c^\alpha X_{t_k})$ for any $t_1,\dots, t_k \in \mathbb{R}$ and $c\in \mathbb{R}_{+}$.
\end{definition}

%---------- 2.2. subsection -------%
\subsection{Normal Distribution and Gaussian Process}
\begin{definition}[1-dimensional normal distribution]
  A $\mathbb{R}$-valued random variable $X$ is said to be \emph{standard normal distributed} or \emph{standard Gaussian}, if its distribution can be described as
  \begin{equation}
	\mathcal{P}\sqbr{X \le x} = (2\pi)^{-\frac{1}{2}}\int_{-\infty}^{x} e^{-\frac{u^2}{2}}\,\mathop{du}  
	\label{sec:21}
  \end{equation}
  for $x \in \mathbb{R}$.
\end{definition}
The integrand of (\ref{sec:21}) is also called \emph{density function} of a standard Gaussian random variable.

\begin{definition}
  A $\mathbb{R}$-valued random variable $X$ is said to be \emph{normally distributed} or \emph{Gaussian} with a \emph{expected value} $\mu$ and a \emph{variance} $\sigma^2$, if
\[
  (X-\mu) / \sigma
\]
is standard Gaussian for $\sigma>0$.
\end{definition}

\begin{proposition}
  Let $X$ be a $\mathbb{R}$-valued Gaussian random variable with expected value $\mu$ and variance $\sigma^2$, then it is distributed as
  \begin{equation*}
	\mathcal{P}[X\le x] = (2\pi\sigma^2)^{-\frac{1}{2}}\int_{-\infty}^x e^{-\frac{(u-\mu)^2}{2\sigma^2}}\mathop{du}
  \end{equation*}
\end{proposition}

\begin{proof}
  Suppose $X = \sigma Y + \mu$ with $Y$ standard Gaussian. We denote this mapping by $g(y) : y \rightarrow \sigma y + \mu$ and give the inverse $g^{-1}(x) : x \rightarrow \frac{(x-\mu)}{\sigma}$. The distribution function of $X$ is 
  \begin{eqnarray*}
	\int_\Omega \mathcal{P}[X \in dx] &=& \int_\Omega \mathcal{P}[Y \circ g \in dx] \\
	&=& \int_{\mathbb{R}\circ g} f_Y \circ g^{-1}(x) \mathop{dx}\\
	&=& \int_{\mathbb{R}} \sigma \frac{1}{\sqrt{2\pi}} \exp\{\frac{(\frac{(x-\mu)}{\sigma})^2}{2}\} \mathop{dy}\\
	&=&  (2\pi\sigma^2)^{-\frac{1}{2}}\int_{\mathbb{R}} \exp\{-\frac{(x-\mu)^2}{2\sigma^2}\}\mathop{dy} ,
  \end{eqnarray*}
 where $f_Y$ is density function of $Y$.
\end{proof}

It is denoted by $X \sim (2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}\mathop{dx} $, if $X$ is standard Gaussian. In order to verifying the behavior of a normal distributed random variable we use the characteristic function in probability theory, cf.\cite{bauer}. 

\begin{theorem}
  Let $X$ be a $\mathbb{R}$-valued Gaussian random variable with expected value $\mu$ and variance $\sigma^2$. The characteristic function of $X$
\begin{equation}
  \Psi_X(\xi) := \int_\mathbb{R} e^{ix\xi}\mathcal{P}\sqbr{X \in \mathop{dx}} = e^{i\mu\xi-\frac{1}{2}(\sigma\xi)^2}
  \label{sec:cht}
\end{equation}
for $\xi \in \mathbb{R}$.
\label{sec:char}
\end{theorem}
\begin{proof}
  Cf.\cite{shilling}. We assume firstly $Y$ is standard Gaussian. In terms of the Definion of characteristic function of a standard Gaussian $Y$, integrating its density function over $\mathbb{R}$ we get
  \begin{equation*}
	\Psi_Y(\xi) = \int_\mathbb{R} (2\pi)^{-\frac{1}{2}}e^{-\frac{y^2}{2}}e^{iy\xi}\,\mathop{dy},
  \end{equation*}
take differentiating both sides of the equation by $\xi$, then
\begin{eqnarray*}
\Psi_Y'(\xi) &=& \int_\mathbb{R}(2\pi)^{-\frac{1}{2}}e^{-\frac{y^2}{2}}e^{iy\xi}ix\,\mathop{dy}\\
             &=& (-i)\cdot\int_\mathbb{R} (2\pi)^{-\frac{1}{2}}(\frac{d}{dx}e^{-\frac{y^2}{2}})e^{iy\xi}\,\mathop{dy}\\
			 &\overset{part.int.}{=}& -\int_\mathbb{R}(2\pi)^{-\frac{1}{2}}e^{-\frac{y^2}{2}}e^{iy\xi}\xi\,\mathop{dy}\\
			 &=& -\xi\Psi_Y(\xi).
\end{eqnarray*}
for $\xi \in \mathbb{R}$. Obviously, 
$\Psi(\xi) = \Psi(0)e^{-\frac{\xi^2}{2}}$ is the solution of the partial differential equation, and $\Psi(0)$ equals $1$, hence $\Psi(\xi) = e^{-\frac{\xi^2}{2}}$.\\
Let $X =\sigma Y + \mu$
\begin{eqnarray*}
  && \Psi_X(\xi)\\
  &=& \mathrm{E}[e^{i\xi X}]\\
  &=& \mathrm{E}[e^{i\xi (\sigma Y + \mu) }]\\
  &=& e^{i\xi\mu} \mathrm{E}[e^{iy\xi (\sigma Y)}]\\
  &=& e^{i\xi\mu} \mathrm{E}[e^{iy (\xi \sigma) Y}]\\
  &=& e^{i\xi\mu} \Psi_Y(\xi\sigma)\\
  &=& e^{i\mu\xi-\frac{1}{2}(\sigma\xi)^2}
\end{eqnarray*}
\end{proof}

\begin{definition}
  Let $X$ be a $\mathbb{R}^{n}$-valued random vector. $X$ is said to be \emph{normally distributed} or \emph{Gaussian}, if for any $d \in \mathbb{R}^{n}$ such that $d^TX$ is Gaussian in $\mathbb{R}$.
  \label{sec:g1}
\end{definition}
%Note that, $<d,X>$ is defined as scalar product on $\mathbb{R}^{n}$ that means $\sum_{j=1}^{n}\,d_j\cdot X_i$. 

\begin{definition}
  A stochastic process $(X_t)_{t\in T}$ is said to be \emph{Gaussian process} if the joint distribution of any finite instance is Gaussian, that means
$
(X_{t_1},\dots, X_{t_n})
$ has joint Gaussian distribution in $\mathbb{R}^n$ for $t_1,\dots,t_n \in T$.
\label{sec:defgau}
\end{definition}
The definition immediately shows every instances $X_t$ in Gaussian process is Gaussian.

\begin{corollary}
  Let $(X_t)_{t\in T}$ be a stochastic process. The following condition is \\equivalent to Definition \ref{sec:defgau}.
  \begin{equation*}
	\sum_j^n c_{t_j} X_{t_j}
  \end{equation*}
  is Gaussian for any $t_1,\dots,t_n \in T, c_{t_j} \in \mathbb{R}$ for $j\in {1,\dots,n}$.
  \label{sec:gauss}
\end{corollary}
\begin{proof}
  It is clear due to Definition \ref{sec:g1}.
\end{proof}

\begin{lemma}
  Let $X$ be a $\mathbb{R}^{n}$-valued normally distributed ramdon vector. Then its characteristic function is 
  \begin{equation}
	\mathrm{E}\,e^{i\xi^TX} = e^{i\xi^Tm - \frac{1}{2}\xi^T \Sigma \xi}.
	\label{sec:mcf}
  \end{equation}
 For $\xi \in \mathbb{R}^{n}$. Where $m \in \mathbb{R}^{n}, \Sigma \in \mathbb{R}^{n\times n}$ are \emph{mean vector} , \emph{covariance matrix} of $X$ \\respectively. Furthermore, the density function of $X$ is
\begin{equation}
  (2\pi)^{-\frac{n}{2}}\, (\det\Sigma) ^{-\frac{1}{2}}\,e^{-\frac{1}{2}(x-m)^T\Sigma^{-1}(x-m)}.
  \label{sec:dsy}
\end{equation}
\end{lemma}

Remark, the equation (\ref{sec:mcf}) can also be as definition of characteristic function of a \\n-dimensional normally distributed random variable. I.e., any normally distributed random variable can be characterized by form of the equation (\ref{sec:mcf}).

\begin{proof}
  Since $X$ normally distributed on $\mathbb{R}^{n}$, then $\xi^T X$ is normally distributed on $\mathbb{R}$. Due to the Theorem \ref{sec:char}, there is
  \begin{eqnarray*}
	\mathrm{E} e^{i\xi^T X} &=& \mathrm{E} e^{i\cdot 1 \cdot \xi^T X}\\
	                        &=& e^{i\mathrm{E}\sqbr{\xi^T X} -\frac{1}{2}\mathrm{Var}\sqbr{\xi^T X}}\\
							&=& e^{i\xi^T\mathrm{E}\sqbr{X} - \frac{1}{2}\xi^T \mathrm{Var}\sqbr{X} \xi}\\
						    &=& e^{i\xi^Tm - \frac{1}{2}\xi^T \Sigma \xi}.
  \end{eqnarray*}
  Moreover, since $\Sigma$ symmetirc and positive definit, there exist $\Sigma^{-1}, \Sigma^{\frac{1}{2}}$ and $\Sigma^{-\frac{1}{2}}$.
  \begin{eqnarray*}
	&&(2\pi)^{-\frac{n}{2}} (\det\Sigma) ^{-\frac{1}{2}}\int_{\mathbb{R}^{n}} e^{i x^T \xi}e^{-\frac{1}{2}(x-m)^T\Sigma^{-1}(x-m)}\, \mathop{dx}\\
	&=& (2\pi)^{-\frac{n}{2}} (\det\Sigma) ^{-\frac{1}{2}}\int_{\mathbb{R}^{n}} e^{ix^T\xi} e^{i (x-m)^T \xi}e^{-\frac{1}{2}(x-m)^T\Sigma^{-1}(x-m)}\, \mathop{dx} \\
	&=& (2\pi)^{-\frac{n}{2}} (\det\Sigma) ^{-\frac{1}{2}}  e^{im^T\xi} \int_{\mathbb{R}^{n}} e^{i(x-m)^T\xi} e^{i (x-m)^T \xi}e^{-\frac{1}{2}(x-m)^T\Sigma^{-1}(x-m)}\, \mathop{dx} \\
	&\overset{y=\Sigma^{-\frac{1}{2}}x}{=}& (2\pi)^{-\frac{n}{2}} e^{im^T\xi} \int_{\mathbb{R}^{n}}e^{i (\Sigma^{\frac{1}{2}}y) \xi}\,e^{-\frac{1}{2}|y|^2}\, \mathop{dy}\\
	&=& (2\pi)^{-\frac{n}{2}} e^{im^T\xi}  \int_{\mathbb{R}^{n}}e^{i y^T (\Sigma^{\frac{1}{2}}\xi)}\,e^{-\frac{1}{2}|y|^2}\, \mathop{dy}\\
    %&=& (2\pi)^{-\frac{n}{2}} e^{i\xi m} \int_{\mathbb{R}^{n}}e^{i y- (\Sigma^{\frac{1}{2}}\xi)}\,e^{-\frac{1}{2}|y|^2}\, \mathop{dy}\\
	&\overset{\text{Fourier transformation}}{=}& e^{im^T\xi}  e^{-\frac{1}{2}|\Sigma^{\frac{1}{2}}\xi|^2}\\
	&=& e^{im^T\xi}  e^{-\frac{1}{2}\xi^T\Sigma\xi}
  \end{eqnarray*}
  In terms of the uniqueness theorem of characteristic function (in \cite{bauer}, p.199, Satz 23.4), then we can deduce (\ref{sec:dsy}) is density function of $X$.
\end{proof}

%A normal distributed normal random variable can be characterized by its expected value and variance respectively mean vector and covariance vector because of the characteristic function.
%\begin{theorem}
%  Let $X$ be a $\mathbb{R}^{n}$-valued normal distributed random vector with independent, normal distributed components. Then $X$ has a joint normal distribution.
%\end{theorem}

%\begin{proof}
  
%\end{proof}

%\begin{corollary}
%  Let $X$ be a $\mathbb{R}^{n}$-valued normal distributed random vector with normal distributed components. If the covariance matrix of $X$ is positive definit and symmtric, then $X$ has a joint normal distribution.
%  \label{sec:mulnor}
%\end{corollary}

\begin{theorem}
  A linear combination of independent normally distributed random \\variable (or vector) is Gaussian.
\end{theorem}

\begin{proof}
  We suppose $X_1, \cdots, X_m$ are independent random vectors  on $\mathbb{R}^n$ and $c_1, \cdots, c_m \in \mathbb{R}$. Let us have a look at the characteristic function of it,
  \begin{eqnarray*}
	\mathrm{E}e^{i\xi^T\sum_{j=1}^m(c_jX_j)} &\overset{independent}{=}&\prod_{j=1}^{m} \mathrm{E}e^{i\xi^T(c_jX_j)}\\
	&=& \prod_{j=1}^m \exp\brkt{i\xi^T\mathrm{E}[c_jX_j]-\frac{1}{2}\xi^T\mathrm{Var}[c_jX_j]\xi}\\
	&=&  \exp\brkt{i\xi^T\mathrm{E}[\sum_{j=1}^{m}c_jX_j]-\frac{1}{2}\xi^T\mathrm\sum_{j=1}^{m}{Var}[c_jX_j]\xi}\\
	&\overset{independent}{=}&  \exp\brkt{i\xi^T\mathrm{E}[\sum_{j=1}^{m}c_jX_j]-\frac{1}{2}\xi^T\mathrm{Var}[\sum_{j=1}^{m}c_jX_j]\xi},
  \end{eqnarray*}
  which is a form of characteristc function of normal distribution. That means $\sum_{j=1}^m c_jX_j$ is Gaussian. 
\end{proof}

\begin{example}[Bivariate Normal Distribution]
  Cf.\cite{severini}, p.241, Example 8.6. Suppose $S_1, S_2$ are independent random variables and have standard normal distributions. $\left(
    \begin{array}{c}
      S_1 \\
      S_2
    \end{array}
  \right)$  has standard normal joint distribution since they are independent. We define
  \begin{eqnarray}
	\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)
	&=& 
	\left(
    \begin{array}{l}
	  \sigma_1,\hspace{3em} 0 \\
	  \sigma_2 \rho, \sigma_2(1-\rho^2)^{\frac{1}{2}}
    \end{array}
  \right) \cdot
	\left(
    \begin{array}{c}
      S_1 \\
      S_2
    \end{array}
  \right)  +
  \left(
    \begin{array}{c}
      \mu_1 \\
      \mu_2
    \end{array}
  \right)
  \label{sec:bi},
  \end{eqnarray}
  where $\mu_1, \mu_2, \sigma_1, \sigma_2 \in \mathbb{R}, -1 \le \rho \le 1$. Again, $Y_1, Y_2$ are Gaussian and the joint distribution  $\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)$ is also Gaussian.  Since $S_1, S_2$ are independent,
	\begin{eqnarray*}
	  \mathrm{Var}[Y_1] &=& \mathrm{Var}[\sigma_1 S_1]\\
						 &=& \sigma_1^2 ,\\
						 \mathrm{Var}[Y_2] &=& \mathrm{Var}[\sigma_2\rho S_1] + \mathrm{Var}[\sigma_2 (1-\rho^2)^{\frac{1}{2}} S_2]\\
						 &=& \sigma_2^2 \rho^2 + \sigma_2^2(1 - \rho^2)\\
						 &=& \sigma_2^2,\\
						 \mathrm{Cov}[Y_1, Y_2] &=& \mathrm{E}[(Y_1 - \mathrm{E}[Y_1])(Y_2 - \mathrm{E}[Y_2])]\\
						 &=& \mathrm{E}[Y_1Y_2 - \mu_1Y_2 - \mu_2Y_1 + \mu_1\mu_2]\\
						 &=& \mathrm{E}[(\sigma_1 S_1 + \mu_1)(\sigma_2\rho S_1 + \sigma_2(1-\rho^2)^{\frac{1}{2}}S_2 + \mu_2)] - \mu_1\mu_2\\
						 &=& \sigma_1\sigma_2\underbrace{\mathrm{E}[S_1^2]}_{=1}\rho + \mu_1\sigma_2\rho\underbrace{\mathrm{E}[S_1]}_{=0} +
						 \sigma_1\sigma_2(1-\rho^2)^{\frac{1}{2}}\underbrace{\mathrm{E}[S_1S_2]}_{=\mathrm{E}[S_1]\mathrm{E}[S_2]=0} \\
						 &+& \mu_1\sigma_2(1-\rho^2)^{\frac{1}{2}}\underbrace{\mathrm{E}[S_2]}_{=0} + \sigma_1\underbrace{\mathrm{E}[S_1]}_{=0}\mu_2 + \mu_1\mu_2 - \mu_1\mu_2\\
						 &=& \rho\sigma_1\sigma_2,
	\end{eqnarray*}
	that means the corrlation of $Y_1, Y_2$ is $\rho$.
	Because of the (\ref{sec:dsy}), the joint density function
	\begin{eqnarray*}
	  f_{Y_1, Y_2}(y_1, y_2) &=& (2\pi)^{-1} (\det(\Sigma))^{-\frac{1}{2}} \exp\brkt{(y_1 - \mu_1) \Sigma^{-1} (y_2 - \mu_2)},
	\end{eqnarray*}
	where $\Sigma =\left(
    \begin{array}{l}
	  \sigma_1^2, \hspace{3em}0 \\
	  \sigma_2^2\rho^2, \sigma_2^2(1-\rho^2)
    \end{array}
  \right)
 $\\
 Indeed, 
 $$
 	\det(\Sigma) = (1-\rho^2)\sigma_1^2\sigma_2^2
 $$ and 
 $$
 \Sigma^{-1} = \frac{
   \left(
    \begin{array}{l}
	  \sigma_2^2(1-\rho^2), \hspace{1em}0 \\
	  -\sigma_2^2\rho,\hspace{3em}\sigma_1^2
    \end{array}
  \right)}
  {\displaystyle (1-\rho^2)\sigma_1^2\sigma_2^2}.
  $$
 Namely,
 \begin{equation}
   f_{Y_1, Y_2}(y_1, y_2) = \frac{1}{2\pi(1 - \rho^2)^{\frac{1}{2}}\sigma_1\sigma_2}\exp\brkt{-\frac{1}{2(1-\rho^2)}(z_1^2 - 2\rho z_1 z_2 + z_2^2)}
   \label{sec:jdt}
 \end{equation}
 where $z_1 = \frac{y_1-\mu_1}{\sigma_1}, z_2=\frac{y_2-\mu_2}{\sigma_2}$.
\end{example}

\begin{corollary}
  Let $Y_1, Y_2$ be $\mathbb{R}$-valued random variables and $\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)$  has a joint normal distribution, then the conditional expected value of $Y_2$ given $Y_1$
    $$
	\mathrm{E}[Y_2| Y_1=y_1] = \mathrm{E}[Y_2] + \rho (y_1 - \mathrm{E}[Y_1])\frac{\sigma_2}{\sigma1},
	$$
	and the conditional variance of $Y_2$ given $Y_2$
	$$
		\mathrm{Var}[Y_2| Y_1 = y_1] = \sigma_2^2 (1 - \rho^2).
	$$
	Where $\sigma_1, \sigma_2$ are standard deviations of $Y_1, Y_2$ and $\rho$ is the correlation of $Y_1, Y_2$.
	\label{sec:condi}
\end{corollary}

\begin{proof}
  Recall (\ref{sec:jdt}), we can specify the joint density function if $\sigma_1, \sigma_2, \rho$ are known. As result of this,
  $\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)$ has a form of (\ref{sec:bi}).
  Suppose $S_1, S_2$ are independent standard normal distributed random variables. Now we have
  \begin{eqnarray*}
	S_1 &\sim& \frac{(Y_1 - \mathrm{E}[Y_1])}{\sigma_1} \\
	Y_2 &\sim& \sigma_2\rho S_1 + \sigma_2(1-\rho^2)^{\frac{1}{2}} S_2 + \mathrm{E}[Y_2],
  \end{eqnarray*}
  more precisely,
  $$
  Y_2 \sim \sigma_2\rho \frac{(Y_1 - \mathrm{E}[Y_1])}{\sigma_1}  + \sigma_2(1-\rho^2)^{\frac{1}{2}} S_2 + \mathrm{E}[Y_2].
  $$
  Take expectation of both sides, 
  \begin{equation*}
	\mathrm{E}[Y_2|Y_1=y_1] = \sigma_2\rho \frac{(y_1 - \mathrm{E}[Y_1])}{\sigma_1} + \mathrm{E}[Y_2].
  \end{equation*}
  Now consider
  \begin{eqnarray*}
	\mathrm{Var}[Y_2|Y_1=y_1] &=&  \mathrm{E}[(Y_2 - \mu_{Y_2|Y_1})^2|Y_1=y_1]\\
							  &=& \int_{-\infty}^{\infty}(y_2 - \mu_{Y_2|Y_1})^2f_{Y_2|Y_1}(y_2, y_1)\,\mathop{dy_2}\\
							  &=& \int_{-\infty}^{\infty}\sqbr{y_2 - \mu_2 - \frac{\rho\sigma_2}{\sigma_1}(y_1-\mu_1)}^2f_{Y_2|Y_1}(y_2, y_1)\,\mathop{dy_2},
  \end{eqnarray*}
 After multiplying both sides by the density function of $Y_1$ and integrating it by $y_1$, we have
\begin{eqnarray*}
 &\,&\int_{-\infty}^{\infty} \mathrm{Var}[Y_2|Y_1=y_1] f_{Y_1}(y_1) \mathop{dy_1} \\
 &=&\int_{-\infty}^{\infty}\,\int_{-\infty}^{\infty} \sqbr{y_2 - \mu_2 
	- \frac{\rho\sigma_2}{\sigma_1}(y_1-\mu_1)}^2\underbrace{f_{Y_2|Y_1}(y_2, y_1)\,f_{Y_1}(y_1)}_{f_{Y_1, Y_2}(y_1, y_2)} \mathop{dy_2}\,\mathop{dy_1}\\
	&\iff&\\
	&\,&\mathrm{Var}[Y_2|Y_1=y_1] \underbrace{\int_{-\infty}^{\infty}  f_{Y_1}(y_1)}_{1} \mathop{dy_1} \\
	&=& \mathrm{E}\sqbr{(Y_2 - \mu_2) - (\frac{\rho\sigma_2}{\sigma_1})(Y_1 - \mu_1)}^2 
\end{eqnarray*}

multiplying right side out, we see
\begin{eqnarray*}
  \mathrm{Var}[Y_2|Y_1=y_1] &=&\underbrace{\mathrm{E}[(Y_2 - \mu_2)^2]}_{\sigma_2^2} - 2\frac{\rho\sigma_2}{\sigma_1}\underbrace{\mathrm{E}[(Y_1 -\mu_1)(Y_2 - \mu_2)]}_{\rho\sigma_1\sigma_2}\\
  &+& \frac{\rho^2\sigma_2^2}{\sigma_1^2}\underbrace{\mathrm{E}[(Y_1-\mu_1)^2]}_{\sigma_1^2}\\
  &=& \sigma_2^2 - 2\rho^2\sigma^2 + \rho^2\sigma_2^2\\
  &=& \sigma_2^2 - \rho^2\sigma_2^2\\
  &=& \sigma_2^2(1-\rho^2).
\end{eqnarray*} 
\end{proof}

\begin{theorem}
  Let $X$ be a Gaussian random variable, then
  \begin{equation}
	\mathrm{E}[\exp(\beta X)] = \exp(\beta\mu + \frac{1}{2}\beta^2\sigma^2).
	\label{sec:expgau}
  \end{equation}
  Where $\mu$ and $\sigma$ are $\mathrm{E}[X]$ and $\mathrm{Var}[X]$ respectively.
\end{theorem}
\begin{proof}
\begin{eqnarray*}
  &&\mathrm{E}[\exp(\beta X)] \\
&=& (2\pi \sigma^2)^{-\frac{1}{2}}\int_\mathbb{R} \exp(\beta x) \exp\brkt{-\frac{(x-\mu)^2}{2\sigma^2}} \mathop{dx}\\
&=& (2\pi \sigma^2)^{-\frac{1}{2}}\int_\mathbb{R} \exp(\beta x) \exp\brkt{-\frac{(x^2- 2x\mu+ \mu^2)}{2\sigma^2}} \mathop{dx}\\
&=& (2\pi \sigma^2)^{-\frac{1}{2}}\int_\mathbb{R} \exp\brkt{-\frac{x^2- 2(\beta\sigma^2+\mu)x+ \mu^2}{2\sigma^2}} \mathop{dx}\\
&=& (2\pi \sigma^2)^{-\frac{1}{2}}\int_\mathbb{R} \exp\brkt{-\frac{x^2- 2(\beta\sigma^2+\mu)x+ (\beta\sigma^2+\mu)^2 -  (\beta\sigma^2+\mu)^2 + \mu^2}{2\sigma^2}} \mathop{dx}\\
&=& (2\pi \sigma^2)^{-\frac{1}{2}}\int_\mathbb{R} \exp\brkt{-\frac{(x- (\beta\sigma^2+\mu))^2+ \mu^2 - (\beta\sigma^2+\mu)^2}{2\sigma^2}} \mathop{dx}\\
&=& \exp\brkt{\frac{(\beta\sigma^2+\mu)^2 - \mu^2}{2\sigma^2}} \underbrace{(2\pi \sigma^2)^{-\frac{1}{2}}\int_\mathbb{R} \exp\brkt{-\frac{(x- (\beta\sigma^2+\mu))^2}{2\sigma^2}}\mathop{dx}}_{1}\\
&=& \exp\brkt{\frac{\beta^2\sigma^4 + 2\mu\beta\sigma^2}{2\sigma^2}}\\
&=& \exp(\mu\beta + \frac{1}{2}\beta^2\sigma^2)
\end{eqnarray*}
\end{proof}
\subsection{Brownian Motion}
The Brownian motion was first introduced by Bachelier in 1900 in his PhD thesis. Now we give the common definition of it.
\begin{definition}
Let $(B_t)_{t\ge0}$ be a $\mathbb{R}^{n}$-valued stochastic process. $(B_t)$ is called \emph{Brownian motion} if it satisfies the following conditions:
\begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
  \item $B_0 = 0 $ a.s. .
  \item $(B_{t_1} - B_{t_0}),\dots,(B_{t_n} - B_{t_{n_1}})$ are independet for $0=t_0<t_1<\dots<t_n$ and $n \in \mathbb{N}$.
  \item $B_t - B_s \sim B_{t-s}$, for $0 \le s \le t < \infty$.
  \item $B_t - B_s \sim \mathcal{N}(0, t-s)^{\otimes n}$.
  \item $B_t$ is continuous in $t$ a.s. .
\end{enumerate}
\end{definition}
A usual saying for $(ii)$ and $(iii)$ is the Brownian motion has independent, stationary increments. In $(iv)$, $\mathcal{N}$ represent a random variable which has a normal distribution. $B_t$ is normally distributed due to $(ii)$. It is clear that the increments of Brownian motion is stationary.

\begin{proposition}
  Let $(B_t)$ be $\mathbb{R}$-valued Brownian motion. Then the covariance of $B_m, B_n$ for $m, n \ge 0$ is $m \wedge n $.
\end{proposition}
\begin{proof}
  Without loss of generality, we assume that $m \ge n$, then
  \begin{eqnarray*}
	\mathrm{E}[B_mB_n] &=& \mathrm{E}[(B_m - B_n)B_n] + \mathrm{E}[B_n^2]\\
	&=& \mathrm{E}[B_m - B_n]\mathrm{E}[B_n] + n\\
	&=& n .
  \end{eqnarray*}
  \label{sec:cor}
\end{proof}

\begin{proposition}
  Let $(B_t)$ be  $\mathbb{R}$-valued Brownian motion. Then $B_{cm} \sim c^{\frac{1}{2}}B_m$.
\end{proposition}
\begin{proof}
  Because $B_m$ is normal distributed for any $m > 0$, we then get
  \begin{eqnarray*}
	\mathrm{E} [e^{i\xi B_{cm}}] &=& e^{-\frac{1}{2}cm\xi^2}\\
	&=& e^{-\frac{1}{2}m(c^{\frac{1}{2}}\xi)^2}\\
	&=& \mathrm{E} [e^{i\xi c^{\frac{1}{2}}B_m}] .
  \end{eqnarray*}
\end{proof}

\begin{theorem}
  A $\mathbb{R}$-valued Brownian motion is a Gaussian process.
\end{theorem}
\begin{proof}
  The following idea using the independence of increments to prove the claim come from \cite{shilling}.
  We choose $0=t_0<t_1<\dots<t_n$, for $n \in \mathbb{N}$. Define
  $V = (B_{t_1},\dots,B_{t_n})^T$,  $K = (B_{t_1}-B_{t_0},\dots, B_{t_n}-B_{t_{n-1}})^T$ and 
  $A = 
  \begin{pmatrix}
	1      & 0      & \cdots & 0\\
	1      & 1      & \cdots & 0\\
	\vdots & \vdots & \ddots & \vdots \\
	1      & 1      & \cdots & 1
  \end{pmatrix}
	$.
  Let us look at the characteristic function of $V$,
  \begin{eqnarray*}
	&&\mathrm{E} [e^{i\xi^T V}]\\
	&=& \mathrm{E} [e^{i\xi^T AK}]\\ 
	&=& \mathrm{E} [e^{iA^T\xi K}]\\
	&=& \mathrm{E} [\exp(i (\xi^{(1)}+\dots+\xi^{(n)}, \xi^{(2)}+\dots+\xi^{(n)}, \cdots,\xi^{(n)})] \\
	&\cdot& (B_{t_1}-B_{t_0}, B_{t_2}-B_{t_1},\dots,B_{t_n}-B_{t_{n-1}})^T)\\
	&\overset{ind.increments}{=}& \prod_{j=1}^n \mathrm{E} [\exp(i(\xi^{(j)+\dots+\xi^{(n)}})(B_{t_j}-B_{t_{t-1}}))]\\
	&\overset{stat.increments}{=}& \prod_{j=1}^n \exp(-\frac{1}{2}(t_j - t_{j-1})(\xi^{(j)}+\dots+\xi^{(n)})^2) \\
	&=& \exp\left(-\frac{1}{2}\sum_{j=1}^n (t_j - t_{j-1})(\xi^{(j)}+\dots+\xi^{(n)})^2\right)\\
    &=& \exp\left(-\frac{1}{2}\left(\sum_{j=1}^n t_j(\xi^{(j)}+\dots+\xi^{(n)})^2 - \sum_{j=1}^n t_{j-1}(\xi^{(j)}+\dots+\xi^{(n)})^2\right)\right)\\
	&=& \exp\left(-\frac{1}{2}\left(\sum_{j=1}^{n-1} t_j((\xi^{(j)}+\dots+\xi^{(n)})^2 - (\xi^{(j+1)}+\dots+\xi^{(n)})^2) + t_n(\xi^{(n)})^2\right)\right)\\
	&=& \exp\left(-\frac{1}{2}\left(\sum_{j=1}^{n-1} t_j\xi^{(j)}(\xi^{(j)}+2\xi^{(j+1)}+\dots+2\xi^{(n)}) + t_n(\xi^{(n)})^2\right)\right)\\
	&=& \exp\left(-\frac{1}{2}\left(\sum_{j,h=1}^n(t_j\wedge t_h)\xi^{(j)}\xi^{(h)}\right)\right).
  \end{eqnarray*}
  In Proposition \ref{sec:cor}, $(t_j\wedge t_h)_{t,h=1,\dots,n}$ is the covariance matrix of $V$ and therefore it is symmetric and positive definit. The mean vector of it is zero, then we have proved that the characteristic function is a form of some normal distributed random vector, i.e., $V$ is Gaussian.
\end{proof}

Schilling gave in his lecture \cite{shilling} the relationship between a one-dimensional Brownian motion and a n-dimensional Brownian motion.
In fact, $(B_t^{(l)})_{l=1,\dots,n}$ is Brownian motion if and only if $B_t^{(l)}$ is Brownian motion and all of the components are independent. Using this independence and the theorem of Fubini in the characteristic function for high dimensional Brownian motion we can say a n-dimensional Brownian motion is also a Gaussian process.

\begin{definition}
  Let $(X_t)_{t\in T}$  be a stochastic process. $(Y_t)_{t\in T}$ is defined on the same probability space as $(X_t)_{t\in T}$ and said to be \emph{modification} of $(X_t)_{t\in T}$, if
  \begin{equation*}
	\mathcal{P}[X_t = Y_t] = 1 \hspace{1em} \forall \hspace{1em} t\in T.
  \end{equation*}
\end{definition}

\begin{theorem}[Kolmogorov Chentsov]
   Let $(X_t)_{t \ge 0}$ be a stochastic process on $\mathbb{R}^{n}$ such that
  \begin{equation*}
	\mathrm[|X_j - X_k|^\alpha] \le c |j - k|^{1+\beta} \hspace{1em}\forall \hspace{1em} j ,k \ge 0 \hspace{1em} \text{and} \hspace{1em} j\neq k,
  \end{equation*}
  for $\alpha, \beta > 0, c < \infty$. Then $(X_t)_t$ has a modification $(Y_t)_t$ with continuous sample path such that 
  \begin{equation*}
	\mathrm{E}[(\frac{|Y_j - Y_k|}{|j - k|^\gamma})^\alpha] < \infty
  \end{equation*}
  for all $\gamma \in (0, \frac{\beta}{\alpha})$.
  \label{sec:kolch}
\end{theorem}
\begin{proof}
  See \cite{loeve}, p.519.
\end{proof}

\begin{lemma}
   Let $(B_t)_{t\ge 0}$ be Brownian motion. Then
   \begin{equation*}
	 \mathrm{E}[B_t^{2k}] = (2k - 1)!! t^{2k}
   \end{equation*}
   for $k \in \mathbb{N}$.
   \label{sec:le1}
\end{lemma}

\begin{proof}
  Cf.\cite{shilling}. Taking expectation of $B_t^{2k}$, we get
  \begin{eqnarray*}
	\mathrm{E}[B_t^{2k}] &=& \frac{1}{\sqrt{2\pi t}}\int_{-\infty}^{\infty} x^{2k} e^{-\frac{x^2}{2t}}\,\mathop{dx}\\
	&\overset{x=\sqrt{2ty}}{=}& \frac{2^kt^k}{\sqrt{\pi}} \int_0^{\infty} y^{k-\frac{1}{2}} e^{-y}\,\mathop{dy}\\
	&=& \frac{2^kt^k}{\sqrt{\pi}} \int_0^{\infty} y^{k+\frac{1}{2}-1} e^{-y}\,\mathop{dy}\\
	&=& \frac{2^kt^k}{\sqrt{\pi}} \Gamma(k + \frac{1}{2})\\
	&=& \frac{2^kt^k}{\sqrt{\pi}} \Gamma(\frac{1}{2})\prod_{j=1}^k(j-\frac{1}{2}) \\
	&=& 2^k t^k\prod_{j=1}^k(\frac{2j-1}{2}) \\
	&=& (2k - 1)!!\cdot t^k
  \end{eqnarray*}
\end{proof}

\begin{corollary}
  Let $(B_t)_{t\ge 0}$ be Brownian motion. Then $B_t$ is $\gamma$-H\"older continuous on a compact scale almost surely for all $\gamma < \frac{1}{2}$.
\end{corollary}

\begin{proof}
  Because of Lemma \ref{sec:le1}, we have
  \begin{eqnarray*}
	\mathrm{E}[(B_t-B_s)^{2k}] &=& \mathrm{E}[B_{t-s}^{2k}]\\
	&=& (2k - 1)!! \cdot |t-s|^{k}.
  \end{eqnarray*}
  In terms of the Theorem of Kolmogorov Chenstov, $B_t$ is $\gamma$-H\"older continuous a.s. for $\gamma \in (0, \frac{k}{2k})$.
\end{proof}

\newpage

%-------- 3. section -----------%
\section{Stable Measures and Stable Integrals}
\setcounter{equation}{0}
In order to represent an integration form of fractional Brownian motion, we deal with the stable integral in this section. In fact, fractional Brownian motion is a Gaussian process with zero mean. To show Gaussian properties of it, we define it by a 
stable integral which can imaged as stochastic process of stable variables on time.

\subsection{Stable Variables}
%%We consider now the one-dimensional Brownian motion. In this section we need some notations, which are defined as followings
%\begin{equation*}
%\Delta^{[0,T]} = \left\{t_1,\dots,t_n|0=t_0<\dots<t_n=T\right\} 
%\end{equation*}
%$$
%|\Delta^{[0,T]}| = \max_{t_j \in \Delta^{[0,T]}}|t_j - t_{j-1}|
%$$.

%\begin{lemma}
%  Let $B_t$ be a Brownian motion. Then
%  $$
%  \sum_{t_j \in \Delta^{[0,T]}} |B_{t_j} - B_{t_{j-1}}|^2 \xmapsto[L^2(\mathcal{P})]{|\Delta^{[0,T]}|\rightarrow 0} T  
%  $$
%\end{lemma}

\begin{definition}
  Let $X$ be a random variable. $X$ is said to have a stable distribution, if there exist $0 < \gamma \le 2, \delta \ge 0, -1 \ge \kappa \ge 1, \theta \in \mathbb{R}$ such that its characteristic function can be described as following
  \begin{equation}
	\mathrm{E} [\exp i\xi X] =  \begin{cases} \exp\{i \xi \theta - |\delta\xi|^\gamma(1-i\kappa\cdot sgn(\xi)\tan \frac{\gamma\pi}{2})\},\hspace{1em}  \text{if}\hspace{1em} \gamma \in (0, 1) \cup (1, 2], \\
	    \exp\{i \xi \theta - |\delta\xi|(1+i\frac{2}{\pi}\kappa\cdot sgn(\xi)\ln |\xi|)\},\hspace{1em} \text{if}\hspace{1em} \gamma = 1.
	  \end{cases}
	\label{sec:stbl}
  \end{equation}
 Where
$$
 sgn(x) = \begin{cases} 1,\hspace{1em} \text{if}\hspace{1em} x > 1\\
   0,\hspace{1em} \text{if}\hspace{1em} x = 0 \\
   -1,\hspace{1em}\text{if}\hspace{1em} x < 0.
 \end{cases}
  $$
\end{definition}

Notice, we write $\Lambda(\gamma, \kappa, \theta, \delta)$ for one random variable whose characteristic function equals (\ref{sec:stbl}).

\begin{theorem}
  $X$ is Gaussian if and only $X \sim \Lambda(\gamma, \kappa, \theta, \delta)$ with $\gamma = 2$.
\end{theorem}
\begin{proof}
  On the one hand, if $X$ is Gaussian, according to the charateric function of a Gaussian variable, $\gamma$ must equal $2$. On the other hand, if $\gamma = 2$, then $i\kappa\cdot sgn(\xi)\tan \frac{\gamma\pi}{2}$ vanishes since $\tan(\pi) = 0$. Therefore, $X$ is Gaussian because $\mathrm{E} [\exp i\xi X]=\exp\{i \xi \theta - |\delta\xi|^2\}$.
\end{proof}
Remark, if $\gamma=2$, then $\kappa$ is irrelevant in Definition. We specific $\kappa = 0$ without loss of generality. For instance, $B_t \sim \Lambda(2, 0, 0, \frac{\sqrt{2t}}{2})$ when $(B_t)_t$ is Brownian motion.

\begin{definition}
  A random variable $X$ is said to be \emph{symmetric} if $X$ and $-X$ have the same distribution.
\end{definition}

\begin{proposition}
  Let $X$ be have a stable distribution. $X$ is  \emph{symmetric} if and only if $X \sim \Lambda(\gamma, 0, 0, \delta)$. I.e. its characteristic function has the form
  \begin{equation}
	\mathrm{E}[\exp\{i \xi X\}] = \exp\{-|\delta\xi|^\gamma\}
  \end{equation}
\end{proposition}
\begin{proof}
  The Definition of symmetricity implies
  \begin{eqnarray*}
	&&\exp\{i \xi \theta - |\delta\xi|^\gamma(1-i\kappa\cdot sgn(\xi)\tan \frac{\gamma\pi}{2})\}\\
	&=& \mathrm{E}[i\xi X]\\
	&=& \mathrm{E}[i\xi (-X)]\\
	&=& \mathrm{E}[i (-\xi) X]\\
	&=& \exp\{i (-\xi) \theta - |\delta\xi|^\gamma(1-i\kappa\cdot sgn(-\xi)\tan \frac{\gamma\pi}{2})\},
  \end{eqnarray*}
  for $\xi \in \mathbb{R}$. This requires $\theta=\kappa=0$.
\end{proof}

\begin{corollary}
  Let $(B_t)_t$ be Brownian motion, then $B_t$ has a symmetric stable distribution.
\end{corollary}
\begin{proof}
  It is clear due to the previous Proposition.
\end{proof}

\subsection{Stable Random Measures}
In this subsection we suppose $(\Omega, \mathscr{A}, \mathcal{P}), (\mathrm{D}, \mathscr{D}, \mu)$  are probability spaces, $\kappa(\cdot) : \mathrm{D} \rightarrow [-1, 1]$ is a measurable function. For the next definition we need a notation
%\begin{definition}
%	Let $\lambda$ be a measure such that
%	\begin{equation*}
%	  \lambda : \mathscr{D} \rightarrow \mathscr{E}\subset \mathscr{A}
%	\end{equation*}
%	$\lambda$ is said to be \emph{independently scattered}, if $\lambda[D_1], \dots, \lambda[D_n]$ are independent for any $D_1,\dots, D_n$ \emph{disjoint} $\in \mathscr{D}$ .
%\end{definition}

\begin{equation}
  \mathscr{G} = \{D \in \mathscr{D} : \mu[D] < \infty\}.
\end{equation}

\begin{definition}
	Let $\nu$ be a set function such that
	\begin{equation}
	  \nu : \mathscr{G} \rightarrow \mathcal{L}^0(\Omega).\nonumber
	\end{equation}
	$\nu$ is said to be \emph{independently scattered}, if $\nu[D_1], \dots, \nu[D_n]$ are independent for any $D_1,\dots, D_n$ \emph{disjoint} $\in \mathscr{D}$ .
\end{definition}


\begin{definition}
  Let $\nu$ be an independent cattered and $\sigma$-additive set function,
%\begin{equation*}
%  \nu: \mathscr{G}  \rightarrow \mathrm{L}^{\infty}(\Omega, \mathscr{A}, \mathcal{P}). 
%\end{equation*}
$\nu$ is said to be \emph{stable random measure} on $(D, \mathscr{D})$ with control measure $\mu$, degree $\gamma$ and skewness intensity $\kappa(\cdot)$ if 
\begin{equation}
  \nu[F] \sim \Lambda\brkt{\gamma, \frac{\int_F \kappa(x)\, \mu[\mathop{dx}]}{\mu[F]}, 0, (\mu[F])^{\frac{1}{\gamma}}}
\end{equation}
for $F \in \mathscr{D}$.
\end{definition}
Samorodnitsky and Taqqu showed the existence of stable measures, see \cite{samorodnitsky}, pp.119$\sim$120.

\begin{example}
  Suppose $[0, T]$ is a index set, $0=t_0 < t_1 <\dots < t_k = T$ for $k\in \mathbb{N}$ and $(B_{t})$ is Brownian motion.  We show the mapping $\nu : \mathscr{B}([0, T]) \rightarrow \mathcal{L}^0(\Omega)$, where $\nu[A_j](\omega):= B_{t_{j+1}}(\omega) - B_{t_{j}}(\omega)$ for $A_j=[t_{j}, t_{j+1})$.\\
	Firstly, we show $\nu$ is independently scattered and $\sigma$-additive. We take $\{A_j\}$ such that $\cup_{j=1}^{\infty}A_j = [0, T]$. $\{\nu[A_k]\}_{k=1}^{\infty}$ has independent elements since $B_{t_1} - B_{t_0}, \dots,  B_{t_{j+1}}- B_{t_{j}} $ are independent.\\
	Secondly, 
	\begin{eqnarray*}
	  \nu[\brkt{\cup_{j=1}^{\infty}A_j}] &=& B_T - B_1\\
	  &=& \sum_{j=1}^{\infty} (B_{t_{j+1}} - B_{t_j})\\
	  &=& \sum_{j=1}^{\infty} \nu[A_j].
	\end{eqnarray*}
	Finally, 
	\begin{eqnarray*}
	  \mathrm{E}[\exp(i\xi\nu[A_j])] &=& \mathrm{E}[\exp(i\xi(B_{t_{j+1}} - B_{t_{j}})]\\
	  &=& \exp(-\frac{(t_{j+1}-t_j)\xi^2}{2})
	\end{eqnarray*}
	Comparing with (\ref{sec:stbl}), we deduce the control measure must be $\frac{|\cdot|}{2}$. In fact, $\nu[A_j] \sim \Lambda(2, 0, 0, \frac{\sqrt{2|t_{j+1} - t_j|}}{2})$.
  \label{sec:ex2}
\end{example}

\subsection{Stable Integrals}
Samorodnitsky and Taqqu defined an integral with respect to stable measure as stochastic process in \cite{samorodnitsky}. 
\begin{definition}
  The \emph{stable integral} is given as follows
\begin{equation}
  \int_F f(x)\, \nu(\mathop{dx}) (\omega)
  \label{sec:stbint}
\end{equation}
with $f : F \rightarrow \mathbb{R}$ is a measurable function, given $\gamma\in (0, 2], \mu : \mathscr{D}\rightarrow \mathscr{B}(\mathbb{R}), \kappa: \mathbb{R}\rightarrow \mathbb{R}$, such that 
\begin{equation}
\begin{cases} \int_F |f(x)|^\gamma \mu(\mathop{dx}) < \infty,\hspace{1em} \text{if} \hspace{1em} \gamma \in (0, 1) \cup (1, 2],\\
	\int_F |\kappa(x) f(x) \ln|f(x)||\mu(\mathop{dx}) < \infty,\hspace{1em} \text{if} \hspace{1em} \gamma = 1,
  \end{cases}
\end{equation}
where $\gamma, \mu , \kappa$ are, respectively, \emph{degree}, \emph{control measure} and \emph{skewness intensity} of the stable measure $\nu$.
\end{definition}
Some properties of the stable function are given by Samorodnitsky and Taqqu.

\begin{proposition}
  Let $J(f)$ be a stable integral as form of (\ref{sec:stbint}). Then 
  \begin{equation*}
	J(f) \sim \Lambda(\gamma, \kappa, \theta, \delta)
  \end{equation*}
with degree, control measure, skewness intensity, respectively, 
\begin{eqnarray*}
\gamma &\in& (0, 2],\\
\kappa &=& \frac{\int_F \kappa(x) |f(x)|^\gamma\cdot sgn(f(x)) \mu(\mathop{dx})}{\int_F|f(x)|^\gamma\mu(\mathop{dx})},\\
\theta &=&
\begin{cases}
0 , \hspace{1em} \text{if} \hspace{1em} \gamma \in (0, 1) \cup (1, 2],\\
  -\frac{2}{\pi}\int_F \kappa(x) f(x) \ln|f(x)|\mu(\mathop{dx}), \hspace{1em} \text{if} \hspace{1em} \gamma = 1,
\end{cases}\\
\delta &=& \brkt{\int_F |f(x)^\gamma|\mu(\mathop{dx})}^{\frac{1}{\gamma}}, 
\end{eqnarray*}
of the stable measure $\nu$.
\label{sec:stbint2}
\end{proposition}
\begin{proof}
  See.\cite{samorodnitsky}, p.124, Proposition 3.4.1 .
\end{proof}
\begin{proposition}
  The stable integral is linear, in fact,
\begin{equation}
  J(c_1f_1 + c_2f_2) \overset{a.s.}{=}c_1J(f_1) + c_2J(f_2)
  \label{sec:stblin}
\end{equation}
for any $f_1, f_2$ integrable with respect to some stable measure and real numbers $c_1, c_2$. 
\end{proposition}
\begin{proof}
  See \cite{samorodnitsky}, p.117, Property 3.2.3 .
\end{proof}
\newpage
%---------- 3. section ------------%
\section{Fractional Brownian Motion}
\setcounter{equation}{0}
The fractional Brownian motion (fBm) was defined by Kolmogorov primitively. After that Mandelbrot and Van Ness has presented the work in detail. This section is concerned with the definition and some properties of it.

\subsection{Definition of Fractional Brownian Motion}
Mandelbrot and Van Ness \cite{mandelbrot} gave an integration representation of fBm.
\begin{definition}
  Let $(U_H(t))_{t\in \mathbb{R}}$ be a $\mathbb{R}$-valued stochatstic process and $H$ be a real number such that $0<H<1$. $(U_H(t))$ is said to be \emph{fractional Brownian motion} if 
  \begin{eqnarray}
	U_H(t) - U_H(s) &=& \frac{1}{\Gamma(H+\frac{1}{2})}\brkt{\int_{\mathbb{R}} \mathbbm{1}_{\{t > u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{s > u\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}}
	\label{sec:frtbm}
  \end{eqnarray}
  for $t\ge s, t, s \in \mathbb{R}$. Where $(B_u)$ is defined as two-sides Brownian motion and the integral is defined in sense of stable integral as in previous section. $H$ is called \emph{Hurst exponent} or \emph{Hurst index} of fBm.
\end{definition}

As usual, we set $U_H(0) = 0$, then equation (\ref{sec:frtbm}) is equivalent to
	 \begin{eqnarray}
	   U_H(t) &=& \frac{1}{\Gamma(H+\frac{1}{2})}\brkt{\int_{\mathbb{R}} \mathbbm{1}_{\{t > u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u < 0\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}}.
	\label{sec:fbm}
  \end{eqnarray}

%  We take (\ref{sec:fbm}) as definion of FBM now.
\begin{lemma}
  The equation (\ref{sec:fbm}) is well-defined, $U_H(t)$ has stable distribution and 
  \begin{equation*}
	U_H(t) \sim \Lambda(2, 0, 0, \frac{1}{\Gamma(H+\frac{1}{2})}(\int_{\mathbb{R}} |f(u)^2|\, \mathop{\frac{du}{2}})^\frac{1}{2}),
  \end{equation*}
  Where $f(x)$ is the integrand of integral in (\ref{sec:fbm}).
  \label{sec:l2}
\end{lemma}
\begin{proof}
  Firstly, $B_t$ is Gaussian and symmetric stable measure with zero mean and $\frac{|\cdot|}{2}$ is the control measure of it shown in Example \ref{sec:ex2}. \\
  Secondly, by $H=\frac{1}{2}$, $\int_{\mathbb{R}} f^2\frac{du}{2} = \frac{1}{2}\int_{0}^{|t|} \mathop{du} = \frac{1}{2}|t| < \infty$. By $H \neq \frac{1}{2}$, we deal it with Taylor expansion. As $u$ goes to $-\infty$, $f(u) = -(H-\frac{1}{2})(t-u)^{H-\frac{3}{2}}-(H-\frac{1}{2})(-u)^{H-\frac{3}{2}} + \mathrm{o}(1)$. Where $\mathrm{o}(1)$ tends to zero when $u$ is  around $-\infty$. Consider $H-\frac{3}{2} < 0$, then $f(u)$ is square integrable around $-\infty$. As $u$ goes to $t$, $f(u) \propto \mathbbm{1}_{\{t > u\}}(t-u)^{H-\frac{1}{2}}$. Hence, $f(u)$ is also square integrable around $u=t$. It is clear $f(u)=0$ when $u$ is around $\infty$. Then it satisfies the condition $\int_{-\infty}^{\infty}f^2(u)\,\mathop{\frac{du}{2}} < \infty$ .\\
  %the well-definition was refered to by Samorodnitksy and Taqqu in \cite{samorodnitsky}, p.321, Proposition 7.2.6 for not only $H= \frac{1}{2}$, but also $H\neq \frac{1}{2}$. Which satisfies, in other words, the condition $\int_{-\infty}^{\infty}f^2(u)\,\mathop{\frac{du}{2}} < \infty$.\\
  Finally, in terms of Proposition \ref{sec:stbint2}, we get the claim.
\end{proof}
It is worth mentioning that, if we take $H=\frac{1}{2}$ and choose a restriction of the integrand on $\mathbb{R}_+$, i.e., $U_{\frac{1}{2}}(t) = \frac{1}{\Gamma(H+\frac{1}{2})} B_t$ is a Brownian motion. 	
\begin{lemma}
  Let $(U_H(t))_t$ be a fBm. Then $U_H(t) \sim \mathcal{N}(0, \frac{1}{\Gamma(H+\frac{1}{2})^2}(\int_{\mathbb{R}} |f(u)^2|\, \mathop{du}))$.
  \label{sec:the1}
\end{lemma}
\begin{proof}
  In terms of Lemma (\ref{sec:l2}), $\mathrm{E}[i\xi U_H(t)] = \exp\{-\xi^2 \frac{1}{2\Gamma(H+\frac{1}{2})^2}(\int_{\mathbb{R}} |f(u)^2|\, \mathop{du})\}$. The rest is clear thanks to the form of characteristic function of a Gaussian random variable.
\end{proof}
Notice that we can also define $U_H(t)$ by (\ref{sec:fbm}) with It\'o integral. It has the same expected value and variance as defined by stable integral. Since $U_H(t)$ is Gaussian, both of two versions have the same distribution. Follwing properties remains true also by It\'o integral version.
\begin{corollary}
  $U_H(t)-U_H(s) \sim \mathcal{N}(0, \frac{1}{\Gamma(H+\frac{1}{2})^2}(\int_{\mathbb{R}} |f_t(u)-f_s(u)|^2\, \mathop{du})$
  \label{sec:the2}
\end{corollary}
\begin{proof}
  This Corollary follows Proposition \ref{sec:stblin} and Lemma \ref{sec:the1}.
\end{proof}
\begin{theorem}
  Let $(U_H(t))_t$ be a fBm. Then $U_H(t)$ has an expected value $0$ and variance $\frac{1}{(\Gamma(H+\frac{1}{2}))^2}t^{2H}\, \mathrm{E} U^2_H(1)$ for any $t \in \mathbb{R}$.
  \label{sec:fbmp1}
\end{theorem}
\begin{proof}
  It is clear that $U_H$ is Gaussian with zero mean due to Lemma \ref{sec:l2}. We suppose that $t \ge s \ge 0,  c(H)=\frac{1}{(\Gamma(H+\frac{1}{2}))^2}$.
  \begin{eqnarray}
	&&\mathrm{E}[(U_H(t) - U_H(s))^2]\nonumber\\
%	&=& c(H)\mathrm{E}[\brkt{\int_{\mathbb{R}} [\mathbbm{1}_{\{t\ge u\}}\cdot(t-u)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{s\ge u\}}\cdot(s-u)^{H-\frac{1}{2}} \,\mathop{U_H(u)}}^2]\nonumber\\
	&\overset{\text{Corollary \ref{sec:the2}}}{=}& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{[\mathbbm{1}_{\{t > u\}}\cdot(t-u)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{s > u\}}\cdot(s-u)^{H-\frac{1}{2}}}^2 \,\mathop{du}]\nonumber\\
	&=& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{\mathbbm{1}_{\{t-s > u\}}\cdot(t-s-u)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{0 > u\}}\cdot(-u)^{H-\frac{1}{2}}}^2 \,\mathop{du}]\nonumber\\
	&\overset{m=t-s}{=}& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{\mathbbm{1}_{\{m > u\}}\cdot(m-u)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{0 > u\}}\cdot(-u)^{H-\frac{1}{2}}}^2 \,\mathop{du}]\nonumber\\
	&\overset{u=ml}{=}& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{\mathbbm{1}_{\{m > ml\}}\cdot(m-ml)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{0 > ml\}}\cdot(-ml)^{H-\frac{1}{2}}}^2 m\cdot\,\mathop{dl}]\nonumber\\
	&=& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{\mathbbm{1}_{\{1 > l \}}\cdot(1-l)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{0 > l\}}\cdot(-l)^{H-\frac{1}{2}}}^2 \cdot m^{2H-1}\cdot m\,\mathop{dl}]\nonumber\\
	&=& c(H)m^{2H}\mathrm{E}[U_H(1)^2]\nonumber\\
	&=& c(H)(t-s)^{2H}\mathrm{E}[U_H(1)^2]
	\label{sec:eqn1}
  \end{eqnarray}
  Using the same calculation, we get
  \begin{equation}
	\mathrm{E}[(U_H(t)^2] = c(H)t^{2H}\mathrm{E}[U_H(1)^2].
	\label{sec:eqn2}
  \end{equation}
%  $c(H), t^{2H}, \mathrm{E}[U_H(1)^2]$ are nonegative, then
%  $$\mathrm{E}[(U_H(t)] = c(H)^{\frac{1}{2}}t^{H}\mathrm{E}[U_H(1)].$$

%  Because of (\ref{sec:eqn1}),
%  $$
%	\mathrm{E}[(U_H(2) - U_H(1))^2] = c(H)\mathrm{E}[U_H(1)^2].
%  $$
%  And again
%  $$
%  \mathrm{E}[(U_H(2) - U_H(1))] = c(H)^{\frac{1}{2}}\mathrm{E}[U_H(1)]
%  $$
%Conserquently,
%\begin{eqnarray}
%	0 &=& \mathrm{E}[(U_H(2)(U_H(2)-2U_H(1)))]\nonumber\\
%	&\overset{Cauchy-Schwartz}{\le}& (\mathrm{E}[U_H(2)^2]\cdot\mathrm{E}[(U_H(2)-2U_H(1))^2])^{\frac{1}{2}}\nonumber
% \end{eqnarray}
% WLOG, let $\mathrm{E}[U_H(2)^2] \neq 0$, otherwise $\mathrm{E}[U_H(1)] = 0$ due to . Then we get 
%  $$
%  \mathrm{E}[(U_H(2)] = 2\mathrm{E}[(U_H(1)]
%  $$
%  \begin{eqnarray}
%	(1+c(H)^{\frac{1}{2}})\mathrm{E}[((U_H(1))] &=& \mathrm{E}[((U_H(2))] \nonumber\\
%	&=&  2\mathrm{E}[((U_H(1))]\nonumber
%  \end{eqnarray}
%  that means $\mathrm{E}[((U_H(1))] = 0$. Due to (\ref{sec:eqn2}), $\mathrm{E}[(U_H(t)] = 0$ for $t\ge 0$.
%  Therefore, (\ref{sec:eqn2}) is variance of $U_H(t)$.
  (\ref{sec:eqn2}) is variance of $U_H(t)$ due to $\mathrm{E}[(U_H(t)] = 0$.
\end{proof}
In order to normalize the variance, a definition of standard fBm is given.

\begin{definition}
  A stochastic process $(U_H(t))_{t}$ is said to be a \emph{standrad fractional Brownian motion} (sfBm) if
  \begin{equation}
U_H(t) = \hat{c}(H) \int_{\mathbb{R}} \mathbbm{1}_{\{t\ge u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u\le 0\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}.
\label{sec:eqn3}
\end{equation}
Where $\hat{c}(H) = \frac{1}{(\Gamma(H+\frac{1}{2})^2)\mathrm{E}[U_H(1)^2]} $.
\end{definition}
We consider from now on sfBm instead of fBm.

\begin{theorem}
  Let  $(U_H(t))_{t}$ be a fBm. The covariance of $U_H(t)$ and $U_H(s)$ is $ \frac{1}{2}(t^{2H} + s^{2H} - |t-s|^{2H})$ for $t, s \in \mathbb{R}$.
\end{theorem}

\begin{proof}
  Cf.\cite{mandelbrot}, Theorem 5.3 .
  \begin{eqnarray}
	\mathrm{Cov}[U_H(t), U_H(s)] &=& \mathrm{E}[U_H(t)U_H(s)] \nonumber\\
	&=& \frac{1}{2}\brkt{\mathrm{E}[U_H(t)^2] + \mathrm{E}[U_H(s)^2] - \mathrm{E}[(U_H(t) - U_H(s))^2]} \nonumber\\
	&\overset{(\ref{sec:eqn2})}{=}& \frac{1}{2}(t^{2H} + s^{2H} - |t-s|^{2H})
	\label{sec:eqn4}
  \end{eqnarray}
\end{proof}

\begin{theorem}
  $(U_H(t))_{t}$ is Gaussian process.
\end{theorem}

\begin{proof}
  We just need to prove that for any finite linear combination of $(U_H(t))_t$ is Gaussian. We take $t_1, \dots, t_k \in \mathbb{R}, c_1, \dots, c_k \in \mathbb{R}$ and the stable integral $J(f) $ is a linear functional with $\gamma=2, \kappa=0, \theta=0, \delta= (\frac{1}{2}\int_{-\infty}^{\infty}f^2(u)\,\mathop{du})^{\frac{1}{2}}$ due to Corollary \ref{sec:stblin}. Suppose $f_1,\dots, f_k$ are integrands of stable integration of $U_H(t_1), \dots, U_H(t_k)$ respectively. \\
  Consider now, according to the Minkowski inequality,
  \begin{eqnarray*}
	\int_{-\infty}^{\infty}(\sum_{j=1}^k c_jf_j)^2\,\mathop{du} &\le& \sum_{j=1}^k \underbrace{\int_{-\infty}^{\infty}(c_jf_j)^2\,\mathop{du}}_{<\infty}\\
	&<& \infty.
  \end{eqnarray*}
Moreover,
  \begin{eqnarray*}
	\sum_{j=1}^k c_jU_H(t_j) &=& \sum_{j=1}^k c_jJ(f_j)\\
	&=& J(\sum_{j=1}^k c_jf_j)\\
	&\sim& \Lambda(2, 0, 0, (\frac{1}{2}\int_{-\infty}^{\infty}(\sum_{j=1}^k c_jf_j)^2\,\mathop{du})^{\frac{1}{2}})
  \end{eqnarray*}
  is Gaussian and the rest follows from Corollary \ref{sec:gauss}.
\end{proof}

\begin{corollary}
  Let $(U_H(t))_{t}$ be a fBm, then $(U_H(t))_{t}$ has stationary and H-self similar increments . 
\end{corollary}
\begin{proof}
  Assume that $s \ge u $. Because the joint distribution of $(U_H(s), U_H(u))^T$ is Gaussian, $(1, -1) \cdot (U_H(s), U_H(u))^T $ is also Gaussian. From (\ref{sec:eqn1}),  $U_H(t_k+\tau) - U_H(s_k+\tau) \sim U_H(t_k) - U_H(s_k) \sim \mathcal{N}(0, (t_k - s_k)^{2H})$ for $k \in \{1\dots d\}$. Corresponding to  (\ref{sec:mcf}), 
	\begin{eqnarray*}
	\mathrm{E}[i\sum_{k=1}^{d}\xi_k (U_H(t_k + \tau) - U_H(s_k + \tau))] = \mathrm{E} [i\sum_{k=1}^{d}\xi_k (U_H(t_k) - U_H(s_k))]
  \end{eqnarray*}
  due to $(U_H(t_{k+\tau} - s_{k+\tau}))_{k=1}^{d}$ and $(U_H(t_k - s_k))_{k=1}^{d}$ have the same expected vector and covariance matrix in their characteristic function, i.e., $(U_H(t))$ has stationary increments.\\
  % $(U_H(t))$ has zero mean and $\mathrm{Var[U_H(s)]} = s^{2H}\mathrm{Var}[U_H(1)]$ we get $U_H(s) \sim s^HU_H(1)$ due to it is Gaussian.
   In order to show fBm has H-self similar increments, we have to prove\\ $(U_H(zt_1), U_H(zt_2),\dots, U_H(zt_n))) \sim (z^HU_H(t_1), z^HU_H(t_2),\dots, z^HU_H(t_n))$ for any $z > 0$. Obviously, the former and the latter of the term is Gaussian and $\mathrm{Var}[U_H(zt_i), U_H(zt_j)] = \mathrm{Var}[z^HU_H(t_i), z^HU_H(t_j)] = \frac{1}{2}z^{2H}(t_i^{2H} + t_j^{2H} - |t_i-t_j|^{2H})$. Thus they have the same expected vector and covariance matrix in their characteristic function. In the same way as mentioned above, we get our claim.
\end{proof}

\subsection{Regularity}
\begin{theorem}[Kolmogorov Chentsov]
  FBM has almost surely continuous sample path.  
\end{theorem}

\begin{proof}
  Cf.\cite{mandelbrot}, Proposition 4.1 . Let $(U_H(t))_{t}$ be a FBM with Hurst index $H$. Fix $\alpha$ such that $1 < \alpha H$. Let us have a look at the expectation of $(U_H(t) - U_H(s))^\alpha$ with respact to the calculation in (\ref{sec:eqn1})
  \begin{eqnarray}
	\mathrm{E} [(U_H(t) - U_H(s))^\alpha] &=& |t-s|^{\alpha H} \cdot \underbrace{\mathrm{E}\brkt{\int_{\mathbb{R}} \mathbbm{1}_{\{1 > u\}}\cdot (1-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u < 0\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}}^\alpha}_{c(\alpha,H)}\nonumber\\
	&=& c(\alpha,H) \cdot |t-s|^{\alpha H}.
	\label{sec:eqn5}
  \end{eqnarray}
  We choose $\beta = \alpha H -1$ and $\gamma \in (0, H-\frac{1}{\alpha})$ then the rest follows from Theorem \ref{sec:kolch} .
\end{proof}

Remark, $(U_H(t))_t$ is, in fact, $\gamma$-H\"older continuous with $\gamma < H$ almost surely.

\begin{theorem}
  The sample path of FBM is almost surely not differentiable.
\end{theorem}

\begin{proof}
  Cf. \cite{mandelbrot} Proposition 4.2 . Fix $\omega \in \Omega$, we assume $c > 0, t_j \rightarrow s$.
\begin{eqnarray}
 && \mathcal{P}[\limsup\limits_{t\rightarrow s} |\frac{U_H(t) - U_H(s)}{t-s}| > c]\nonumber\\
 &=& \mathcal{P}[\lim\limits_{j\rightarrow\infty}\sup\limits_{t_j\neq s} |\frac{U_H(t_j) - U_H(s)}{t_j-s}| > c]\nonumber\\
 \label{sec:mass}
 \end{eqnarray}
 Since continuity of measures from above, then
 \begin{eqnarray*}
 \text{(\ref{sec:mass})} &=& \lim\limits_{j\rightarrow\infty} \mathcal{P}[\sup\limits_{t_j\neq s} |\frac{U_H(t_j) - U_H(s)}{t_j-s}| > c]\\
 &\ge& \lim\limits_{j\rightarrow\infty} \mathcal{P}[|\frac{U_H(t_j) - U_H(s)}{t_j-s}| > c]\\
 &=& \lim\limits_{j\rightarrow\infty} \mathcal{P}[\frac{|(t_j-s)^{H}U_H(1)}{t_j - s}| > c]\\
 &=& \lim\limits_{j\rightarrow\infty} \mathcal{P}[|(t_j-s)^{H-1}U_H(1)| > c]\\
 &=& \lim\limits_{j\rightarrow\infty} \mathcal{P}[|U_H(1)| > \underbrace{|t_j - s|^{1-H}}_{\overset{j\rightarrow\infty}{\longrightarrow}\, 0}c]\\
 &\overset{j\rightarrow \infty}{\longrightarrow}& 1 
\end{eqnarray*}
\end{proof}

\begin{theorem}
  Let $(U_H(k))_{k}$ be a fBm. The conditional expectation of $U_H(s)$  given $U_H(t)=x$ is
\begin{equation*}
  \frac{|\frac{s}{t}|^{2H} + 1 - |\frac{s}{t} - 1|^{2H}}{2} \cdot x
\end{equation*}
for all $ s \ge t$ and $t \neq 0$.
\end{theorem}
\begin{proof}
  Cf. \cite{mandelbrot} Theorem 5.3. Taking conditional expectation of $U_H(s)$ given $U_H(t)$,
  \begin{eqnarray*}
  &&\mathrm{E}[U_H(s)|U_H(t)]\\
  &\overset{\text{Corollary \ref{sec:condi}}}{=}& \mu_s + \rho_{s,t} (\frac{\sigma_s}{\sigma_t}U_H(t) - \mu_t)\\
  &=& \rho_{s,t}\frac{\sigma_s}{\sigma_t}U_H(t)\\
  &=& \frac{\rho_{s,t}\cdot\sigma_s\sigma_t\cdot U_H(t)}{\sigma_t^2}\\
  &=& \frac{\mathrm{E}[U_H(s)U_H(t)]}{\mathrm{E}[U_H^2(t)]} \cdot U_H(t)\\
  &\overset{\text{(\ref{sec:eqn4})}}{=}& \frac{s^{2H} + t^{2H} - |s-t|^{2H}}{2\mathrm{E}[U_H^2(t)]}\cdot U_H(t)\\
  &=&  \frac{s^{2H} + t^{2H} - |s-t|^{2H}}{2t^{2H}}\cdot U_H(t)\\
  &=& \frac{|\frac{s}{t}|^{2H} + 1 - |\frac{s}{t} - 1|^{2H}}{2} \cdot U_H(t)
  \end{eqnarray*}
\end{proof}

\subsection{Fractional Brownian Noise}
\begin{definition}
  Let $(U_H(t))_{t\in \mathbb{R}}$ be a fBm. The \emph{fractional Brownian noise} is a sequence $(S_k)_{k\in \mathbb{R}}$ defined as follows
\begin{eqnarray*}
  S_H(k) &=& U_H(k+1) - U_H(k)
\end{eqnarray*}
for $k \in \mathbb{R}$.
\end{definition}
\begin{proposition}
  Fractional Brownian noise is stationary and its autocovariance is 
  \begin{eqnarray}
	\varsigma_{S_H}(\tau) &=&  \frac{1}{2} (|\tau + 1|^{2H} - 2|\tau|^{2H} + |\tau-1|^{2H})
  \label{sec:auto}
  \end{eqnarray}
  for $\tau \in \mathbb{R}$.
\end{proposition}

\begin{proof}
  Cf. \cite{nourdin}, p.333, Proposition 7.2.9 .\\
  The first part of the claim is clear due to fBm has stationary increments.\\
  In terms of definition of fractional Brownian noise, for a $k \in \mathbb{R}$, we have
  \begin{eqnarray*}
	&&\varsigma_{S_H}(\tau)\\
	&=& \mathrm{E}[S_H(k+\tau)S_H(k)] \\
	&=& \mathrm{E}[(U_H(\tau+k+1) - U_H(\tau+k))(U_H(k+1)-U_H(k))]\\
	&=& \mathrm{E}[U_H(\tau+k+1)U_H(k+1)] - \mathrm{E}[U_H(\tau+k+1)U_H(k)] \\
	&-& \mathrm{E}[U_H(\tau+k)U_H(k+1)] + \mathrm{E}[U_H(\tau+k)U_H(k)]\\
	&\overset{\text{(\ref{sec:eqn4})}}{=}& \frac{1}{2} ( (\tau+k+1)^{2H} + (k+1)^{2H} - |\tau|^{2H} - (\tau+k+1)^{2H} - k^{2H} + |\tau+1|^{2H}\\
	&-& (\tau+k)^{2H} - (k+1)^{2H} + |\tau-1|^{2H}+ (\tau+k)^{2H} + k^{2H} - |\tau|^{2H})\\
	&=& \frac{1}{2}(|1+\tau|^{2H} - 2|\tau|^{2H} + |1-\tau|^{2H})
  \end{eqnarray*}
  for $\tau \in \mathbb{R}$.
\end{proof}


\begin{definition}
  A stationary stochastic process $(X_t)_t$ is said to have \emph{long memory} if its autocovariance $\varsigma_X(\tau)$ tends to $0$ so slowly such that
  $ \sum_{\tau = 0} ^{\infty} \varsigma_X(\tau)$ diverges.
\end{definition}

\begin{lemma}[Cauchy Condensation test]
  Let $(a_n)$ be a $\mathbb{R}$-valued positive non-increasing sequence. Then $\sum_{n=1}^{\infty} a_n$ converges if and only if $\sum_{n=0}^{\infty} 2^n a_{2^n}$ converges.
  \label{sec:cauchy}
\end{lemma}
\begin{proof}
  See \cite{michael}, p.391, Theorem 13.13 .
\end{proof}

\begin{lemma}[Limit comparison test]
  Let $\sum_{k=0}^{\infty} a_k$ and $\sum_{k=0}^{\infty} b_k$ be two series with $a_k \ge 0, b_k \ge 0$ for all $k$. If $0 <\lim\limits_{k\rightarrow \infty}\frac{a_k}{b_k} < \infty$, then either both series converge or both series diverge.
\end{lemma}

\begin{proof}
  Suppose $\lim\limits_{k\rightarrow \infty}\frac{a_k}{b_k}=c$ with $c<\infty$. Then there exists a $N$ such that if $k > N, |\frac{a_k}{b_k} - c| < \frac{c}{2}$. In other words, 
  \begin{eqnarray*}
	\frac{c}{2}|b_k| < |a_k| < \frac{3c}{2}|b_k|
  \end{eqnarray*}
  Hence, if $\sum_{k=0}^{\infty} a_k$ converges, then $\sum_{k=0}^{\infty} b_k$ converges due to the first '$<$'. Also, if $\sum_{k=0}^{\infty} b_k$ converges then $\sum_{k=0}^{\infty} a_k$ converges due to the last '$<$'. One can easy to verify the claim of the divergence in the same way.
\end{proof}
\begin{theorem}
  The fractional Brownian noise with $H \in (\frac{1}{2}, 1)$ has long memory.
  \label{sec:lmemory}
\end{theorem}
\begin{proof}
 % Cf. \cite{chriel}, p.2, (1.2).\\
 % Using the Newton's generalised binomial formula, 
 % \begin{eqnarray*}
%	&&\varsigma_{S_H}(\tau) \\
%	&=& \frac{1}{2}((1+\tau)^{2H} - 2\tau^{2H} + (1-\tau)^{2H})\\
%	&=& \sum\limits_{k=0}^{\infty} {2H \choose k} \tau^{2H-k} 1^k + \sum\limits_{k=0}^{\infty} {2H \choose k} \tau^{2H-k} (-1)^k - 2\tau^{2H}\\
%	&=& \brkt{\tau^{2H} + 2H \tau^{2H-1} + \frac{2H(2H-1)}{2!}\tau^{2H-1}+\dots)} \\
%	&+&  \brkt{\tau^{2H} - 2H \tau^{2H-1} + \frac{2H(2H-1)}{2!}\tau^{2H-1}+\dots)} - 2\tau^{2H}\\
%	&=&  \sum\limits_{k=1}^{\infty} \frac{1}{(2k)!}\brkt{\prod_{j=0}^{2n-1}(2H-j)}\tau^{2H-2k}
%  \end{eqnarray*}
%  Since $2H < 2k$ for $k\in \mathbb{N}_0$, 
%  \begin{eqnarray*}
%	&&\varsigma_{S_H}(\tau) \\
%	&=& \sum\limits_{k=1}^{N} \frac{1}{(2k)!}\brkt{\prod_{j=0}^{2n-1}(2H-j)}\tau^{2H-2k} + \mathrm{o}(\tau)
%  \end{eqnarray*}
%  for $N\in \mathbb{N}_0$.
  Cf. \cite{nourdin}, p.335, Proposition 7.2.10 .\\
  Without loss of generality, we suppose $\tau \in \mathbb{N}$ because $\varsigma_{S_H}(0)=1$. 
  \begin{eqnarray*}
	&&\varsigma_{S_H}(\tau) \\
  &=& \frac{1}{2} \tau^{2H-2}\{ \tau^2[(1+\frac{1}{\tau})^{2H} - 2 + (1-\frac{1}{\tau})^{2H}] \} \\
  &=& \frac{1}{2} \tau^{2H-2}\{ \frac{(1+\frac{1}{\tau})^{2H} - 1 } {\frac{1}{\tau^2}} -  \frac{1 - (1-\frac{1}{\tau})^{2H}} {\frac{1}{\tau^2}} \}
\end{eqnarray*}

We deal with the former of the content in $\{\,\}$ with L'H\^opital's rule as $\tau$ tends to infinity.
\begin{eqnarray*}
  &&\frac{(1+\frac{1}{\tau})^{2H} - 1 } {\frac{1}{\tau^2}} \\
&=& \frac{2H(1+\frac{1}{\tau})^{2H-1}(-\frac{1}{\tau^2})}{-\frac{2}{\tau^3}} + \mathrm{o}(1)\\
&=& \frac{H(1+\frac{1}{\tau})^{2H-1}}{\frac{1}{\tau}} + \mathrm{o}(1)
\end{eqnarray*}

We calculate the Latter of the content in $\{\,\}$ in a similar way. Then
\begin{eqnarray*}
  &&\varsigma_{S_H}(\tau)\\
&=& \tau^{2H-2} \frac{1}{2}\{ \frac{H(1 +\frac{1}{\tau})^{2H-1}}{\frac{1}{\tau}} - \frac{H(1-\frac{1}{\tau})^{2H-1}}{\frac{1}{\tau}}\}+ \mathrm{o}(1)\\
&=& \tau^{2H-2} \frac{1}{2}\{ \frac{H(1 +\frac{1}{\tau})^{2H-1}- H}{\frac{1}{\tau}} - \frac{H(1-\frac{1}{\tau})^{2H-1} - H}{\frac{1}{\tau}}\}+ \mathrm{o}(1)\\
&\overset{\text{L'H\^opital}}{=}& \frac{1}{2} \tau^{2H-2}\{\frac{H(2H-1)(1+\frac{1}{\tau})^{2H-2}(-\frac{1}{\tau^2})}{-\frac{1}{\tau^2}} - \frac{H(2H-1)(1-\frac{1}{\tau})^{2H-2}\frac{1}{\tau^2}}{-\frac{1}{\tau^2}}\}+ \mathrm{o}(1)\\
&=& \frac{1}{2} \tau^{2H-2}\{H(2H-1)(1+\frac{1}{\tau})^{2H-2} + H(2H-1)(1-\frac{1}{\tau})^{2H-2}\}+ \mathrm{o}(1)\\
&=& \frac{1}{2} \tau^{2H-2} 2H(2H-1)+ \mathrm{o}(1)\\
&=& H(2H-1) \tau^{2H-2}+ \mathrm{o}(1)
\end{eqnarray*}


%In fact, for $\tau$ is large,
%\begin{eqnarray*}
%  \frac{\varsigma(\tau+1)}{\varsigma(\tau)} &=& (\frac{\tau+1}{\tau})^{2H-2}
%\end{eqnarray*}

When $H \in (0, \frac{1}{2})$, $\sum\limits_{\tau=1}^{\infty} \tau^{2H-2}$ converges.  We use the Cauchy condesation test to verify it
 \begin{eqnarray*}
&& \sum_{\tau=0}^{\infty} 2^{\tau} (2^\tau)^{2H-2} \\
&=& \sum_{\tau=0}^{\infty} 2^{\tau(2H-1)}
\end{eqnarray*}
This is geometric series if $2H-1<0$, namely, $H\in(0,\frac{1}{2})$.  \\
Otherwise, if $\frac{1}{2} < H < 1$, namely, $-1<2H-2<0, \sum\limits_{\tau=1}^{\infty} \tau^{2H-2} $ diverges because it is greater than the harmolic series.\\
$H(2H-1) > 0$ when $H\in (\frac{1}{2}, 1)$. Since the limit comparison test, $\sum\limits_{\tau=1}^{\infty} \varsigma(\tau)$ diverges because $\sum\limits_{\tau=1}^{\infty} \tau^{2H-2} $ diverges. It is clear that $\lim\limits_{\tau\rightarrow \infty}\varsigma_{S_H}(\tau)=0$, then claim is proved. 
\end{proof}

\begin{corollary}
  Let $S_H$ be fractional Brownian noise and $\varsigma_{S_H}(\cdot)$ be its autocovariance. Then $\sum_{\tau=0}^{\infty}\varsigma^2_{S_H}(\tau)<\infty$ if and only if $H < \frac{3}{4}$.
\end{corollary}
\begin{proof}
  Cf. \cite{nourdin}, p.72, Lemma 6.3. As by Theorem \ref{sec:lmemory}, we have $\varsigma_{S_H}^2(\tau) = H^2(2H-1)^2\tau^{4H-4} + \mathrm{o}(1) $. The sum of it over the range of $\tau$ is finite if and only if, according to the same reason as in Theorem \ref{sec:lmemory}, $4H<3$. That means $H < \frac{3}{4}$.
\end{proof}
\subsection{fBm is not Semimartingale for $H\neq \frac{1}{2}$}
Let us have a look at our integration representation for fBm. In the case of fBm with Hurst index $\frac{1}{2}$, it must be an ordinary Brownian motion. Otherwise, we'll show fBm is not a seimimartingale.

\begin{definition}
  The \emph{Hermite polynomials} forms as following
  \begin{equation}
	H_n(u) = (-1)^n e^{\frac{u^2}{2}} (\frac{\partial^n }{\partial u^n}e^{-\frac{u^2}{2}}),
	\label{sec:hermite}
  \end{equation}
  for $u\in \mathbb{R}, n\in \mathbb{N}_0$. 
\end{definition}
\begin{proposition}
  Let $(H_n)_{n\in \mathbb{N}_0}$ be a family of Hermite polynomials, $f, g: \mathbb{R}\rightarrow \mathbb{R}$ continuously differentiable. It has then following properties 
  \begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
	\item  $H_{n+2}(u)=u\cdot H_{n+1}(u) - (n+1)H_n(u)$  and $H_{n+1}(u)= (n+1)H^{'}_n(u)$ for all $n\in\mathbb{N}_0, u\in\mathbb{R}$.
%	\item The family $\{\frac{1}{\sqrt{n!}}\eta_n\}_{n\in \mathbb{N}}$ is an orthonormal basis of $\mathcal{L}^2(\mathbb{R}, \frac{1}{\sqrt{2\pi}}e^{-\frac{u^2}{2}}\, du)$
	\item Let $W, V$ be standard Gaussian such that $(W, V)$ have a disjoint Gaussian distribution. Then
	  \begin{equation*}
		\int_{\Omega} H_j(W)\cdot H_k(V) \mathop{\mathcal{P}} = \begin{cases} j!\,(\mathrm{E}[WV])^j & \mbox{if } j=k,\\
		  0 &\text{otherwise}.
		\end{cases}
	  \end{equation*}
	\item Let $W$ be standard Gaussian distributed, then
	  \begin{equation*}
		\frac{1}{j!} \int_\Omega H_j(W) H_k(W) \mathop{\mathcal{P}} = \begin{cases} 1 & \mbox{if } j=k,\\
		  0 & \text{otherwise}.
		\end{cases}
	  \end{equation*}
\end{enumerate}
Remark that, $(iii)$ means the fact, $\{\frac{1}{\sqrt{j!}}\cdot H_j(x)\}_{j=0}^{\infty}$ is an orthonormal basis in $\mathcal{L}^2(\mathbb{R}, \mathscr{B}(\mathbb{R}), e^{-\frac{x^2}{2}}\mathop{dx})$.
		\label{sec:herpro}
\end{proposition}
\begin{proof}
  See \cite{nourdin} p.3, Propostion 1.3.
\end{proof}

\begin{lemma}
  Let $(U_H(t))_{t}$ be a fBm, $W$ standrad Gaussian variable, and $f: \mathbb{R}\rightarrow \mathbb{R}$, Borel-measurable function such that $\mathrm{E}[f^2(W)] < \infty$. Then, 
  \begin{equation*}
	\frac{1}{n}\sum\limits_{j=1}^n f(U_H(j)-U_H(j-1)) \overset{\text{in }\mathcal{L}^2(\mathcal{P})}{\rightarrow} \mathrm{E}[f(W)],
  \end{equation*}
  as $n$ tends to $\infty$. In particular,
  \begin{equation}
	\sum_{j=1}^n |U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^\beta \overset{\text{in }\mathcal{L}^2(\mathcal{P})}{\rightarrow} \begin{cases}
	  0 &\mbox{if } \beta > \frac{1}{H}\\
	  \mathrm{E}[|W|^\beta] &\mbox{if } \beta = \frac{1}{H}\\
	  \infty &\mbox{if } \beta < \frac{1}{H}
	\end{cases}
	\label{sec:semilemma}
  \end{equation}
  as $n$ tends to $\infty$.
\end{lemma}
\begin{proof}
  C.f. \cite{nourdin}, p.17, Theorem 2.1 .\\
  Firstly, because $\mathrm{E}[f^2(W)] < \infty$, one has $f \in \mathcal{L}^2(\mathbb{R}, \mathscr{B}(\mathbb{R}), e^{-\frac{x^2}{2}}\mathop{dx})$. In terms of Proposition \ref{sec:herpro}(iii), taking expectation
 \begin{equation*}
	\mathrm{E}[f(x)] = \mathrm{E}[\sum\limits_{j=0}^{\infty} \frac{a_jH_j(x)}{\sqrt{j!}}], 
  \end{equation*}
  for $x \in \mathbb{R}$.
  Notice $H_0(u)=1$ for $u\in \mathbb{R}$ due to (\ref{sec:hermite}). Setting $x=W$, equalling coefficients leads to $a_0 = \mathrm{E}[f(W)]$. Moreover,
  \begin{eqnarray*}
	&& \mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n f(U_H(j) - U_H(j-1)) - \mathrm{E}[f(W)]\}^2]\\
	&=& \mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n (f(U_H(j) - U_H(j-1)) - \mathrm{E}[f(W)])\}^2] \\
	&=& \mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n (\sum\limits_{k=0}^\infty \frac{a_k}{\sqrt{k!}} H_k(U_H(j) - U_H(j-1))) - \mathrm{E}[f(W)]\}^2] \\
	&=& \mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n (\sum\limits_{k=1}^\infty \frac{a_k}{\sqrt{k!}} H_k(U_H(j) - U_H(j-1)))\}^2] \\
  \end{eqnarray*}
  Consider now 
  \begin{equation*}
	\mathrm{E}[f^2(W)] < \infty,
  \end{equation*}
  which requires $\sum\limits_{k=1}^\infty (a_k)^2 < \infty$. Then
  \begin{eqnarray*}
	&& \mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n f(U_H(j) - U_H(j-1)) - \mathrm{E}[f(W)]\}^2]\\
	&=& \frac{1}{n^2} \mathrm{E}[\sum\limits_{k=1}^\infty \frac{a_k^2}{k!}(\sum\limits_{j=1}^n H_k(U_H(j) - U_H(j-1)))^2] \\
	&=& \frac{1}{n^2} \sum\limits_{k=1}^\infty \frac{a_k^2}{k!}\sum\limits_{j=1, m=1}^n \mathrm{E}[H_k(U_H(j) - U_H(j-1))  H_k(U_H(m) - U_H(m-1)]\\
	&\overset{\text{Proposition \ref{sec:herpro}(ii)}}{=}&  \frac{1}{n^2} \sum\limits_{k=1}^\infty a_k^2\sum\limits_{j=1, m=1}^n (\mathrm{E}[(U_H(j) - U_H(j-1)) (U_H(m) - U_H(m-1))])^k \\
	&=& \frac{1}{n^2} \sum\limits_{k=1}^\infty a_k^2\sum\limits_{j=1, m=1}^n (\mathrm{E}[S_H(j-1) S_H(m-1)])^k\\
	&=& \frac{1}{n^2} \sum\limits_{k=1}^\infty a_k^2\sum\limits_{j=1, m=1}^n (\varsigma_{S_H}(j-m))^k
  \end{eqnarray*}
	Notice that,
	\begin{eqnarray*}
	  |\varsigma_{S_H}(k)| &=& |\varsigma_{S_H}(|k|)|\\
	  &=& \mathrm{E}[(U_H(1) - U_H(0))(U_H(|x|+1)-U_H(|x|))]\\
	  &\overset{\text{Cauchy Schwartz}}{\le}& \underbrace{\sqrt{\mathrm{E}[U_H(1)^2]}}_{= 1}\cdot \underbrace{\sqrt{\mathrm{E}[U_H(|k|+1) - U_H(|k|)]^2}}_{= 1}\\
	  &=& 1
	\end{eqnarray*}
	Consequently, $(\varsigma_{S_H}(j-m))^k \le |\varsigma_{S_H}(j-m)|$. In fact,
	\begin{eqnarray*}
	  && \mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n f(U_H(j) - U_H(j-1)) - \mathrm{E}[f(W)]\}^2]\\
	  &\le& \frac{1}{n^2} \underbrace{\sum\limits_{k=1}^\infty a_k^2}_{=:\alpha < \infty}\sum\limits_{j=1, m=1}^n |\varsigma_{S_H}(j-m)|\\
	  &=& \frac{\alpha}{n^2} \sum\limits_{j=1, m=1}^n |\varsigma_{S_H}(j-m)|\\
	  &=& \frac{\alpha}{n^2} 2 \cdot \sum\limits_{j=1}^n  \sum\limits_{m<j} |\varsigma_{S_H}(j-m)|\\
	  &\le& \frac{\alpha}{n^2} 2n \sum\limits_{k=1}^{n-1} |\varsigma_{S_H}(k)|\\
	  &=& \frac{2\alpha}{n}  \sum\limits_{k=1}^{n-1} |\varsigma_{S_H}(k)|,
	\end{eqnarray*}
	As in the proof in Theorem \ref{sec:lmemory}, 
	\begin{eqnarray*}
	&&\sum\limits_{k=1}^{n-1} |\varsigma_{S_H}(k)| \\
	&\propto& H(2H-1)\sum\limits_{k=1}^{n-1}k^{2H-2} \\
	&\propto& H(2H-1)n\cdot n^{2H-2} \\
	&\propto& n^{2H-1}, 
  \end{eqnarray*}
  as n goes to infinity. Then
	\begin{eqnarray*}
	  \frac{2\alpha}{n} \sum\limits_{k=1}^{n-1} |\varsigma_{S_H}(k)| \propto n^{2H-2}
	\end{eqnarray*}
	as n goes to infinity. This leads to,  for $0<H<1$, as $n \rightarrow \infty$,  $\mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n f(U_H(j) - U_H(j-1)) - \mathrm{E}[f(W)]\}^2]\rightarrow 0$ due to $n^{2H-2}\rightarrow 0$. I.e., $\frac{1}{n}\sum\limits_{j=1}^n f(U_H(j) - U_H(j-1)) \overset{\text{in} \mathcal{L}^2}{\rightarrow} \mathrm{E}[f(W)]$.\\

	Secondly, we apply previous result for (\ref{sec:semilemma}), in fact,
	\begin{eqnarray*}
		&&\sum_{j=1}^n |U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^\beta\\
		&=& \frac{1}{n^{\beta H}} \sum_{j}^n|U_H(j) - U_H(j-1)|^\beta\\
		&=& \frac{1}{n^{\beta H-1}} \frac{1}{n}\sum_{j}^n|U_H(j) - U_H(j-1)|^\beta\\
		&\overset{\text{in }\mathrm{L}^2}{\longrightarrow}& n^{1-\beta H}\mathrm{E}[|W|^\beta]
	  \end{eqnarray*}
	  Due to $\mathrm{E}[|W|^\beta] < \infty$, (\ref{sec:semilemma}) holds as well as $n\rightarrow \infty$.
\end{proof}
\begin{theorem}
  fBm is not a semimartingale for $H\neq \frac{1}{2}$.
\end{theorem}
\begin{proof}
  Without loss of generality, we set the time scale $T=[0, 1]$. Choosing $\beta=2$ in (\ref{sec:semilemma}), we suppose $U_H(t)$ were a semimartingale.\\
  Case $H < \frac{1}{2}$. Then $\sum_{j=1}^n |U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^2\rightarrow\infty$ contradicts that semimartingale has finite quadratic variation.\\
  Case $H > \frac{1}{2}$. $\sum_{j=1}^n |U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^2\rightarrow 0$. On the one hand, according to Doob-Meyer decomposition, $U_H(t) = M(t) + A(t)$, where $M(t)$ is a local martingale and $A(t)$ is local finite variation process. Consider now,
  \begin{eqnarray*}
	&&\sum_{j=1}^{n} |A(\frac{j}{n}) - A(\frac{j-1}{n})|^2\\
	&\le& \underbrace{\sum_{j=1}^{n} |A(\frac{j}{n}) - A(\frac{j-1}{n})|}_{\rightarrow 0} \cdot \sup\limits_{j} |A(\frac{j}{n}) - A(\frac{j-1}{n})|\\
	&\overset{n\uparrow \infty}{\rightarrow}& 0
  \end{eqnarray*}
  Hence $A(t)$ has quadratic variation zero and we have $0=[U_H, U_H] = [M, M] $, where $[\cdot,\cdot]$ is denoted for quadratic variation. Consequently, $M(t)$ is zero process due to Cauchy Schwarz inequality. In other words, $U_H(t)=A(t)$ which has finite variation. On the ohter Hand, choosing $1<\gamma<\frac{1}{H}$, then $\sum_{j=1}^n |U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^\gamma \rightarrow \infty$. Precisily, 
  \begin{eqnarray*}
	 &&\infty \leftarrow \sum_{j=1}^n |U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^\gamma\\
	 &\hspace{2em}&\le \underbrace{\sup\limits_{1\le j\le n}|U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^{\gamma-1}}_{\overset{(\gamma-1)-\text{H\"older}}{\rightarrow}0} \cdot \sum_{j=1}^n|U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|,
  \end{eqnarray*}
  this leads to $\sum_{j=1}^n|U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|\rightarrow \infty$, which contradicts mentioned above that $U_H(t)$ has finite variation.\\
  Given all that, fBm is not a semimartingale for $H\neq \frac{1}{2}$.
\end{proof}

%\subsection{Integration with respect to FBM}
%\begin{definition}
%  Let $(F(t))_{t \in T}$ and $(G(t))_{t \in T}$ be two stochastic processes with continuous path. The \emph{symmetric integral} of $F$ with respecct to $G$ is defined as following
%  \begin{equation*}
%	(\textbf{symmetric})\int_0^T F(u) \mathop{dG(u)} = \frac{1}{2}\lim\limits_{n\rightarrow\infty}\sum_{j=0}^{n-1} (F_{\frac{(j+1)T}{n}} + F_{\frac{jT}{n}}) (G_{\frac{(j+1)T}{n}} - G_{\frac{jT}{n}}).
%  \end{equation*}
%\end{definition}
%\begin{theorem}
%  (Cf.\cite{chenu} and \cite{gradin})Let $U_H$ be a FBM. If the Hurst index $H>\frac{1}{6}$ and $f\in \mathcal{C}^{\infty}(\mathbb{R})$ with polynomial growth for all its derivatives, then $(\textbf{symmetric})\int_0^T f'(U_H(u))\,\mathop{dU_H(u)}$ exist. Moreover ons has
%  \begin{equation}
%	f(U_H(T)) - f(U_H(0)) = (\textbf{symmetric})\int_0^T f'(U_H(u)) \mathop{dU_H(u)}
%	\label{sec:intfbm}
%  \end{equation}
%  for $T \ge 0$.
%\end{theorem}

%\begin{proof}
%  See \cite{nourdin}, p.32, Theorem 3.5 .
% \end{proof}


\newpage
%---------- 4. section ------------%
\section{Fractional Ornstein-Uhlenbeck Process}
\setcounter{equation}{0}
In this section we turn our attention on the fractional Ornstein-Uhlenbeck process (for short, we denote it by fOU). 
\subsection{Fractional Ornstein-Uhlenbeck Process}
Consider the following stochastic dynamics
\begin{eqnarray}
 \mathop{dX_t} = -aX_t\mathop{dt} + \gamma \mathop{dU_H(t)},
 \label{sec:ou1}
\end{eqnarray}
where $(X_t)_{t\ge 0}$ is a stochastic process, $a, \gamma\in\mathbb{R}_{+}$ and $(U_H(t))_{t\ge 0} $ fBm with Hurst exponent $H$. In fact, given an initial condition $X_0(\omega)=b(\omega)$, then in the theory of SDE, (\ref{sec:ou1}) is understood as
\begin{eqnarray}
  X_t(\omega) = b(\omega) - a\int_0^t X_u(\omega) \mathop{du} + \gamma U_H(t)(\omega)
  \label{sec:oup}
\end{eqnarray}
for $t \ge 0$.

\begin{definition}
  The \emph{fractional Ornstein-Uhlenbeck} (fOU) process is defined as the solution of (\ref{sec:ou1}).
\end{definition}

If $H=\frac{1}{2}$, fOU is said to be \emph{Ornstein-Uhlenbeck process}. Cheridito et al.\cite{chriel} shows following Lemmas with H\"older continuity of $U_H(t)$.
\begin{lemma}
  Let $U_H(t)$ be a fBm, $a \in \mathbb{R}_+$ and $s, d\in \mathbb{R}$ such that $d\le s$. Then there exists a Riemann-Stieljes integral such that 
  \begin{eqnarray}
	\int_d^s e^{au} \mathop{dU_H(u)} = e^{as} U_H(s) - e^{ad} U_H(d) - a\int_d^s U_H(u) e^{au}\mathop{du}.
	\label{sec:rie}
  \end{eqnarray}
\end{lemma}

\begin{proof}
  See. \cite{chriel}, p.11, Proposition A.1 .
\end{proof}

\begin{lemma}
  Let $H \in (0, \frac{1}{2}) \cup (\frac{1}{2}, 1), a >0$ and $-\infty\le m < n \le j < k < \infty.$ Then 
  \begin{eqnarray*}
	&&\mathrm{E}[\int_m^n e^{au}\mathop{dU_H(u)}\int_j^k e^{as}\mathop{dU_H(s)}]\\
	&=& H(2H-1)\int_m^n e^{au} \brkt{\int_j^k e^{as}(s-u)^{2H-2}\mathop{ds}}\mathop{du}.
\end{eqnarray*}
  \label{sec:lemma2}
\end{lemma}

\begin{proof}
  See. \cite{chriel}, p.5, Lemma 2.1 .
\end{proof}

\begin{theorem}
  $\hat{X}^{b,H}_t := e^{-at} \brkt{b + \gamma\int_0^t e^{au}\mathop{dU_H(u)}}$ is the solution that solves (\ref{sec:oup}) for $u\ge 0$.
  \label{sec:sol}
\end{theorem}

\begin{proof}
  Cf. \cite{chriel}, p.11, Proposition A.1 . We define
  \begin{eqnarray*}
	Y(t):=\int_0^t X_u \mathop{du},
  \end{eqnarray*}
 
 for $t\ge 0$. Rewirte (\ref{sec:oup}) with $Y_t$ and $Y(0)=0$,
  \begin{eqnarray*}
	Y'(t) = b - aY(t) + \gamma U_H(t)
  \end{eqnarray*}
  And the solution of that linear differential equation with $Y(0)=0$ is
  \begin{eqnarray*}
	Y(t) = e^{-at}\int_0^t e^{au}(b+\gamma U_H(u)) \mathop{du},
  \end{eqnarray*}
  in terms of definition above, using (\ref{sec:rie})
	\begin{eqnarray*}
	&&X(t) =  Y'(t)\\
	&=& -ae^{-at}\int_0^t e^{au}(b+\gamma U_H(u)) \mathop{du} + e^{-at}e^{at}(b+\gamma U_H(t)) \\
	&=&  -ae^{-at}\int_0^t e^{au}(b+\gamma U_H(u)) \mathop{du} + b +\gamma U_H(t)\\
	&=& e^{-at} \brkt{\underbrace{-a\int_0^t e^{au} \gamma U_H(u) \mathop{du} + e^{at}\gamma U_H(t)}_{\gamma\int_0^t e^{au} \mathop{dU_H(u)}} -a\int_0^t e^{au} du \cdot b } + b\\
  &=&  e^{-at} \brkt{\gamma\int_0^t e^{au} \mathop{dU_H(u)} - e^{au}|^{u=t}_{u=0} \cdot b } + b\\
  &=&  e^{-at} \brkt{\gamma\int_0^t e^{au} \mathop{dU_H(u)} + b}
	\end{eqnarray*}
\end{proof}

In order to have a stationary solution, we assume that the initial value is centered Gaussian that $\hat{X}_{H,t}:= \hat{X}_t^{\gamma\int_{-\infty}^0 e^{au}\mathop{dU_H(u)}, H} := e^{-at}\brkt{\gamma\int_{-\infty}^t e^{au}\mathop{dU_H(u)}}$.
\begin{theorem}
  $(\hat{X}_{H,t})_{t\ge 0}$ is centered Gaussian and stationary.
  \label{sec:gsp}
\end{theorem}

\begin{proof}
  For the sake of simplicity, we let $\hat{X}_{H,t} = \int_{-\infty}^t e^{au}\mathop{dU_H(u)}$.
  Fix $\epsilon^j > 0, H\in (0,1)$ , then there exists  $\{u_0^j<u_1^j<\dots<u^j_{k_j}\le t_j\}$ such that
  \begin{eqnarray*}
	&&|\hat{X}_{H,t_j} - \sum_{l=0}^{k_j-1} e^{au^j_l}(U_H(u^j_{l+1}) - U_H(u^j_{l}))|\\
	&=& |\int_{-\infty}^{t_j} e^{au}\mathop{dU_H(u)} - \sum_{l=0}^{k_j-1} e^{au^j_l}(U_H(u^j_{l+1}) - U_H(u^j_{l}))|\\
	&<& \epsilon^j.
  \end{eqnarray*}
  for $0 \le j \le d $.\\
  On the one hand, we calculate the characteristic function of $(\hat{X}_{H, t_1},\dots, \hat{X}_{H, t_d})$ approximately with respect to $\epsilon^1,\dots,\epsilon^d$.
  \begin{eqnarray*}
	\sum_{j=1}^d \xi_j\hat{X}_{H,t_j} \approx \sum_{j=1}^d \xi_j\brkt{\sum_{l=0}^{k_j-1} e^{au^j_l}(U_H(u^j_{l+1}) - U_H(u^j_{l}))}
  \end{eqnarray*}
  Notice that since $(U_H(t))$ is centered Gaussian process, any infinite linear combination of its instances is centered Gaussian again. In other words, 
  \begin{eqnarray*}
	\sum_{j=1}^d \xi_j \brkt{\sum_{l=0}^{k_j-1} e^{au^j_l}(U_H(u^j_{l+1}) - U_H(u^j_{l})) }
  \end{eqnarray*}
  is centered Gaussian. Passing $\epsilon^1,\dots,\epsilon^d$ to zero, it implies immediately $(\hat{X}_{t_1},\dots, \hat{X}_{t_d})$ is centered Gaussian process due to Corolary \ref{sec:gauss}.\\
  On the other hand,
	\begin{eqnarray*}
	\mathrm{E}[\exp\{i\sum_{j=1}^d \xi_j\hat{X}_{H, t_j}\}] \approx \mathrm{E}[\exp\{ i\sum_{j=1}^d \xi_j\brkt{\sum_{l=0}^{k_j-1} e^{au^j_l}(U_H(u^j_{l+1}) - U_H(u^j_{l})) }\}]
  \end{eqnarray*}
  Since $\{U_H(t)\}$ has stationary increments, then 
  \begin{eqnarray*}
	&& \mathrm{E}[\exp\{ i\sum_{j=1}^d \xi_j\brkt{\sum_{l=0}^{k_j-1} e^{au^j_l}(U_H(u^j_{l+1}) - U_H(u^j_{l})) }\}] \\
	&=& \mathrm{E}[\exp\{ i\sum_{j=1}^d \xi_j\brkt{\sum_{l=0}^{k_j-1} e^{au^j_l}(U_H(u^j_{l+1}+\tau) - U_H(u^j_{l}+\tau)) }\}]
  \end{eqnarray*}
  which converges if $\epsilon$'s tend to zero and must be equal $\mathrm{E}[\exp\{ i\sum_{j=1}^d \xi_j \hat{X}_{H, t_j+\tau}\}]$, i.e., $(\hat{X}_{H,t})$ is stationary process.
\end{proof}

\begin{theorem}
  Let $H$ be that $H\in (0, \frac{1}{2}) \cup (\frac{1}{2}, 1)$. Then
  \begin{eqnarray*}
	\varsigma_{\hat{X}_{H}}(\tau) = \frac{1}{2} \gamma^2 \sum_{k=1}^{N} a^{-2k}\brkt{\prod_{j=0}^{2k-1}(2H-j)}\tau^{2(H-k)} + \mathrm{O}(\tau^{2H-2N-2})
\end{eqnarray*}
for $N \in \mathbb{N}, \tau\in\mathbb{R}$, $\gamma, a \in \mathbb{R}_+$ as in (\ref{sec:oup}).
  \label{sec:autocov}
\end{theorem}
\begin{proof}
  See \cite{chriel}, p.7, Theorem 2.3 .
\end{proof}

\begin{corollary}
  $(\hat{X}_{H,t})_{t\ge 0}$ has long memory for $H\in (\frac{1}{2}, 1)$.
  \label{sec:longmer}
\end{corollary}
\begin{proof}
  Consider the autocovariance of $(\hat{X}_{H,t})$, with a given function \\$c(a, \gamma, H):= \frac{1}{2} \gamma^2 a^{-2k}\brkt{\prod_{j=0}^{2k-1}(2H-j)}$,
  \begin{eqnarray*}
	\varsigma_{\hat{X}_{H}}(\tau) = \sum_{k=1}^N c(a, \gamma,  H) \tau^{2H-2k} + \mathrm{O}(\tau^{{2H-2N-2}})
  \end{eqnarray*}
  is obviously tending to zero as $\tau$ goes to infinity. And in order to check convergence of $\sum_{\tau=1}^{\infty} \varsigma_{\hat{X}_{H}}(\tau)$, we only need to check the term of $k=1$ in the summation, because in the case $k>1$, $\tau^{2H-2k} < \tau^{2H-2}$ for $\tau\in\mathbb{N}$. Note that, $0 <c(a, \gamma, H) < \infty$ when $H\in(\frac{1}{2}, 1)$. We deal with it in the same way as in Theorem \ref{sec:lmemory} (use limit comparison test). That means, if $2H < 1$, namely $H<\frac{1}{2}, \sum_{\tau=1}^{\infty} \varsigma_{X_{H}}(\tau)$ converges. For $H\in (\frac{1}{2}, 1)$, it diverges. Thus $(\varsigma_{\hat{X}_{H}}(t))_{t\ge 0}$ has long memory for $H\in(\frac{1}{2}, 1)$.
\end{proof}
\newpage

%---------- 6. Anwendung in die finanzmathe bzw. in der volatility process -------%
\section{Applications in Financial Mathematics}
\setcounter{equation}{0}
\subsection{Fractional Black-Scholes Model}
In this subsection, we'll introduce fBm to the Black-Scholes model. To be specific, our finance market is modeled with two stochastic processes, i.e. a process of a riskless asset $(A_t)_t$ and a process of price of a stock $(S_t)_t$.  The stock is assumed that it pays no dividends. Setting initial conditions $A_0=1, S_0=1$, we give our fractional Black-Scholes model as follows
%
\begin{eqnarray}
  A_t &=& \exp(rt)\nonumber\\
  S_t &=& \exp(rt + \mu(t) +\sigma U_H(t)), t\in [0, T],
  \label{sec:fbs2}
\end{eqnarray}
where  $r\in\mathbb{R}, \sigma\in\mathbb{R}_+, \sup\limits_{t\in[0, T]}\mu(t) < \infty$.
 Through out this section we denote by $(\mathcal{F}^X_t)_t$ the filtration of a stochatic process $(X_t)_t$ and give definitions %Our finance market consist of $\hat{X}:=(X_t^0,\dots, X_t^n)$.

\begin{definition}
  A $\mathbb{R}^{2}$-valued stochastic process $(\xi_t^0, \xi_t^1)_{t\in [0, T]}$ is said to be a \emph{strategy} to (\ref{sec:fbs2}) , if $\xi_t^0 \in \mathcal{F}^A_{j}$ and $\xi_t^1 \in \mathcal{F}^S_j$, for $0\le j \le t$.
\end{definition}

\begin{definition}
  A stochastic process $(V_t)_{t\in[0, T]}$ is said to be \emph{value process} with respect to strategy $(\xi_t^0, \xi_t^1)$, if 
  \begin{eqnarray*}
	V_t =  \xi^0_t A_t + \xi^1_t S_t %=: \hat{\xi}_t\cdot\hat{X}_t.
  \end{eqnarray*}
  for $t \in [0, T]$.
\end{definition}

%\begin{definition}
%  A strategy $(\hat{\xi}_t)_{t\in [0, T]}$ is said to be \emph{self-financing}, if 
%  \begin{eqnarray*}
%	\hat{\xi}_t \cdot \hat{X}_t = \hat{\xi}_{t+\Delta} \cdot \hat{X}_t
%  \end{eqnarray*}
%  for $t \in [0, T]$.
%\end{definition}

\begin{definition}
  A stochastic process $(\tilde{V}_t)_{t\in[0,T]}$ is said to be \emph{discounted value process} of a value process $(V_t)_{t\in [0, T]}$ with respect to $(\xi_t^0, \xi_t^1)$, if 
  \begin{eqnarray*}
	\tilde{V}_t = \frac{V_t}{A_t}
  \end{eqnarray*}
  for $t \in [0, T]$.
\end{definition}
Obviously, $\tilde{V}_t = \xi^0 + \xi^1 \tilde{S}_t$ with $\tilde{S}_t=\exp(\mu(t) + \sigma U_H(t))$.
\begin{definition}
  A strategy $(\xi^0_t, \xi^1_t)_{t\in T}$ is said to be \emph{self-financing}, if
  \begin{eqnarray}
	V_T = V_0 + \sum_{j=1}^{m} \xi^0_{s_j} (A_{s_{j}} - A_{s_{j-1}}) + \xi^1_{s_j} (S_{s_{j}} - S_{s_{j-1}}). 
	\label{sec:123}
  \end{eqnarray}
  \label{sec:def}
  for $0=s_0\le s_1\le \dots\le s_m=T$.
\end{definition}

\begin{definition}
  A self-financing strategy $(\xi^0_t, \xi^1_t)_{t\in T}$ is said to be have \emph{arbitrage}, if its discounted process satisfies following conditions
  \begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
	\item $\mathcal{P}[\tilde{V}_T - \tilde{V}_0]=1$.
	\item $\mathcal{P}[\tilde{V}_T > 0] > 0$.
	\end{enumerate}
  \end{definition}
%To be specific, our finance market is modeled with two stochastic processes that a process of a riskless asset $(A_t)_t$ and a process of price of a stock $(S_t)_t$.  The stock is assumed that it pays no dividends. Setting initial values $A_0=1, S_0=1$, we give our fractional Black-Scholes model as follows
%\begin{eqnarray}
%  \mathop{dA_t} &=& rA_t\mathop{dt}\nonumber\\
%  \mathop{dS_t} &=& \mu_t S_t \mathop{dt} + \sigma U_H(t)
%  \label{sec:fbs1}
%\end{eqnarray}
%for $t\in [0, T]$.

%Setting initial values $A_0=1, S_0=1$, 
%\begin{eqnarray}
%  A_t &=& \exp(rt)\nonumber\\
%  S_t &=& \exp((rt + \mu(t) +\sigma U_H(t)), t\in [0, T],
% \label{sec:fbs2}
%\end{eqnarray}
%where  $r\in\mathbb{R}, \sigma\in\mathbb{R}_+, \sup\limits_{t\in[0, T]}\mu(t) < \infty$.
%is the solution of (\ref{sec:fbs1}). \\ 
  Cheridito shows that in reality, if there exist a minimal amount of time between two successive transactions, the market(\ref{sec:fbs2}) is arbitagefree. In following we complete the proof in \cite{chridito} of the claim.
\begin{lemma}
  Let $(X_t)_{t\ge 0}$ be a stochastic process continueous in $t$. If $(X_t)$ is a modification of the process 
  $$
  \brkt{\int_0^t (t-u)^{H-\frac{1}{2}}\mathop{dB_u}}_{t\ge 0}
  $$
  for $(B_t)_{t\ge 0}$ a Brownian motion and $H\in (0, \frac{1}{2})\cup(\frac{1}{2}, 1)$,
  then 
  $$
  \mathcal{P}[\sup\limits_{t\in[a,b]} X_t \le -c] > 0
  $$
  for $c \ge 0$ and $0< a\le b$.
  \label{sec:lll} 
\end{lemma}

\begin{proof}
  See \cite{chridito}, p.15, Lemma 4.2 .
\end{proof}

\begin{theorem}
  Let $(S_t)_{t\in[0, T]}$ be a stochastic process such that
  \begin{eqnarray}
	\tilde{S}_t = \exp\brkt{\mu(t) + \sigma U_H(t)},
	\label{sec:fbs}
  \end{eqnarray}
  where $\mu, \sigma$ are as in (\ref{sec:fbs2}), $U_H(t)$ is a fBm. If there exist
  \begin{eqnarray*}
  \xi^1_t = f_0\mathbbm{1}_{\{0\}}(t)+\sum_{k=1}^{n-1} f_k \mathbbm{1}_{(\tau_k, \tau_{k+1}]}(t)
  \end{eqnarray*}
  where $t\in[0, T], f_k$ is family of  $\mathcal{F}^{U_H}_k $-measurable function for $k \in \{1,\dots,n-1\}$. $0 = \tau_1 < \cdots <\tau_n = T$ are stopping times with respect to $\mathcal{F}^{U_H}_{\tau_k} $ respectively,  with $\tau_{k+1} - \tau_k\ge m$ for some $m>0$. If there exists a $k \in \{0,\dots,n-1\}$ such that $\mathcal{P}[f_k\neq 0]>0$ , then
  \begin{eqnarray*}
	\mathcal{P}[(\xi^1 \cdot \tilde{S})_T < 0] > 0,
  \end{eqnarray*}
	\label{sec:claim}
	where $(\xi^1 \cdot \tilde{S})_T := \sum_{k=1}^{n} \xi^1_{\tau_k} (\tilde{S}_{k} - \tilde{S}_{k-1})$.
\end{theorem}

\begin{proof}
 % We prove the $\mahtcal{\mu^w} [(\xi^1 \codt S)_T < 0] > 0$ to obtain the claim. \\
  Cf.\cite{chridito}, p.18, Theorem 4.3 . For sake of simplicity, we let $\tilde{S}_t = \exp\brkt{ U_H(t)}$ and $f_0=0$. Note that, $\xi^1$ is predictable. %We have
    %\begin{eqnarray*}	
	%  (\xi^1 \cdot S)_T = \sum_{k=1}^{n-1} f_k(e^{U_H(\tau_{k+1})} - e^{U_H(\tau_k)}).
	%\end{eqnarray*}
  Assume ${\mathcal{P}}[(\xi^1 \cdot \tilde{S})_T < 0] = 0$, then there exisis 
  $$
  l = \min \{ j: \mathcal{P}[f_j \neq 0] > 0,\hspace{1em} \mathcal{P}[\sum_{k=1}^{j} f_k (e^{U_H(\tau_{k+1})} - e^{U_H(\tau_k)})\ge 0]=1 \}
  $$
  Then either 
	$$
	\sum_{k=1}^{j-1} f_k (e^{U_H(\tau_{k+1})} - e^{U_H(\tau_k)}) = 0
	$$
	a.s., or 

	$$
	\mathrm{P}[\sum_{k=1}^{j-1} f_k (e^{U_H(\tau_{k+1})} - e^{U_H(\tau_k)}) < 0] < 0 .
	$$
  This leads to
  \begin{eqnarray*}
	\mathcal{P} \left[\brkt{\sum_{k=1}^{l-1} f_k (e^{U_H(\tau_{k+1})} - e^{U_H(\tau_k)}} \le 0\right] = 1 
  \end{eqnarray*}
  Ignoring constant term, we define 
  $$
  U_H(t)(\omega) = \int_{\mathbb{R}} \mathbbm{1}_{\{t\ge u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u\le 0\}} (-u)^{H-\frac{1}{2}} \, \mathop{d\omega(u)} .
  $$
  where $\omega(u) := B_u(\omega)$ for all $\omega \in \Omega^w$.
  We give the filtration $(\mathcal{F}_t^{\Omega^w})$ denoting by 
  $$
  \mathcal{F}_t^{\Omega^w} := \sigma(\{\{w \in \Omega^w : \omega(u) \in \mathbb{R}\} : -\infty < u \le t, t\in\mathbb{R}\}).
  $$
  Then $\tau_k$ is also stopping time of $\mathcal{F}_t^{\Omega^w}$ due to
  $$
  \mathcal{F}^{U_H}_t \subset \mathcal{F}_t^{\Omega^w},	t \in \mathbb{R}
  $$
 For $\omega \in \Omega^w$, we split it at the time point $\tau_l(\omega)$ into two parts as follows
 \begin{eqnarray*}
 \psi_{\omega}(u) := \omega(u)\mathbbm{1}_{(-\infty, \tau_l(\omega)]}(u), u\in \mathbb{R} \\
   \phi_{\omega}(u) := \omega(\tau_l(\omega) + u) - \omega(\tau_l(\omega)), u\ge 0.
 \end{eqnarray*}
Corresponding to each part, we define
\begin{eqnarray*}
  \Omega^1 := \{\psi_{\omega} \in \mathcal{C}(\mathbb{R}) : \omega \in \Omega^w\}\\
  \Omega^2 := \{\phi_{\omega} \in \mathcal{C}([0, \infty)) : \omega \in \Omega^w\}
\end{eqnarray*}
And for the smallst $\sigma$-algebra of all subsets, respectively, of $\Omega^1, \Omega^2$ are denoted by $\mathscr{B}^1, \mathscr{B}^2$.
Notice that
\begin{eqnarray*}
  \{\tau_l \le t\} \cap \{\psi_{\omega} \in \Omega^w\} &=& \{\{\omega \in \Omega^w : \omega(u) \in \mathbb{R}\} : -\infty < u \le t\}\\
  &\in& \mathcal{F}^{\Omega^w}_t,
\end{eqnarray*}
therefore is $\psi_{\omega}$ a $\mathcal{F}^{\Omega^w}_{\tau_l}$- measurable mapping. Moreover, since the strong Markovian property of Brownian motion, $\phi_{\omega}$ is independent of $\mathcal{F}^{\Omega^w}_{\tau_l}$ and it must be a Brownian motion.  Plugging  $\psi_{\omega}, \phi_{\omega}$ into $\Omega$, we calculate the value process
\begin{eqnarray*}
  &&\brkt{\sum_{k=1}^{l-1} f_k (e^{U_H(\tau_{k+1})}  - e^{U_H(\tau_{k})}) + f_l(e^{U_H(\tau_l+m)}-e^{U_H(\tau_l)})} (\omega)  \\
  &=& \underbrace{\sum_{k=1}^{l-1} f_k (e^{U_H(\tau_{k+1})} - e^{U_H(\tau_{k})})}_{:=J^1} (\omega) + f_l(e^{U_H(\tau_l+m)}-e^{U_H(\tau_l)}) (\omega) \\
  &=& J^1(\omega) + f_l(\exp\{\int_{\mathbb{R}}\mathbbm{1}_{\{\tau_l\ge u\}}(\tau_l(\omega)+m-u)^{H-\frac{1}{2}}-\mathbbm{1}_{\{\tau_l\ge u\}}(\tau_l(\omega)-u)^{H-\frac{1}{2}}\mathop{d\omega(u)}\\
&+& \int_{\mathbb{R}} \mathbbm{1}_{(\tau_l, \tau_l+m]}(\tau_l+m - u)^{H-\frac{1}{2}}\mathop{d\omega(u)}\})\\
&=&  J^1(\psi_{\omega}) + \underbrace{f_l\brkt{\exp\{\int_{-\infty}^{\tau_l(\psi_{\omega})}(\tau_l(\psi_{\omega})+m+u)^{H-\frac{1}{2}}-(\tau_l(\psi_{\omega})-u)^{H-\frac{1}{2}}\mathop{d\psi_{\omega}(u)}\}}_{=:J^2(\psi_{\omega}, m)}}\\
&\cdot&  \underbrace{\brkt{\exp\{\int_0^{m}(m-u)^{H-\frac{1}{2}}\mathop{d \phi_{\omega}(u)}\} - 1}}_{=:J^3(\phi_{\omega}, m)}\\
  &=& J(\psi_{\omega}, \phi_{\omega}, m)
\end{eqnarray*}
where $J$ is defined as
\begin{eqnarray*}
  J(\psi, \phi, t) : = J^1(\psi) + J^2(\psi, t)  &\cdot&  J^3(\phi, t)
\end{eqnarray*}
for $\psi\in\Omega^1, \phi\in\Omega^2$.\\

Indeed, for $\psi\in\Omega^1, \phi\in\Omega^2$, $J(\psi, \phi, \cdot)$ has continueous path on $(\Omega^1 \times \Omega^2, \mathscr{B}^1\otimes\mathscr{B}^2)$, then we can define a $\mathscr{B}^1\otimes\mathscr{B}^2$- measurable set
$$
E := \{(\psi, \phi) : \sup\limits_{m\le t\le T} J(\psi, \phi, t) < 0\}
$$
Note that
\begin{eqnarray*}
  \mathrm{E} [\mathbbm{1}_{E}(\psi_{\omega}, \phi_{\omega})| \psi_{\omega}=\omega_1] &=& \mathcal{P}[\sup\limits_{m\le t \le T} J(\omega_1, \phi_{\omega}, t) < 0] \\
  &\ge& \mathcal{P}[J^1(\omega_1) + \sup\limits_{m\le t \le T}J^2(\omega_1, t) \cdot \sup\limits_{m\le t \le T}J^3(\phi_{\omega}, t) < 0]
\end{eqnarray*}
It is clear $J^2(\omega_1)$ is bounded for a fixed $\omega_1$ on the compact set and $J^1(\omega_1)$ also. We set $J^1(\omega_1) = c_1 , \sup\limits_{_{m\le t \le T}}J^2(\omega_1, t) = c_2$. Thanks to Lemma \ref{sec:lll}, there exist $D \subset \Omega^{w} $  such that $\mathcal{P}^{w}[D] >0$ and 
\begin{eqnarray*}
  \int_0^{m}(m-u)^{H-\frac{1}{2}}\mathop{d \phi_{\omega}(u)} < c_3 
\end{eqnarray*}
for $\omega \in D$.
where $c_3$ could be small enough that $c_2 \cdot e^{c_3 - 1} < 0 $. Under assumption at beginning, $c_1 \le 0$ almost surely. All of this leads to 
\begin{eqnarray*}
  && \mathrm{E} [\mathbbm{1}_{E}(\omega_1, \phi_{\omega})| \psi_{\omega}=\omega_1]\\ 
  &\ge& \mathcal{P}[J^1(\omega_1) + \sup\limits_{m\le t \le T}J^2(\omega_1, t) \cdot \sup\limits_{m\le t \le T}J^3(\phi_{\omega}, t) < 0]\\
  &>& 0
\end{eqnarray*}
for $\omega \in \Omega^w$.
Then
\begin{eqnarray*}
  &&\mathcal{P}[\sum_{k=1}^{l} f_k (e^{U_H(\tau_{k+1})} - e^{U_H(\tau_k)}) < 0] \\
&\ge&\mathcal{P}[\sum_{k=1}^{l-1} f_k (e^{U_H(\tau_{k+1})}  - e^{U_H(\tau_{k})}) +  \sup\limits_{m\le t \le T}f_l(e^{U_H(\tau_l+t)}-e^{U_H(\tau_l)})  < 0]\\
&=& \mathrm{E}[ \mathrm{E} [\mathbbm{1}_{E}(\omega_1, \phi_{\omega})| \psi_{\omega}=\omega_1]] > 0
\end{eqnarray*}
this contradicts our assumption. It must be that ${\mathcal{P}}[(\xi^1 \cdot S)_T < 0] > 0$.
\end{proof}

\begin{corollary}
  Let strategy $(\xi^0, \xi^1)$ be such that, $\xi^1$ is given as in Theorem \ref{sec:claim}. Then the strategy has no arbitrage in our finance market (\ref{sec:fbs2}).
\end{corollary}
\begin{proof}
  Assume $(\xi^0, \xi^1)$ is a self-financing strategy. In terms of Definition \ref{sec:def},
\begin{eqnarray*}
  \tilde{V}_T - \tilde{V}_0 &=&  \sum_{k=1} ^ {n} \frac{(\xi^0_kA_{k} + \xi^1_kS_{k})}{A_{k}} - \frac{(\xi^0_{k}A_{k-1} + \xi^1_{k}S_{k-1})}{A_{k-1}}\\
  &=&  \sum_{k=1}^{n} \xi^1_k (\tilde{S}_{k} - \tilde{S}_{k-1})\\
%  &=& (\xi^1 \cdot \tilde{S})_T
\end{eqnarray*}
It follows then from Theorem \ref{sec:claim}, $\mathcal{P}[(\tilde{V}_T - \tilde{V}_0) < 0] > 0$, since and therefore the strategy has no arbitrage in (\ref{sec:fbs2}).
\end{proof}


\subsection{Fractional Calculus and Discretization of Fractionally Integrated Process}
The fractional integral could be derived from the repeated integral which is approached by Riemann-Liouville integral.
\begin{eqnarray*}
  &&\int_0^s\int_0^{s_1}\int_0^{s_2}\cdots\int_0^{s_{n-1}}\, f(s_n)ds_n\cdots ds_2 ds_1\\
  &=& \frac{1}{(n-1)!} \int_0^s (s-u)^{n-1} f(u) \mathop{du}
\end{eqnarray*}
for $n\in \mathbb{N}$. Where $n$ is said to be \emph{the order} of the fractional integral. We extend it with the order $\alpha \in \mathbb{R}_+$.
\begin{definition}
  Let $f$ be a locally integrable function. The \emph{Riemann-Liouville fractional integral of order $\alpha$} is defined as follows
\begin{eqnarray}
  I^\alpha f(s) := \frac{1}{\Gamma(\alpha)} \int^s_0 (s-u)^{\alpha-1} f(u) \mathop{du}
  \label{sec:fraint}
\end{eqnarray}
for $s, \alpha \in \mathbb{R}_+$.
\end{definition}

Remark, $I^\alpha I^\beta = I^\beta I^\alpha = I^{\alpha+\beta}$.
Let $\Phi_\alpha(s):= \frac{s^{\alpha-1}}{\Gamma(\alpha)}$,  then
\begin{eqnarray}
  I^\alpha f(s) = \Phi_\alpha(s) \ast f(s)
\end{eqnarray}

We give the definition of fractional derivative.
\begin{definition}
  Let $f$ be a locally integrable function. The \emph{Riemann-Liouville fractional derivative of order $\alpha$} is defined as follows
\begin{eqnarray}
  D^\alpha f(s) := \begin{cases} \frac{d^n}{ds^n}[\frac{1}{\Gamma(n-\alpha)}\int_0^s \frac{f(u)}{(s-u)^{\alpha + 1 - n}}\mathop{du}] &\mbox{if } n-1 < \alpha < n,\\
	\frac{d^n}{ds^n} f(s) & \mbox{if } \alpha=n
  \end{cases}
  \label{sec:frader}
\end{eqnarray}
\end{definition}
for $n \in \mathbb{Z}$ and $\alpha \in \mathbb{R}, s \in \mathbb{R}_+ $.
%This Long memory property may explain why in stock market, large upheavals tend to be followed by large upheavals and small upheavals happend after by small upheavals.  In order to model long memory volatility process, Comte and Renault are forced to set $H\in (\frac{1}{2}, 1)$ in (\ref{sec:fv2}) named fractional volatility stochastic(FSV), see\cite{comren}. \\
In \cite{comte}, Comte gave a truncated representation fBm $(U_{\alpha,t})$.
\begin{eqnarray}
  U_{\alpha,t} &=& \int_0^t \frac{(t-u)^\alpha}{\Gamma(1+\alpha)}\mathop{dB_u}, 
  \label{<sec:cc1>}
\end{eqnarray}
Where $|\alpha| < \frac{1}{2}, (B_u)$ Brownian motion. Notice that $\alpha$ is given by $H-\frac{1}{2}$ as in previous representation and the long memory property remains true.

\begin{definition}
  The \emph{fractionally integrated process of order $\alpha$}, $|\alpha| < \frac{1}{2}$ is defined as
  \begin{eqnarray}
	X(t) &=&  \int_0^{t} g(t - u) \mathop{dB_u}
	\label{sec:frac}
  \end{eqnarray}
  where $(B_t)_t$ is Brownian motion and 
  \begin{eqnarray}
	g(s) &=& \Phi_{\alpha+1}(s)h(s)\\
	&=& \frac{s^\alpha h(s)}{\Gamma(1+\alpha)}
	\label{sec:wer}
  \end{eqnarray}
  with $h \in C^1([0, T])$.
\end{definition}
\begin{proposition}
  Let $X(t)$ be a fractionally integrated process of order $\alpha$, $|\alpha| < \frac{1}{2}$, then
  \begin{eqnarray}
	X(t) &=& \int_0^t c(t-u)\mathop{dU_{\alpha, u}}\\
	&:=& \frac{d}{dt}\brkt{\int_0^t c(t-u) U_{\alpha, u} \mathop{du}}
	\label{sec:bb1}
  \end{eqnarray}
  where $c \in C([0, T]),$ $c$ and $g$ are functions related by
  \begin{eqnarray}
	c(s) &=& \frac{d}{ds}\brkt{\int_0^s \frac{(s-u)^{-\alpha}u^\alpha h(u) \mathop{du}}{\Gamma(1-\alpha)\Gamma(1+\alpha)}}.
\label{sec:frac2}
  \end{eqnarray}
  \begin{eqnarray}
	u^\alpha h(u) = \frac{d}{\mathop{du}}\brkt{\int_0^u c(s) (u-s)^\alpha \mathop{ds}}.
	\label{sec:frac3}
  \end{eqnarray}
\end{proposition}

\begin{proof}
  See \cite{core}, pp.106-108, Lemma 1, Proposition 1. 
\end{proof}
\begin{definition}
  Let  $X_t$ be a fractionally integrated process of order $\alpha$ on $[0, T]$ and $|\alpha| < \frac{1}{2}$. The \emph{fractional derivation of order $\alpha$} is defined as
  \begin{eqnarray}
   && X^{(\alpha)}(t)\nonumber\\
	&:=&  \int_0^t \frac{(t-u)^{-\alpha}}{\Gamma(1-\alpha)} \mathop{dX_{t}}\nonumber\\
    &:=&  \frac{d}{dt}\int_0^t \frac{(t-u)^{-\alpha}}{\Gamma(1-\alpha)} X_{t} \mathop{du} \nonumber
	\label{sec:cc4}
  \end{eqnarray}
\end{definition}

\begin{proposition}
  $ X^{(\alpha)}(t)$ is well-defined and mean square continuous. If $h(0)$ is invertible and $h \in C^2([0, T])$ , then $X^{(\alpha)}(t)$ has the $MA(\infty)$ representation
  %\begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
  $$X^{(\alpha)}(t) = \int_0^t c(t-s) \mathop{dB_s}.$$
  where $c$ and $h$ are one-to-one related by (\ref{sec:frac2}) and (\ref{sec:frac3}).
%\item $(X^{(\alpha)})^{-\alpha}=X$ for $\alpha\in(-\frac{1}{2}, \frac{1}{2})$.
%\end{enumerate}
\end{proposition}
\begin{proof}
  See  \cite{core}, p.111, Proposition 4.
  \end{proof}
  \begin{theorem}
	Let  $X_t$ be a locally integrable function on $[0, T]$ and $|\alpha| < \frac{1}{2}$. Then
	\begin{eqnarray}
	X(t) &=& \int_0^t \frac{(t-u)^\alpha}{\Gamma(1+\alpha)}\mathop{dX^{(\alpha)}_u}
	\label{sec:imp}
  \end{eqnarray}
  \end{theorem}
  \begin{proof}
	Since all the integrands is nonnegative, we could apply Fubini theorem.
  \begin{eqnarray*}
	&&\int_0^t \frac{(t-u)^\alpha}{\Gamma(1+\alpha)}\mathop{dX^{(\alpha)}_u}\\
	&=& \frac{d}{dt}\int_0^t \frac{(t-u)^\alpha}{\Gamma(1+\alpha)} X^{(\alpha)}_u\mathop{du}\\
	&=& \frac{d}{dt}\int_0^t \frac{(t-u)^\alpha}{\Gamma(1+\alpha)} \brkt{ \int_0^u \frac{(u-s)^{-\alpha}}{\Gamma(1-\alpha)}\mathop{dX_s}}\mathop{du}\\
	%&=& \int_0^t \frac{(t-u)^\alpha}{\Gamma(1+\alpha)} \brkt{\frac{d}{du} \int_{0}^{u} \frac{(u)^{-\alpha}}{\Gamma(1-\alpha)}X_s\mathop{ds}}\mathop{du}\\
%	&=& \int_0^t \brkt{\frac{d}{du} \int_0^u \frac{(t-u)^\alpha(u-s)^{-\alpha}}{\Gamma(1+\alpha)\Gamma(1-\alpha)}X_s\mathop{ds}}\mathop{du}\\
	&=& \frac{d}{dt}\int_{0}^{t} \brkt{\int_s^t \frac{(t-u)^\alpha (u-s)^{-\alpha}}{\Gamma(1+\alpha)\Gamma(1-\alpha)}\mathop{du}} \mathop{dX_s}
  \end{eqnarray*}
  Changing variable with $v:= \frac{u-s}{t-s}$,
  \begin{eqnarray*}
   &&\int_s^t \frac{(t-u)^\alpha (u-s)^{-\alpha}}{\Gamma(1+\alpha)\Gamma(1-\alpha)}\mathop{du}\\
   &=& \int_0^1 \frac{(t-s - (t-s)v)^\alpha (t-s)^{-\alpha}}{\Gamma(1+\alpha)\Gamma(1-\alpha)}(t-s)\mathop{dv}\\
   &=& \frac{(t-s)}{\Gamma(1+\alpha)\Gamma(1-\alpha)}\int_0^1 (1-v)^\alpha v^{-\alpha} \mathop{dv}
  \end{eqnarray*}
Note that the beta function 
\begin{eqnarray*}
  B(x,y) &=& \int_0^1 t^{x-1} (1-t)^{y-1} \mathop{dt}\\
  &=& \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}
\end{eqnarray*}
for $x, y \in \mathbb{R}_+$.
Then,
\begin{eqnarray*}
   &&\int_s^t \frac{(t-u)^\alpha (u-s)^{-\alpha}}{\Gamma(1+\alpha)\Gamma(1-\alpha)}\mathop{du}\\
   &=& \frac{(t-s)}{\Gamma(1+\alpha)\Gamma(1-\alpha)} B(1+\alpha, 1-\alpha)\\
   &=& \frac{(t-s)}{\Gamma(1+\alpha)\Gamma(1-\alpha)} \frac{\Gamma(1+\alpha)\Gamma(1-\alpha)}{\Gamma(2)}\\
   &=& t-s
  \end{eqnarray*}
  Plugging it back,
  \begin{eqnarray*}
	&&\int_0^t \frac{(t-u)^\alpha}{\Gamma(1+\alpha)}\mathop{dX^{(\alpha)}_u}\nonumber\\
	&=& \frac{d}{dt}\int_0^t t-s \mathop{dX_s}\\
%	&=& \frac{d^2}{dt^2}\int_0^t \frac{t-s}{\Gamma(2)} X_s \mathop{ds}\\
	&=& \frac{d^2}{dt^2}\int_0^t (t-s) X_s \mathop{ds}\\
	&=& \frac{d^2}{dt^2}\int_0^t X_s \int_s^t \mathop{du}  \mathop{ds}\\
	&=& \frac{d^2}{dt^2} \int_0^t \int_s^t X_s \mathop{du}  \mathop{ds}\\
	&=& \frac{d^2}{dt^2} \int_0^t \int_0^u X_s \mathop{ds}  \mathop{du}\\
	&=& X_t
  \end{eqnarray*}
\end{proof}
  \begin{example}
  The truncated fOU process driven by $U_{\alpha, t}$ is 
  \begin{eqnarray}
  F_{\alpha,t} = \gamma\int_0^t e^{-a(t-u)}\mathop{dU_{\alpha, u}} 
  \label{sec:cc2}
\end{eqnarray}
In terms of (\ref{sec:frac})
\begin{eqnarray*}
  c(u) &=&  \gamma e^{-au}
\end{eqnarray*}
And according to  (\ref{sec:frac3}),
\begin{eqnarray*}
  &&g(u)\\
  &=& \frac{\frac{d}{du}\int_0^{u} c(s)(u-s)^\alpha \mathop{ds}}{\Gamma(1+\alpha)}\\
  &=& \frac{\gamma}{\Gamma(1+\alpha)}\frac{d}{\mathop{du}}\int_0^u e^{-as} (u-s)^\alpha \mathop{ds}
\end{eqnarray*}
Using partial integration,
\begin{eqnarray*}
  &&g(u)\\
  &=& \frac{\gamma}{\Gamma(1+\alpha)}\brkt{\brkt{\frac{d}{\mathop{du}} (-\frac{1}{1+\alpha}e^{-as}(u-s)^{1+\alpha}|_{s=0}^u)} - \brkt{\frac{d}{\mathop{du}}(\int_0^u -a e^{-as}(u-s)^\alpha)\mathop{ds}}}\\
  &=& \frac{\gamma}{\Gamma(1+\alpha)}\brkt{\brkt{\frac{d}{\mathop{du}}(\frac{u^{1+\alpha}}{1+\alpha})} - a e^{-au}\brkt{\int_0^u e^{-as}s^\alpha\mathop{ds}}}\\
  &=& \frac{\gamma}{\Gamma(1+\alpha)} (u^\alpha - a e^{-au}\int_0^u e^{-as}s^\alpha\mathop{ds})
\end{eqnarray*}
According to (\ref{sec:wer}), $h(u) = \gamma (1 + \frac{a e^{-au}\int_0^u e^{-as}s^\alpha\mathop{ds}}{u^\alpha})$. It is clear $h'(0)\neq 0$ and $h\in C^2([0,T])$. I.e., its fractional derivative of order $\alpha$ is 
\begin{eqnarray}
  F^{(\alpha)}_t &=&  \int_0^t c(t-u) \mathop{dB_u}\nonumber\\
  &=& \gamma\int_0^t e^{-a(t-u)}\mathop{dB_u}
  \label{sec:wie}
\end{eqnarray}
and 
\begin{eqnarray}
  F_{\alpha,t} &=& \int_0^t \frac{(t-s)^\alpha}{\Gamma(1+\alpha)} \mathop{dF^{(\alpha)}(s)}.
  \label{sec:was}
\end{eqnarray}

Obviously, $F^{(\alpha)}_t$ is the solution of follows (as by fOU, c.f. Theorem \ref{sec:sol})
\begin{eqnarray*}
  dF^{(\alpha)}(t) &=& -a F^{(\alpha)}(t)\mathop{dt} + \gamma \mathop{dB_t},\hspace{3mm}F^{(\alpha)}(0)=0
\end{eqnarray*}
I.e., $(F^{(\alpha)}_t)_{t\ge 0}$ is the Ornstein-Uhlenbeck process.
\end{example}

In order to approximate a fOU $ F_{\alpha,t}$ on a discrete time scale, according to (\ref{sec:was}), Comte and Renault define an approximation by step functions
\begin{eqnarray}
 \tilde{F}_{\alpha,n}(t) = \sum_{k=1}^{\floor{nt}} \frac{(t - \frac{k-1}{n})^\alpha}{\Gamma(1+\alpha)} \brkt{F^{(\alpha)}_{\frac{k}{n}} - F^{(\alpha)}_{\frac{k-1}{n}}}
  \label{sec:cc7}
\end{eqnarray}
for $n\in \mathbb{N}, t\in\mathbb{R}_+$.
\begin{theorem}
  $\tilde{F}_{\alpha,n}(t)\rightarrow F_{\alpha,n}(t)$ in distribution as $n$ tends to infinity.  
  \label{sec:cont}
\end{theorem}
\begin{proof}
  See. \cite{comren}, p.298, Proposition 3.1 .
\end{proof}
%We suppose the initial value $F_{\alpha, 0}$ is centered Gaussian. Then  ${F}_{\alpha, n}(t)$ is asymptotically stationary and Gaussian solution of fOU,  an AR(1) approach of $F^{(\alpha)}$ could be given,
Notice that, $F^{(\alpha)}(t)$ is an $AR(1)$ process due to (\ref{sec:wie}), i.e.,
\begin{eqnarray}
  (1-c_nL_n) {F}^{(\alpha)}_{\frac{k}{n}} = \epsilon(\frac{k}{n})
  \label{sec:cc6}
\end{eqnarray}
where $\epsilon(\cdot) \sim \mathcal{N}(0, \mathrm{Var}[F^{(\alpha)}_{\cdot}]), L_n $ represents the lag operator such that $L_n F^{(\alpha)}_{\frac{k}{n}} = F^{(\alpha)}_{\frac{k-1}{n}}$  and $c_n$ are normalized coefficients corresponding to (\ref{sec:cc7}). Moreover, on the one hand, as n goes to infinity, we have asymptotically following equation in distribution .
\begin{eqnarray}
  F_{\alpha,n}(\frac{k}{n}) &=& \sum_{l=1}^{k} \frac{(\frac{k}{n} - \frac{l-1}{n})^\alpha}{\Gamma(1+\alpha)} \brkt{F^{(\alpha)}_{\frac{l}{n}} - F^{(\alpha)}_{\frac{l-1}{n}}}\nonumber\\
  &=& \sum_{l=1}^{k} \frac{(k - l + 1)^\alpha}{\Gamma(1+\alpha)n^\alpha} \brkt{F^{(\alpha)}_{\frac{l}{n}} - F^{(\alpha)}_{\frac{l-1}{n}}}\nonumber\\
  &\overset{m=k-l}{=}& \sum_{m=0}^{k-1} \frac{(m + 1)^\alpha}{\Gamma(1+\alpha)n^\alpha} \brkt{F^{(\alpha)}_{\frac{k-m}{n}} - F^{(\alpha)}_{\frac{k-m-1}{n}}}\nonumber\\ 
  &=& \brkt{\sum_{m=0}^{k-1} \frac{(m + 1)^\alpha - m^\alpha}{\Gamma(1+\alpha)n^\alpha} \cdot L_n^m} F^{(\alpha)}_{\frac{k}{n}} \nonumber\\
  &\overset{(\ref{sec:cc6})}{=}&  \brkt{\sum_{m=0}^{k-1} \frac{(m + 1)^\alpha - m^\alpha}{\Gamma(1+\alpha)n^\alpha} \cdot L_n^m} (1-c_nL_n)^{-1}\epsilon(\frac{k}{n}).
\end{eqnarray}
We rewrite this then
\begin{eqnarray*}
  (1-c_nL_n)\brkt{\sum_{m=0}^{k-1} \frac{(m + 1)^\alpha - m^\alpha}{\Gamma(1+\alpha)n^\alpha} \cdot L_n^m}^{-1}  F_{\alpha,n}(\frac{k}{n}) =\epsilon(\frac{k}{n}).
\end{eqnarray*}

On the other hand, if we take consideration with the $ARFIMA(1, \alpha, 0)$, replaced the content with $\{\}^{-1}$ by the $\alpha$-integrated term $(1-L_n)^\alpha$. Then we have
\begin{eqnarray*}
  (1-c_nL_n) (1-L_n)^\alpha F_{\alpha,n}(\frac{k}{n}),
\end{eqnarray*}
which is in general not Gaussian. Furthermore, $(1-L_n)^\alpha F_{\alpha,n}$ is in general not a $AR(1)$.  In other words, the ARFIMA$(1, \alpha, 0)$ may not fit such a high-frequency data that is comfortable with the fractional stochastic volatility model driven by fBm. 

\subsection{Fractional Stochastic Volatility Model}
In framework of the Black-Scholes model, a risky asset price is modelded as follows
\begin{eqnarray}
  \mathop{dS_t} &=& r_tS_t\mathop{dt} + \sigma_t S_t \mathop{dB_t},\hspace{3mm} t\in [0, T],
\end{eqnarray}
for $r, \sigma \in \mathbb{R}_{+}$. In the simplest case, the volatility is assumed as a constant or a deterministic function of time and underlying price of the asset. Such models, however, generate unrealistic volatility dynamics. To solve this problem, in 
such  Hull-White, Heston or SABR models, the volatility also modelded as a stochastic process(e.g. semimartingale).\\
In this thesis,  the log-volatility $\log(\sigma)_{t \ge 0}$ is assumed to obey fractional Ornstein-Uhlenbeck process $(X_t)_{t\ge0}$. That means

\begin{eqnarray}
  \sigma_t &=& \exp\{X_t\} \nonumber\\
  \mathop{dX_t} &=&  -a X_t \mathop{dt} + \gamma \mathop{dU_H(t)}
  \label{sec:fv}
\end{eqnarray}
where $a, \gamma \in \mathbb{R}_{+}$. In the proceeding section, we have a stationary solution 
\begin{eqnarray}
\hat{X}_{H,t}= e^{-at}\int_{-\infty}^t e^{au}\mathop{dU_H(u)}
\label{sec:fv2}
\end{eqnarray}
for an appropriate initial condition. Note that, recall the (\ref{sec:cc2}), $\hat{X}_{H,t} -  F_{H-\frac{1}{2},t} = \\e^{-at}(X_0 - F_{H-\frac{1}{2},0})\rightarrow 0 $, as $t\rightarrow \infty$. We set $\hat{\sigma}_{H,t} := \exp\{\hat{X}_{H,t}\}$, which has following property. %As result of this, $(\hat{\sigma}_{H,t}):= \exp\{\hat{X}_{H,t}\}$ is weak stationary, we show this in following.


\begin{proposition}
  Let $\hat{X}_{H,t}$ be such that as in (\ref{sec:fv2}) and $\hat{\sigma}_{H,t} = \exp\{\hat{X}_{H,t}\}$, then $(\hat{\sigma}_{H,t})$ is weak stationary and has long memory for $H\in(\frac{1}{2}, 1)$. 
\end{proposition}
\begin{proof}
  We start our proof by definition of the autocovariance of $\hat{\sigma}_{H,t}$
  \begin{eqnarray*}
	&& \varsigma_{\hat{\sigma}_H}(\tau) \\
	&=& \mathrm{Cov}[\hat{\sigma}_{H,t}, \hat{\sigma}_{H, t+\tau}]\\
	&=& \mathrm{E}[\hat{\sigma}_{H,t} \hat{\sigma}_{H, t+\tau}] - \mathrm{E}[\hat{\sigma}_{H,t}]\mathrm{E}[\hat{\sigma}_{H,t+\tau}]\\
	&=& \mathrm{E}[\exp(\hat{X}_{H,t} + \hat{X}_{H, t+\tau})] - \mathrm{E}[\exp(\hat{X}_{H,t})]\mathrm{E}[\exp(\hat{X}_{H,t+\tau})] 
  \end{eqnarray*}
  Since $\hat{X}_{H,t}, \hat{X}_{H,t+\tau}$ are centred Gaussian, we apply (\ref{sec:expgau}) for it
  \begin{eqnarray*}
	&& \varsigma_{\hat{\sigma}_H}(\tau) \\
	&=& \exp(\frac{1}{2}\mathrm{Var}[\hat{X}_{H,t} + \hat{X}_{H, t+\tau}]) - \exp(\frac{1}{2}\mathrm{Var}[\hat{X}_{H,t}])  \exp(\frac{1}{2}\mathrm{Var}[\hat{X}_{H,t+\tau}])\\
  \end{eqnarray*}
  Since $(\hat{X}_{H,t})_t$ is stationary, we have
  \begin{eqnarray*}
	&& \varsigma_{\hat{\sigma}_H}(\tau) \\
	&=&  \exp(\frac{1}{2}\mathrm{Var}[\hat{X}_{H,t} + \hat{X}_{H, t+\tau}]) - \exp(\mathrm{Var}[\hat{X}_{H,t}])\\
	&=& \exp(\frac{1}{2}(\mathrm{Var}[\hat{X}_{H,t}] + \mathrm{Var}[\hat{X}_{H,t+\tau}] + 2\mathrm{E}[\hat{X}_{H,t}\hat{X}_{H,t+\tau}])) - \exp(\mathrm{Var}[\hat{X}_{H,t}])\\
	&=& \exp(\mathrm{Var}[\hat{X}_{H,t}] + \mathrm{Cov}[\hat{X}_{H,t}, \hat{X}_{H,t+\tau}]) - \exp(\mathrm{Var}[\hat{X}_{H,t}])\\
  \end{eqnarray*}
  The term $\mathrm{Var}[\hat{X}_{H,t}]$ is independent of $\tau$. We define $\mathrm{Var}[\hat{X}_{H,t}] = C$. Then
\begin{eqnarray*}
	&& \varsigma_{\hat{\sigma}_H}(\tau) \\
	&=& \exp(C) \exp(\varsigma_{\hat{X}_{H}}(\tau)) - \exp(C)\\
	&=& \exp(C)(\exp(\varsigma_{\hat{X}_{H}}(\tau)) - 1)\\
	&=& \kappa(\exp(\varsigma_{\hat{X}_{H}}(\tau)) - 1)
 \end{eqnarray*}
 for some $\kappa$. Obviously, $\mathbb{E}[\hat{\sigma}_{H}(t)]=1$. With $ \varsigma_{\hat{\sigma}_H}(0) = \kappa(\exp(\varsigma_{\hat{X}_{H}}(0)) - 1)$, the first claim is proved.\\
 On the one hand, consider in Theorem \ref{sec:autocov},
\begin{eqnarray*}
  \varsigma_{\hat{X}_{H}}(\tau)) = \mu \tau^{2H-2} + \mathrm{O}(\tau^{{2H-2N-2}})
\end{eqnarray*}
for some $\mu$.
 $\varsigma_{\hat{X}_{H}}(\tau)$ vanishes, for $H\in(\frac{1}{2}, 1)$, as $\tau$ goes to infinity. I.e.,
 $$
 \lim\limits_{\tau\rightarrow \infty}\kappa(\exp(\varsigma_{\hat{X}_{H}}(\tau)) - 1) = 0.
 $$
 On the other hand, 
And $\varsigma_{\hat{X}_{H}}(\tau)$ is the equivalence infinitesimal of $\exp(\varsigma_{\hat{X}_{H}}(\tau)) - 1$. Hence,
\begin{eqnarray*}
  &&  \lim\limits_{\tau\rightarrow \infty} |\frac{\varsigma_{\hat{\sigma}_H}(\tau) - \kappa \mu \tau^{2H-2}}{\tau^{{2H-2N-2}}}|\\
  &=& \lim\limits_{\tau\rightarrow \infty} |\frac{\kappa(\exp(\varsigma_{\hat{X}_{H}}(\tau)) - 1) - \kappa \mu \tau^{2H-2}}{\tau^{{2H-2N-2}}}|\\
  &=& \lim\limits_{\tau\rightarrow \infty} |\frac{\kappa(\varsigma_{\hat{X}_{H}}(\tau) - \mu\tau^{2H-2})}{\tau^{{2H-2N-2}}}|\\
  &<& \infty
\end{eqnarray*}
This implies $\varsigma_{\hat{\sigma}_H}(\tau) = \kappa\mu \tau^{2H-2} + \mathrm{O}(\tau^{{2H-2N-2}})$. For the same reason as in Theorem \ref{sec:lmemory}, the long memory property requires $1<2H$, namely, $H\in(\frac{1}{2}, 1)$.
\end{proof}

The long memory property may explain why in stock market, large upheavals tend to be followed by large upheavals and small upheavals often happen after by small upheavals.  In order to model long memory volatility process, Comte and Renault are forced to set $H\in (\frac{1}{2}, 1)$ in (\ref{sec:fv2}) named fractional volatility stochastic (FSV), see\cite{comren}. \\

\subsection{Rough Fractional Stochastic Volatility Model}
In order to generate a desirable volatility dynamics, Gatheral et al.(2014) take the implied volatility $\sigma^{BS}(m, \tau)$ into account, where $m$ is the log-moneyness and $\tau$ is time to expiration date. The implied volatility refer to the value of volatility required in the Black-Scholes model such that the pricing is coincide with the asset price we observed.  Graphing implied Volatility as a function of moneyness and time to expiration seems to be a U-shape which is so-called volatility smile. In particular, the term structure of volatility skew of at-the-money of stylized data
\begin{eqnarray*}
  \kappa(\tau)=|\frac{\partial}{\partial m}\sigma^{BS}(m, \tau)|_{m=0}|,
\end{eqnarray*}
acts as a power law with exponent around $-\frac{1}{2}$, which is explained for example in \cite{gradin}. On the one hand, in the FSV model with $H \in (\frac{1}{2}, 1)$, the volatility smile is depressed by arising $\tau$, see \cite{comteetla}, p.350, Eq. (4.7). On the other hand,  in \cite{fukasawa}, the volatility is driven by FBM with Hurst exponent $H$ in Fukasawa's model whose volatility skew of at-the-money has a form $\kappa(\tau) \sim \tau^{H-\frac{1}{2}}$ as $\tau$ goes to zero. This requires that $H$ is near zero to match the power law decay of $\kappa(\tau)$.  All of this suggest us, we should apply a stochastic volatility model which is driven by FBM with Hurst exponent $H \in (0, \frac{1}{2})$.\\
Replaced by $H \in (0, \frac{1}{2})$ in (\ref{sec:fv}), a 'rough fractional stochastic volatility model' (RFSV) is
\begin{eqnarray}
  \mathop{dX_t} = -a X_t \mathop{dt} + \gamma \mathop{dU_H(t)},\hspace{3mm} t\in[0, T]
  \label{sec:rfm}
\end{eqnarray}
for $a,\gamma \in \mathbb{R}_+$ and a stationary solution
\begin{eqnarray}
\hat{X}_{H,t}=e^{-at}\gamma\int_{-\infty}^t e^{au}\mathop{dU_H(u)}.
\label{sec:rfm2}
\end{eqnarray}

Consider a quantity defined on $[0, T]$ with mesh $\tau$
\begin{eqnarray}
  s(\tau, \sigma) = \frac{1}{N}\sum\limits_{k=1}^N|\log(\sigma_{k\tau}) - \log(\sigma_{(k-1)\tau})|^2
  \label{sec:smo}
\end{eqnarray}
Where $N = \floor{T/\tau}$. This quantity could describe the smoothness of $(\sigma_t)_t$. Due to the volatilities are not observable, we could take spot volatility values to estimate them. In \cite{chridito}, the daily spot variances by daily realized variance estimates $\tilde{\sigma}$ are used. A plotting of $\log(s(\tau, \tilde{\sigma}))$ against $\log(\tau, \tilde{\sigma})$ looks then as straight line, i.e.,
\begin{eqnarray}
  s(\tau, \tilde{\sigma}) =  k\tau^z.
  \label{sec:smth}
\end{eqnarray}
RFSV model does match the observed phenomenon. Replaced  $\sigma(t)$ by $\hat{X}_{H,t}$ 
\begin{eqnarray}
  s(\tau, \hat{X}_{H}) &=&  \frac {1}{N}\sum\limits_{k=1}^N|\hat{X}_{H,k\tau} - \hat{X}_{H,(k-1)\tau}|^2\nonumber
\end{eqnarray}

Since $(\hat{X}_{H,t})$ is stationary, we could apply weak law of large number, as $N$ goes to infinity,
\begin{eqnarray}
  s(\tau, \hat{X}_{H}) &\overset{\tau\downarrow0}{\rightarrow}& \mathrm{E}[|\hat{X}_{H,t+\tau}-\hat{X}_{H,t}|^2]\nonumber\\
  &=& 2 \mathrm{Var}[\hat{X}_{H,t}] - 2 \mathrm{Cov}[\hat{X}_{H,t}, \hat{X}_{H,t+\tau}]\nonumber
  \label{sec:smtsv}
\end{eqnarray}

\begin{theorem}
  Let $(\hat{X}_{H, t, a})$ be as in (\ref{sec:rfm2}) driven by a fBm $(U_H(t))$ with $H\in (0,\frac{1}{2})$, then
  \begin{eqnarray*}
	\mathrm{E}[\sup\limits_{t\in [0, T]}|\hat{X}_{H,t+\tau, a} - \gamma U_H(t)|]\rightarrow 0
  \end{eqnarray*}
 as $a$ goes to zero for $T > 0$.
\end{theorem}
\begin{proof}
  Cf. \cite{gradin}, p.15, Proposition 3.1 .
\end{proof}

The theorem shows if $a$ is small enough, $(\hat{X}_{H, t})_t$ behaves essentially as fBm at a compact time scale.

\begin{theorem}
  Let $(\hat{X}_{H, t, a})$ the solution by (\ref{sec:rfm2}) with $H\in(0, \frac{1}{2})$, then 
  \begin{eqnarray}
	\mathrm{Var}[\hat{X}_{H,t, a}] - \mathrm{Cov}[\hat{X}_{H,t, a}, \hat{X}_{H,t+\tau, a}] \rightarrow \frac{1}{2} \gamma^2\tau^{2H}
	\label{sec:rfsv}
  \end{eqnarray}
 as $a$ goes to zero, for $t>0, \tau>0$.
\end{theorem}
\begin{proof}
  Cf. \cite{gradin}, p.16, Corollary 3.2 .
\end{proof}

(\ref{sec:rfsv}) shows, the choise of $H\in(0,\frac{1}{2})$ enable us to model the log-volatility process has a form of (\ref{sec:smth}). It may turn out to be RFSV is more reasonable than FSV for this empirical result.

\subsection{Weighted Fractional Stochastic Volatility Model}
\begin{definition}
  A \emph{mixed fractional Brownian motion} is defined as follows
\begin{eqnarray}
  M_{\alpha,\beta,H_1,H_2}(t) = \alpha U_{H_1}(t) + \beta U_{H_2}(t)
  \label{sec:mfsv}
\end{eqnarray}
for $t\in \mathbb{R}$, where $\alpha, \beta$ are real numbers and $U_{H_1}, U_{H_2}$ are two independent fBm's with Hurst exponents $H_1 \in (0, \frac{1}{2}), H_2 \in (\frac{1}{2}, 1)$ respectively.
\end{definition}

\begin{proposition}
  The mixed fractional Brownian motion $M_{\alpha,\beta,H_1,H_2}(t)_{t\in\mathbb{R}}$ has following properties
   \begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
	 \item $M_{\alpha,\beta,H_1,H_2}(0) = 0$ and $(M_{\alpha,\beta,H_1,H_2}(t))_t$ is a centered Gaussian  process.
	 \item $\mathrm{Cov}[M_{\alpha,\beta,H_1,H_2}(t), M_{\alpha,\beta,H_1,H_2}(s)] = \alpha^2 \mathrm{Cov}[U_{H_1}(t), U_{H_1}(s)] + \beta^2\mathrm{Cov}[U_{H_2}(t), U_{H_2}(s)] = \frac{1}{2}\brkt{\alpha^2(t^{2H_1}+s^{2H_1}+|t-s|^{2H_1}) + \beta^2(t^{2H_2} + s^{2H_2} + |t-s|^{2H_2})}$. 
	 \item $M_{\alpha, \beta, H_1, H_2}(qt) \sim M_{\alpha q^{H_1}, \beta q^{H_2}, H_1, H_2}(t)$, for $q\in\mathbb{R}$.
	 \end{enumerate}
	 \label{sec:prop}
\end{proposition}

\begin{proof}
  (i): $\mathrm{E}[M_{\alpha,\beta,H_1,H_2}(t)] = \alpha \mathrm{E}[U_{H_1}(t)] + \beta \mathrm{E}[U_{H_2}(t)] = \alpha \cdot 0 + \beta \cdot 0 = 0$.
  Consider,
  \begin{eqnarray*}
	&&\sum\limits_{k=1}^{d} c_k M_{\alpha,\beta,H_1,H_2}(k)\\
	&=& \sum\limits_{k=1}^{d} c_k (\alpha U_{H_1}(k) + \beta U_{H_2}(k))\\
	&=& \sum\limits_{k=1}^{d} c_k \alpha U_{H_1}(k) + \sum\limits_{k=1}^{d} c_k \beta U_{H_1}(k)
  \end{eqnarray*}
  Since $U_{H_1}(t), U_{H_2}(t)$ are independent and $(U_{H_1}(t))_t, (U_{H_2}(t))_t$ are centered Gaussian process, $\sum\limits_{k=1}^{d} c_k \alpha U_{H_1}(k) + \sum\limits_{k=1}^{d} c_k \beta U_{H_2}(k)$ is centered Gaussian and therefore $(M_{\alpha,\beta,H_1,H_2}(t))_t$ is centered Gaussian process. \\
  (ii): Using independence of $U_{H_1}(t)$ and $U_{H_2}(t)$, we have
  \begin{eqnarray*}
	&&\mathrm{Cov}[M_{\alpha,\beta,H_1,H_2}(t), M_{\alpha,\beta,H_1,H_2}(s)]\\
	&=& \mathrm{Cov}[\alpha U_{H_1}(t)+\beta U_{H_2}(t), \alpha U_{H_1}(t)+\beta U_{H_2}(s)]\\
	&=& \mathrm{E}[(\alpha U_{H_1}(t)+\beta U_{H_2}(t)) (\alpha U_{H_1}(t)+\beta U_{H_2}(s))]\\
	&=& \mathrm{E}[\alpha^2U_{H_1}(t)U_{H_1}(s)] + \underbrace{\mathrm{E}[\alpha \beta U_{H_2}(t)U_{H_1}(s)]}_{=0} + \underbrace{\mathrm{E}[\alpha\beta U_{H_1}(t)U_{H_2}(s)]}_{=0} +\mathrm{E}[\beta^2 U_{H_2}(t)U_{H_2}(s)] \\
	&=& \alpha^2 \mathrm{Cov}[U_{H_1}(t), U_{H_1}(s)] + \beta^2\mathrm{Cov}[U_{H_2}(t), U_{H_2}(s)].
  \end{eqnarray*}
And the rest is clear.\\
(iii): 
\begin{eqnarray*}
  M_{\alpha,\beta,H_1,H_2}(qt) &=&  \alpha U_{H_1}(qt)+\beta U_{H_2}(qt)\\
  &\sim& \alpha q^{H_1} U_{H_1}(t) + \beta q^{H_2} U_{H_2}(t)\\
  &=& M_{\alpha q^{H_1}, \beta q^{H_2}, H_1, H_2}(t)
\end{eqnarray*}
\end{proof}
We could give our stochastic volatility model driven by the mixed fBm. Given all parameter as in the assumption as before, we have 

\begin{eqnarray}
  \mathop{dX_{\alpha,\beta,H_1,H_2}(t)} = -aX_{\alpha,\beta,H_1,H_2}(t)\mathop{dt} + \gamma \mathop{dM_{\alpha,\beta,H_1,H_2}(t)}
  \label{sec:wfsv}
\end{eqnarray}
for $t\ge 0$ and where $a, \gamma \in \mathbb{R}_+$.

\begin{proposition}
  For an appropriate initial condition of (\ref{sec:wfsv}), there exists a solution $\hat{X_t}$ satisfying follwing properties
\begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
  \item $(\hat{X_t})_{t\ge 0}$ is a centered Gaussian stationary process.
  \item $\hat{X_t}$ has long memory.
  \end{enumerate}
\end{proposition}
\begin{proof}
  In terms of (\ref{sec:wfsv}), then
  \begin{eqnarray*}
	X_{\alpha,\beta,H_1,H_2}(t) = X_{\alpha,\beta,H_1,H_2}(0) - a\int_0^t X_u \mathop{du} + \gamma M_{\alpha,\beta,H_1,H_2}(t).
\end{eqnarray*}
Recall by (\ref{sec:rie}), the integral in sense of Riemann-Stieljet of $\int_0^t e^{au} \mathop{dM_{\alpha,\beta,H_1,H_2}}$ is well-defined because $\int_0^t e^{au} \mathop{dU_{H_i}}$ is well-defined for $i\in\{1,2\}$.

As in Theorem \ref{sec:sol}, we have the solution
\begin{eqnarray*}
  &&X_t\\ 
  &=& e^{-at} \brkt{\gamma\int_0^t e^{au} \mathop{dM_{\alpha,\beta,H_1,H_2}} + X_0}\\
  &=&  e^{-at} \brkt{\gamma\int_0^t e^{au} \mathop{d(\alpha U_{H_1} + \beta U_{H_2})} + X_0}\\
  &=&  e^{-at} \brkt{\alpha \gamma\int_0^t e^{au} \mathop{d U_{H_1}} + \beta \gamma\int_0^t e^{au} \mathop{d U_{H_2}} + X_0}
\end{eqnarray*}
Given $X_0$ so that
\begin{eqnarray}
  \hat{X}_{\alpha,\beta,H_1,H_2}(t) &=&  \alpha \underbrace{\gamma e^{-at}\int_{-\infty}^t e^{au} \mathop{d U_{H_1}}}_{:=J_{H_1}(t)} + \beta\underbrace{ \gamma e^{-at}\int_{-\infty}^t e^{au} \mathop{d U_{H_2}}}_{:=J_{H_2}(t)}.
  \label{sec:jjj}
\end{eqnarray}
Notice $(J_{H_1}(t)), (J_{H_2}(t))$ are stationary fractional Ornstein-Uhlenbeck process. Since $J_{H_1}(t), J_{H_2}(t)$ defined as integral of $U_{H_1}, U_{H_2}$ of Riemann-Stieljes sense, they are therefore independent.  Hence 
\begin{eqnarray*}
  && \sum\limits_{k=1}^{d} c_k \hat{X}_{\alpha,\beta,H_1,H_2}\\
  &=& \sum\limits_{k=1}^{d} c_k \alpha J_{H_1}(k) + \sum\limits_{k=1}^{d} c_k \beta J_{H_2}(k)
\end{eqnarray*}
are centered Gaussian.\\
For (i): 
 \begin{eqnarray*}
   &&\mathrm{E}[\exp(i\xi\sum\limits_{k=1}^{d} c_k X_{\alpha,\beta,H_1,H_2}(k))]\\
   &=& \mathrm{E}[\exp(i\xi\sum\limits_{k=1}^{d} c_k (\alpha J_{H_1}(k) + \beta J_{H_2}(k)))]\\
   &=& \mathrm{E}[\exp(i\xi\sum\limits_{k=1}^{d} c_k (\alpha J_{H_1}(k)))]\mathrm{E}[\exp(i\xi\sum\limits_{k=1}^{d} c_k (\beta J_{H_2}(k)))] \\
   &=& \mathrm{E}[\exp(i\xi\sum\limits_{k=1}^{d} c_k (\alpha J_{H_1}(k+s)))]\mathrm{E}[\exp(i\xi\sum\limits_{k=1}^{d} c_k (\beta J_{H_2}(k+s)))] \\
   &=& \mathrm{E}[\exp(i\xi\sum\limits_{k=1}^{d} c_k (\alpha J_{H_1}(k+s) + \beta J_{H_2}(k+s)))]
  \end{eqnarray*}
  for a fixed $s$. This shows $(\hat{X}_{\alpha,\beta,H_1,H_2}(t))$ is stationary.\\
  For (ii): 
  \begin{eqnarray*}
	&&\varsigma(\tau) \\
	&=& \mathrm{Cov}[\hat{X}_{\alpha,\beta,H_1,H_2}(0), \hat{X}_{\alpha,\beta,H_1,H_2}(\tau)]\\
	&=& \mathrm{E}[\hat{X}_{\alpha,\beta,H_1,H_2}(0) \hat{X}_{\alpha,\beta,H_1,H_2}(\tau)]\\
	&=& \mathrm{E}[(\alpha J_{H_1}(0) + \beta J_{H_2}(0))(\alpha J_{H_1}(\tau) + \beta J_{H_2}(\tau)]\\
	&=& \alpha^2\mathrm{E}[J_{H_1}(0) J_{H_1}(\tau)] + \alpha\beta\mathrm{E}[J_{H_1}(0)J_{H_2}(\tau)] + \alpha\beta\mathrm{E}[J_{H_2}(0)J_{H_1}(\tau)] + \beta^2\mathrm{E}[J_{H_2}(0)J_{H_2}(\tau)].
  \end{eqnarray*}
   They are centered Gaussian, using independece again, we have
  \begin{eqnarray*}
	&&\varsigma(\tau) \\
	&=& \alpha^2\mathrm{E}[J_{H_1}(0) J_{H_1}(\tau)] + \alpha\beta\mathrm{E}[J_{H_1}(0)]\mathrm{E}[J_{H_2}(\tau)] \\
	&+& \alpha\beta\mathrm{E}[J_{H_2}(0)]\mathrm{E}[J_{H_1}(\tau)] + \beta^2\mathrm{E}[J_{H_2}(0)J_{H_2}(\tau)]\\
	&=& \alpha^2\mathrm{E}[J_{H_1}(0) J_{H_1}(\tau)] + \beta^2\mathrm{E}[J_{H_2}(0)J_{H_2}(\tau)]\\
	&\overset{\text{Theorem \ref{sec:autocov}}}{=}& \frac{1}{2} (\alpha\gamma)^2 \sum_{k=1}^{N} a^{-2k}\brkt{\prod_{j=0}^{2k-1}(2H_1-j)}\tau^{2(H_1-k)} + \mathrm{O}(\tau^{{2H-2N-2}})\\
  &+& \frac{1}{2} (\beta\gamma)^2 \sum_{k=1}^{N} a^{-2k}\brkt{\prod_{j=0}^{2k-2}(2H_2-j)}\tau^{2(H_2-k)} + \mathrm{O}(\tau^{{2H-2N-2}}).
  \end{eqnarray*}
  \end{proof}
  $\varsigma(\tau)$ tends to zero as $\tau$ goes to zero. As by Corollary \ref{sec:longmer},  the last line of the equation diverges, when $H_2 \in (\frac{1}{2}, 1)$. Thus, $(\hat{X}_t)$ has long memory property. 

  \begin{definition}
	We add a restriction for $\alpha > 0, \beta > 0$ such that $\alpha^2 + \beta^2 = 1$ to (\ref{sec:wfsv}) and let $a$ so small such that it close to zero , then we get our \emph{weighted fractional Brownian motion}.
  \end{definition}
  
\begin{proposition}
  Let $M_{\alpha,\beta,H_1,H_2}$ be a weighted fractional brownian motion with respect to $U_{H_1}$ and $U_{H_2}$, $T, \tau>0$, $a, \gamma$ are defined by (\ref{sec:wfsv}), $J_{H_1}, J_{H_2}$ are defined by (\ref{sec:jjj}), $\phi=H_1-\frac{1}{2}, \psi=H_2-\frac{1}{2}$. Then, for $t\in [0, T]$,
\begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]	
  \item $\mathrm{E}[\sup\limits_{t\in[0,T]}|\hat{X}_{\alpha,\beta,H_1,H_2}(t) - U_{H_1}(t)|] \rightarrow 0$
	as $a\rightarrow 0, \alpha\rightarrow 1$.
  \item $\mathrm{E}[|\hat{X}_{\alpha,\beta,H_1,H_2}(t+\tau) - \hat{X}_{\alpha,\beta,H_1,H_2}(t)|^2] \rightarrow  \gamma^2 \tau^{2H}$
	as $a\rightarrow 0, \alpha\rightarrow 1$.
  \item Let $n\in \mathbb{N}$, define
	\begin{eqnarray*}
	  &&\tilde{X}_{\alpha,\beta,H_1,H_2}(t) \\
	  &:=&  \alpha\sum_{k=1}^{\floor{nt}} \frac{(t - \frac{k-1}{n})^\phi}{\Gamma(1+\phi)} \brkt{J_{H_1}^{(\phi)}(\frac{k}{n}) - J_{H_1}^{(\phi)}(\frac{k-1}{n})} \\
	  &+& \beta\sum_{k=1}^{\floor{nt}} \frac{(t - \frac{k-1}{n})^\psi}{\Gamma(1+\psi)} \brkt{J_{H_2}^{(\psi)}(\frac{k}{n}) - J_{H_2}^{(\psi)}(\frac{k-1}{n})}
	  \label{sec:disc}
	\end{eqnarray*}
	then, as $n$ goes to infinity,
	\begin{eqnarray*}
	  \tilde{X}_{\alpha,\beta,H_1,H_2}(t) \rightarrow \hat{X}_{\alpha,\beta,H_1,H_2}(t)
	\end{eqnarray*}
	in distribution.
  \end{enumerate}
  \end{proposition}
  \begin{proof}
(i): 
\begin{eqnarray*}
  &&\mathrm{E}[\sup\limits_{t\in[0,T]}|\hat{X}_{\alpha,\beta,H_1,H_2}(t) - U_{H_1}|]\\
  &=&\mathrm{E} [\sup\limits_{t\in[0,T]}|\hat{X}_{\alpha,\beta,H_1,H_2}(t) - \alpha J_{H_1}(t) + \alpha J_{H_1}(t) - U_{H_1}(t)|]\\
  &\le&\mathrm{E} [\sup\limits_{t\in[0,T]}\underbrace{|\hat{X}_{\alpha,\beta,H_1,H_2}(t) - \alpha J_{H_1}(t)|}_{\overset{\alpha\uparrow 1}{\rightarrow}0 }] + \mathrm{E} [\sup\limits_{t\in[0,T]}|\underbrace{\alpha J_{H_1}(t)}_{\overset{\alpha\uparrow 1}{\rightarrow} J_{H_1}(t)} - U_{H_1}(t)|]\\ 
  &\overset{\alpha\uparrow 1}{\rightarrow}&  \mathrm{E} [\sup\limits_{t\in[0,T]}|J_{H_1}(t) - U_{H_1}(t)|]\\
	&\overset{a\downarrow 0}{\rightarrow}& 0
\end{eqnarray*}
(ii):
\begin{eqnarray*}
  && \mathrm{E}[|\hat{X}_{\alpha, \beta, H_1, H_2}(t+\tau) - \hat{X}_{\alpha, \beta, H_1, H_2}(t)|^2]\\
  &=& 2\mathrm{Var}[\hat{X}_{\alpha, \beta, H_1, H_2}(t)] - 2\mathrm{Cov}[\hat{X}_{\alpha, \beta, H_1, H_2}(t), M_{\alpha, \beta, H_1, H_2}(t+\tau)]\\
	&=& 2(\alpha^2\mathrm{Var}[J_{H_1}(t)] + \beta^2\mathrm{Var}[J_{H_2}(t)]  \\
	&-& \alpha^2\mathrm{Cov}[J_{H_1}(t), J_{H_1}(t+\tau)] - \beta^2\mathrm{Cov}[J_{H_2}(t), J_{H_2}(t+\tau)])\\
    &=&  2(\alpha^2\mathrm{Var}[J_{H_1}(t)] - \alpha^2\mathrm{Cov}[J_{H_1}(t), J_{H_1}(t+\tau)] \\
	&+& \beta^2\mathrm{Var}[J_{H_2}(t)] - \beta^2\mathrm{Cov}[J_{H_2}(t), J_{H_2}(t+\tau)])\\
	&\rightarrow& 2 (\alpha^2(\frac{1}{2}\gamma^2 \tau^{2H_1}) + \beta^2\varsigma_{\hat{J}_{H_2}}(\tau)) \\
	&\overset{\alpha\uparrow 1}{\rightarrow}& \gamma^2\tau^{2H_1}
\end{eqnarray*}
(iii): Suppose $\{Y_t\}_t, \{Z_t\}_t$ are two families of random variables with $Y_t \rightarrow Y, Z_t \rightarrow Z$ in distribution. $Y_t, Z_t$ are independent for each $t$. Then  $Y_t + Z_t \rightarrow Y+Z$, because, using continuity theorem of characteristic function
\begin{eqnarray*}
  \mathrm{E}[\exp{i\xi(Y_t+Z_t)}]&=& \mathrm{E}[\exp{i\xi Y_t}]\mathrm{E}[\exp{i\xi Z_t}]\\
  &\rightarrow& \mathrm{E}[\exp{i\xi Y}] \mathrm{E}[\exp{i\xi Z}]\\
  &=& \mathrm{E}[\exp{i\xi (Y+Z)}].
\end{eqnarray*}
Consider 
$$
\alpha\sum_{k=1}^{\floor{nt}} \frac{(t - \frac{k-1}{n})^\phi}{\Gamma(1+\phi)} \brkt{J_{H_1}^{(\phi)}(\frac{k}{n}) - J_{H_1}^{(\phi)}(\frac{k-1}{n})}$$
and 
$$\beta\sum_{k=1}^{\floor{nt}} \frac{(t - \frac{k-1}{n})^\psi}{\Gamma(1+\psi)} \brkt{J_{H_2}^{(\psi)}(\frac{k}{n}) - J_{H_2}^{(\psi)}(\frac{k-1}{n})}
$$
are independent. According Theorem \ref{sec:cont} and Theorem of Cramer Slutsky. 
$$
\alpha\sum_{k=1}^{\floor{nt}} \frac{(t - \frac{k-1}{n})^\phi}{\Gamma(1+\phi)} \brkt{J_{H_1}^{(\phi)}(\frac{k}{n}) - J_{H_1}^{(\phi)}(\frac{k-1}{n})} \rightarrow \alpha J_{H_1}(t)
$$ and
$$
\beta\sum_{k=1}^{\floor{nt}} \frac{(t - \frac{k-1}{n})^\psi}{\Gamma(1+\psi)} \brkt{J_{H_2}^{(\psi)}(\frac{k}{n}) - J_{H_2}^{(\psi)}(\frac{k-1}{n})} \rightarrow \beta J_{H_2}(t)
$$ in distribution.
Then, since mentioned above,
\begin{eqnarray*}
  \tilde{X}_{\alpha,\beta,H_1,H_2}(t) &\rightarrow& \alpha J_{H_1}(t) + \beta J_{H_2}(t)\\
  &=& \hat{X}_{\alpha,\beta,H_1,H_2}(t)
\end{eqnarray*}
	\end{proof}
	\subsection{Discussion}
   In order to model log-volatility, we take fOU process into account. In FSV, we choose $H > \frac{1}{2}$ and that will make sure the solution of log-volatility SDE has long memory. In contrast, although it could not exhibit the long memory by $H < \frac{1}{2}$, RFSV demonstrates a more reasonable smoothness of volatility. For instance, RFSV ensures  the slop of the plotting of $log(s(\tau, \hat{X}))$ against $log(\tau, \hat{X})$, which consists with the empirical result we observed, see (\ref{sec:smo}).\\
	The weighted-FSV model inherits long memory of FSV. With an adjustable factor, one can achieve a result of smoothness of volatility close to it by RFSV. As for RFSV, when $a$ goes to zero, the $\hat{X}$ acts locally as fBm at any compact time scale.\\
	Not only in FSV but also in RFSV, there is a discretization with the fractional derivative, which is an $AR(1)$ process. However, it is not the case by weighted-FSV, in which $\hat{X}$ is approached as the sum of two fractional derivatives.
%	Future works could be focused on  the question, whether a desirable discretization can be found which works with a mixed order of fractional derivatives.




\newpage



%---------- x. conclusion ---------%
\begin{thebibliography}{99}
	\bibitem{bauer} \textsc{Bauer,~H.} (2002). Wahrscheinlichkeitstheorie(5th. durchges. und verb. Aufl.). Berlin: W. de Gruyter.
	\bibitem{chenu} \textsc{Cheridito,~P., \& Nualart,~D.} (2005). Stochastic Integral of Divergence Type with Respect to Fractional Brownian Motion with Hurst Parameter $H \in(0, \frac{1}{2})$. Ann. Inst. H. Poincar\'e Probab. Statist. 41(6):1049-1081. 
	\bibitem{chridito} \textsc{Cheridito,~P.} (2003). Arbitrage in Fractional Brownian Motion Models. Finance Stoch. 7. no. 4, 533-553.
	\bibitem{chriel} \textsc{Cheridito,~P., \& Kawaguchi,~H., \& Maejima, ~M.} (2003) Fractional Ornstein- Uhlenbeck Processes. Electron. J. Probab., 8(3):1-14.
	\bibitem{comte} \textsc{Comte,~F} (1996). Simulation and Estimation of Long Memory Continuous Times Models. J. Series Anal. 17(1):19-36.
	\bibitem{core} \textsc{Comte,~F. \& Renault,~E.} (1996). Long Memory Continuous Time Models. J. Econometrics. 73:101-149.
	\bibitem{comren} \textsc{Comte,~F. \& Renault,~E.} (1998). Long Memory in Continuous-time Stochastic Volatility Models. Mathematical Finance, 8(4):291-323.
	  \bibitem{comteetla} \textsc{Comte,~F. \& Coutin,~L, \& Renault,~E} (2012). Affine Fractional Stochastic Volatility Models. Annals of Finance. 8(2-3):337-378.
	  \bibitem{fukasawa} \textsc{Fukasawa,~M.} (2011). Asymmptotic Analysis for Stochastic Volatility: Martingale expansion. Finance and Stochastics. 15(4):634-654.
	\bibitem{gradin} \textsc{Gradinaru,~M., Nourdin,~I., Russo,~F., Vallois,~P.} (2005). m-Order Integrals and Generalized Ito's Formula; The Case of a Fractional Brownian Motion with Any Hurst Index. Ann. Inst. H. Poincar\'e Probab. Statist. 41(4):781-806. 
	\bibitem{kolm} \textsc{Kolmogorov,~A.,~N.} (1940) Wienersche Spiralen und einige andere interessante Kurven im Hilbertschen Raum, C.R. (Doklady) Acad. Sci. URSS(N.S.), 26, pp.115-118.
	\bibitem{liptshir} \textsc{Liptser,~R.~S. \& Shiryayev,~A.~N} (1989). Theory of Martingales,Vol. 49 of Mathematics and its Applications (Soviet Series), Kluwer Academic Publishers Group, Dordrecht. Translated from the Russian by K. Dzjaparidze.
	\bibitem{loeve} \textsc{L\`oeve,~M.} (1960). Probability Theory (2nd ed.). N.J.: D. Van Nostrand Princeton.
	\bibitem{michael} \textsc{Schramm,~J.M.} (2008). Introduction to Real Analysis(dovered edition). New York: Dover Publications.
	\bibitem{samorodnitsky} \textsc{Samorodnitsky,~G., \& Taqqu, ~M.} (1994). Stable Non-Gaussian Random Processes: Stochastic Models with Infinite Variance. New York: Chapman \& Hall.
	\bibitem{severini}\textsc{Severini,~T.} (2005). Elements of Distribution Theory. New York, NY: Cambridge University Press.
	\bibitem{mandelbrot} \textsc{Mandelbrot, ~B.B., \& Van Ness, ~J.W.} (1968). Fractional Brownian Motions, Fractional Noises and Applications. SIAM Review 10(4):422–437.
	\bibitem{nourdin} \textsc{Nourdin,~I.} (2012). Selected Aspects of Fractional Brownian Motion. Milano: Springer Milan.
	\bibitem{shilling} (2012, October 1). Stochastic Processes. Lecture conducted from \textsc{Schilling, R.}, Dresden.
	\bibitem{Gatheral} \textsc{Gatheral,~J. \& Jaisson,~T. \& Rosenbaum,~M} (2014). Volatility is Rough.

\end{thebibliography}

\newpage
\thispagestyle{empty}
\Large
\textbf{ERKL\"ARUNG} \\[1em]
\large
Hiermit erkl\"are ich, dass ich die am heutigen Tag eingereichte Diplomarbeit zum Thema ,,Fractional Brownian motion and applications in financial mathematics`` unter Betreuung von Prof.\ Dr.\ rer.\ nat.\ M.\ Keller-Ressel selbstst\"andig erarbeitet, verfasst und Zitate kenntlich gemacht habe. Andere als die angegebenen Hilfsmittel wurde von mir nicht benutzt. \\[5em]
\vspace{2cm}
\large
Datum \hfill Unterschrift 
\end{document}

\end{document}
