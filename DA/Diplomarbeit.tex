%        File: Diplomarbeit.tex
%     Created: Fri Jan 02 04:00 PM 2015 C
% Last Change: Thu Feb 12 08:00 PM 2015 C
%      Author: Eduard Zhu
%       Email: Kewell1203@gmail.com

\documentclass[a4paper, twoside, 11pt]{article}

%------------------------------%
\synctex=1
%----------- preamble ---------%
%----------- packages ---------%
\usepackage[body={15cm, 23cm}, top=4.5cm, left=4cm]{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage[perpage, symbol]{footmisc}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{bbm}

%----------- pagestyle setting ----------%
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\renewcommand{\headrulewidth}{.4pt}
\renewcommand{\footrulewidth}{.4pt}
\fancyhead[RO]{\leftmark}
\fancyhead[LE]{\rightmark}
\fancyfoot[LE, RO]{\large \thepage}

%---------- new commands ---------%
\theoremstyle{definition}
\newtheorem{definition}{\scshape Definition}[section]
\newtheorem{theorem}[definition]{\scshape Theorem}
\newtheorem{lemma}[definition]{\scshape Lemma}
\newtheorem{proposition}[definition]{\scshape Proposition}
\newtheorem{corollary}[definition]{\scshape Corollary}
\newtheorem{example}[definition]{\scshape Example}
\renewcommand{\proofname}{\upshape\bfseries Proof.}
\renewcommand{\theequation}{\thesection.\arabic{equation}}


%---------- definitions of math -----------%
% R, N
\def\RR{$\mathcal{R}$}
\def\NN{$\mathcal{N}$}
\def\AA{$\mathscr{A}$\ }
% complement
\newcommand{\compl}[1]{{#1}^{c}}
% sigma algebra
\def\sa{$\sigma$- Algebra\ } 
% prob. space
\def\bs{$(\Omega, \mathscr{A}, \mathcal{P})$\ } 
\def\bsigma{\mathscr{B}\brkt{\mathbb{R}^{n}}}
\newcommand{\sqbr}[1]{\left[ {#1} \right]}
\newcommand{\brkt}[1]{\left({#1} \right)}

  %---------- global variables setting -----%
  \setlength{\parindent}{0em}
  \setlength{\parskip}{1.5ex plus .5ex minus .5ex}
  \renewcommand{\baselinestretch}{1.3}
  \setfnsymbol{wiley}
  \renewenvironment{abstract}{
	\begin{center}
		  \Large
		  \textbf{Abstract}
		  \hspace{2em}
	\end{center}				
  }{}

  %---------- beginning of document --------%
  \begin{document}

  %---------- title page -----------%
  \input{./title.tex}
  \newpage

  %---------- abstract -------------%
  \thispagestyle{empty}
  \begin{abstract}
	blahblah
  \end{abstract}
\newpage

%---------- contents -------------%
\thispagestyle{empty}
\mbox{}
\newpage
\fancyhead[LO, RE]{}
\fancyfoot[LE, RO]{}
\tableofcontents
\newpage
\thispagestyle{empty}
\mbox{}
\newpage

%---------- 1. introduction -------%
\fancyhead[RO]{\leftmark}
\fancyhead[LE]{\rightmark}
\fancyfoot[LE, RO]{\large \thepage}
\setcounter{section}{0}
\setcounter{page}{1}
\section{Introduction}

\newpage

%---------- 2. section ------------%
\section{Gaussian Process and Brownian Motion}
In this section we start off by looking at some general concepts of probability spaces and stochastic processes. Of this, a most important case we then discribe is Gaussian process. Within the framework of Gaussian processes, one could specific a stationary and independent behaviour of increments of it. This lead us to introduce the Brownian motion as a fine example.

%---------- 2.1. subsection -------%
\subsection{Probability Space and Stochastic Process }
\begin{definition}
  Let \AA be a collection of subsets of a set $\Omega$. \AA is said to be a \emph{$\sigma$- Algebra} on $\Omega$, if it satisfies the following conditions:
  \begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
	\item $\Omega \in $ \AA.
	\item For any set $F \in \mathscr{A}$, its complement $\compl{F} \in$ \AA.
	\item If a serie $\{F_n\}_{n \in \mathbb{N}} \subseteq \mathscr{A}$, then $\cup_{n \in \mathbb{N}}F_n \in $ \AA.
  \end{enumerate}
\end{definition}

\begin{definition}
  A mapping $\mathcal{P}$ is said to be a \emph{probability measure} from $\mathscr{A}$ to $\bsigma$, if $\mathcal{P}\sqbr{\sum_{n=1}^{\infty} F_n} = \sum_{n=1}^{\infty} \mathcal{P}\sqbr{F_n}$ for any $\{F_n\}_{n \in \mathbb{N}}$ disjoint in $\mathscr{A}$ satisfying $\sum_{n=1}^{\infty}F_n \in \mathscr{A}$. 
\end{definition}

\begin{definition}
  A \emph{probability space} is defined as a triple \bs of a set $\Omega$, a \sa \AA  of $\Omega$ and a measure $\mathcal{P}$ from $\mathscr{A}$ to $\bsigma$.
\end{definition}

The $\sigma$- Algebra generated of all open sets on $\mathbb{R}^{n}$ is called the \emph{Borel $\sigma$- Algebra} which we denote as usual by $\mathscr{B}\left(\mathbb{R}^{n}\right)$. Let $\mu$ be a probability measure on $\mathbb{R}^{n}$. Indeed, $\brkt{\mathbb{R}^{n}, \mathscr{B}\brkt{\mathbb{R}^{n}}, \mu}$ is a special case that probability space on $\mathbb{R}^{n}$. A function $f$ mapping from $\brkt{\mathcal{D}, \mathscr{D}, \mu}$ into $\brkt{\mathcal{E}, \mathscr{E}, \nu}$ is \emph{measurable}, if its collection of the inverse image of $\mathscr{E}$ is a subset of $\mathscr{D}$. A \emph{random variable} is a $\mathbb{R}^{n}$-valued measurable function on some probability space. Let $\mathcal{P}$ represent a probability measure, recall that in probability theory, for $B \in \bsigma$ we call $\mathcal{P}\sqbr{\left\{X \in B\right\}}$ the \emph{distribution} of $X$. We write also $\mathcal{P}_X \sqbr{\cdot}$ or $\mathcal{P}\sqbr{X}$ for convenience for those notations .

\begin{definition}
  Let $\brkt{\Omega, \mathscr{A}, \mathcal{P}}$ be a probability space. A $n$-dimensional \emph{stochastic process} $\brkt{X_t}_{t\in T}$ is a family of random variable such that $X_t\brkt{\omega} : \Omega \longrightarrow  \mathbb{R}^{n},  \forall t \in T$, where $T$ denotes the set of Index of Time.    
\end{definition}

Some basically definitions, which are needed in following sections, are given.
\begin{definition}
  A stochastic process $\brkt{X_t}_{t \in T}$ is said to be \emph{stationary}, if the joint distribution 
\[
  \mathcal{P}\sqbr{X_{t_1},\dots,X_{t_n}} = \mathcal{P}\sqbr{X_{t_1+\tau},\dots,X_{t_n+\tau}} 
\]
for $t_1, \dots, t_n$ and $t_1+\tau,\dots,t_n+\tau \in T$. 
\label{sec:stn}
\end{definition}

Remark that, Definition \ref{sec:stn} means the distribution of a stationary process is independent of a shift of time.

\begin{definition}
  Let $(X_t)_t$ be a stochastic process. 
  \begin{equation*}
	\varsigma_X(t,s) := \mathrm{Cov}(X_t, X_s) 
  \end{equation*} is called \emph{autocovariance} between $s, t$ and 
  \begin{equation*}
	\eta_X(t, s) := \frac{\mathrm{Cov[X_t, X_s]}}{\sqrt{\mathrm{Var}[X_t]\mathrm{Var}[X_s]}}
  \end{equation*}
  is called \emph{autocorrelation} betwenn $s, t$.
\end{definition}
If $(X_t)_t$ is stationary process, we write $\varsigma_X(\tau)$ for $\varsigma_X(t, t+\tau)$ for any $t$. $\eta_X(\tau)$ is used in the same way. 

%\begin{definition}
%  A stochastic process $(X_t)_t$ is said to be \emph{ergodic} if the moving average of $X_t$ over $T$ tend to infinity, in fact,
%  \begin{equation}
%	\frac{1}{T}\int_0^T\,X_t\,\mathop{dt} \longrightarrow \infty\nonumber\\.
%  \end{equation}
%	\label{sec:erg}
%\end{definition}

We use a notation $X \sim Y$ represents $X$ equals $Y$ \emph{in distribution}. 
\begin{definition}
  A stochastic process $(X_t)_{t\in T}$ is said to be \emph{$\alpha$-self similar} if $(X_{ct_1},\dots,X_{ct_k}) \sim (c^\alpha X_{t_1},\dots, c^\alpha X_{t_k})$ for any $t_1,\dots, t_k, ct_1, \dots, ct_k \in T$ and $c>0$.
\end{definition}

%---------- 2.2. subsection -------%
\subsection{Normal Distribution and Gaussian Process}
\begin{definition}[1-dimensional normal distribution]
  A $\mathbb{R}$-valued random variable $X$ is said to be \emph{standard normal distributed} or \emph{standard Gaussian}, if its distribution can be discribed as
  \begin{equation}
	\mathcal{P}\sqbr{X \le x} = (2\pi)^{-\frac{1}{2}}\int_{-\infty}^{x} e^{-\frac{u^2}{2}}\,\mathop{du}  
	\label{sec:21}
  \end{equation}
  for $x \in \mathbb{R}$.
\end{definition}
The integrand of (\ref{sec:21}) is also called \emph{density function} of a Gaussian random variable.

\begin{definition}
  A $\mathbb{R}$-valued random variable $X$ is said to be \emph{normal distributed} or \emph{Gaussian} with a \emph{expected value} $\mu$ and a \emph{variance} $\sigma^2$, if
\[
  (X-\mu) / \sigma
\]
is standard Gaussian.
\end{definition}

\begin{proposition}
  Let $X$ be a $\mathbb{R}-valued$ Gaussian random variable with expected value $\mu$ and variance $\sigma^2$, then it is distributed as
  \begin{equation*}
	\mathcal{P}[X\le x] = (2\pi\sigma^2)^{-\frac{1}{2}}\int_{-\infty}^x e^{-\frac{(u-\mu)^2}{2\sigma^2}}\mathop{du}
  \end{equation*}
\end{proposition}

\begin{proof}
  Suppose $X = \sigma Y + \mu$ with $Y$ standard Gaussian. We denote this mapping by $g(y) : y \rightarrow \sigma y + \mu$ and give the inverse $g^{-1}(x) : x \rightarrow \frac{(x-\mu)}{\sigma}$. The distribution function of $X$ is 
  \begin{eqnarray*}
	\int_\Omega \mathcal{P}[X \in dx] &=& \int_\Omega \mathcal{P}[Y \circ g \in dy] \\
	&=& \int_{\mathbb{R}\circ g} f_X \circ g^{-1}(y) \mathop{dy}\\
	&=& \int_{\mathbb{R}} \sigma \frac{1}{\sqrt{2\pi}} \exp\{\frac{(\frac{(y-\mu)}{\sigma})^2}{2}\} \mathop{dy}\\
	&=&  (2\pi\sigma^2)^{-\frac{1}{2}}\int_{\mathbb{R}} \exp\{-\frac{(y-\mu)^2}{2\sigma^2}\}\mathop{dy} ,
  \end{eqnarray*}
 where $f_X$ is density function of $X$.
\end{proof}

It is denoted by $X \sim (2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}\mathop{dx} $, if $X$ is standard Gaussian. In order to verifying the behaviour of a normal distributed random variable we use the characteristic function in probability theory, Cf.\cite{bauer}. 

\begin{theorem}
  Let $X$ be a $\mathbb{R}$-valued Gaussian random variable. The characteristic function of $X$
\begin{equation}
  \Psi_X(\xi) := \int_\mathbb{R} e^{ix\xi}\mathcal{P}\sqbr{X \in \mathop{dx}} = e^{i\mu\xi-\frac{1}{2}(\sigma\xi)^2}
  \label{sec:cht}
\end{equation}
for $\xi \in \mathbb{R}$.
\label{sec:char}
\end{theorem}
\begin{proof}
  Cf.\cite{shilling}. We assume firstly $X$ is standard Gaussian. In terms of the Definion of characteristic function of a standard Gaussian $X$, integrating its density function over $\mathbb{R}$ we get
  \begin{equation*}
	\Psi_X(\xi) = \int_\mathbb{R} (2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}e^{ix\xi}\,\mathop{dx},
  \end{equation*}
take differentiating both sides of the equation by $\xi$, then
\begin{eqnarray*}
\Psi_X'(\xi) &=& \int_\mathbb{R}(2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}e^{ix\xi}ix\,\mathop{dx}\\
             &=& (-i)\cdot\int_\mathbb{R} (2\pi)^{-\frac{1}{2}}(\frac{d}{dx}e^{-\frac{x^2}{2}})e^{ix\xi}\,\mathop{dx}\\
			 &\overset{part.int.}{=}& -\int_\mathbb{R}(2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}e^{ix\xi}\xi\,\mathop{dx}\\
			 &=& -\xi\Psi_X(\xi).
\end{eqnarray*}
Obviously, 
$\Psi(\xi) = \Psi(0)e^{-\frac{\xi^2}{2}}$ is the solution of the partial differential equation, and $\Psi(0)$ equals $1$, hence $\Psi(\xi) = e^{-\frac{\xi^2}{2}}$.
In particular, the characteristic function of a Gaussian random variable with a expected value $\mu$ and a variance $\sigma^2$, which denoted by $\Psi_{X_{\mu,\sigma^2}}(\xi)$, is $e^{i\mu\xi-\frac{1}{2}(\sigma\xi)^2}$. To achieve this result, we just need to substitute $x$ by $(x-\mu)/\sigma$ in the previous calculation. 
\end{proof}

\begin{definition}
  Let $X$ be a $\mathbb{R}^{n}$-valued random vector. $X$ is said to be \emph{normal distributed} or \emph{Gaussian}, if for any $d \in \mathbb{R}^{n}$ such that $d^TX$ is Gaussian in $\mathbb{R}$.
  \label{sec:g1}
\end{definition}
%Note that, $<d,X>$ is defined as scalar product on $\mathbb{R}^{n}$ that means $\sum_{j=1}^{n}\,d_j\cdot X_i$. 

\begin{definition}
  A stochastic process $(X_t)_{t\in T}$ is said to be \emph{Gaussian process} if the joint distribution of any finite instance is Gaussian, that means
$
(X_{t_1},\dots, X_{t_n})
$ has joint Gaussian distribution in $\mathbb{R}^n$ for $t_1,\dots,t_n \in T$.
\label{sec:defgau}
\end{definition}
The definition immediately shows every instance $X_t$ in Gaussian process is Gaussian.

\begin{corollary}
  Let $(X_t)_{t\in T}$ be a stochastic process. The following condition is equivalent to Definition \ref{sec:defgau}.
  \begin{equation}
	\sum_j^n c_{t_j} X_{t_j}
  \end{equation}
  is Gaussian for any $t_1,\dots,t_n \in T$.
  \label{sec:gauss}
\end{corollary}
\begin{proof}
  It is clear due to Definition \ref{sec:g1}.
\end{proof}

\begin{lemma}
  Let $X$ be a $\mathbb{R}^{n}$-valued normal distributed ramdon vector. Then its characteristic function is 
  \begin{equation}
	\mathrm{E}\,e^{i\xi^TX} = e^{i\xi^Tm - \frac{1}{2}\xi^T \Sigma \xi}.
	\label{sec:mcf}
  \end{equation}
 For $\xi \in \mathbb{R}^{n}$. Where $m \in \mathbb{R}^{n}, \Sigma \in \mathbb{R}^{n\times n}$ are \emph{mean vector} , \emph{covariance matrix} of $X$ respectively. Furthermore, the density function of $X$ is
\begin{equation}
  (2\pi)^{-\frac{n}{2}}\, (\det\Sigma) ^{-\frac{1}{2}}\,e^{-\frac{1}{2}(x-m)^T\Sigma^{-1}(x-m)}.
  \label{sec:dsy}
\end{equation}
\end{lemma}

Remark, the equation (\ref{sec:mcf}) can also be as definition of characteristic function of a n-dimensional normal distributed random variable. I.e., any normal distributed random variable can be characterized by form of the equation (\ref{sec:mcf}).

\begin{proof}
  Since $X$ normal distributed on $\mathbb{R}^{n}$, then $\xi^T X$ is normal distributed on $\mathbb{R}$. Due to the Theorem \ref{sec:char}, there is
  \begin{eqnarray*}
	\mathrm{E} e^{i\xi^T X} &=& \mathrm{E} e^{i\cdot 1 \cdot \xi^T X}\\
	                        &=& e^{i\mathrm{E}\sqbr{\xi^T X} -\frac{1}{2}\mathrm{Var}\sqbr{\xi^T X}}\\
							&=& e^{i\xi^T\mathrm{E}\sqbr{X} - \frac{1}{2}\xi^T \mathrm{Var}\sqbr{X} \xi}\\
						    &=& e^{i\xi^Tm - \frac{1}{2}\xi^T \Sigma \xi}.
  \end{eqnarray*}
  Moreover, since $\Sigma$ symmetirc and positive definit, there exist $\Sigma^{-1}, \Sigma^{\frac{1}{2}}$ and $\Sigma^{-\frac{1}{2}}$.
  \begin{eqnarray*}
	&&(2\pi)^{-\frac{n}{2}} (\det\Sigma) ^{-\frac{1}{2}}\int_{\mathbb{R}^{n}} e^{i x^T \xi}e^{-\frac{1}{2}(x-m)^T\Sigma^{-1}(x-m)}\, \mathop{dx}\\
	&=& (2\pi)^{-\frac{n}{2}} (\det\Sigma) ^{-\frac{1}{2}}\int_{\mathbb{R}^{n}} e^{ix^T\xi} e^{i (x-m)^T \xi}e^{-\frac{1}{2}(x-m)^T\Sigma^{-1}(x-m)}\, \mathop{dx} \\
	&=& (2\pi)^{-\frac{n}{2}} (\det\Sigma) ^{-\frac{1}{2}}  e^{im^T\xi} \int_{\mathbb{R}^{n}} e^{i(x-m)^T\xi} e^{i (x-m)^T \xi}e^{-\frac{1}{2}(x-m)^T\Sigma^{-1}(x-m)}\, \mathop{dx} \\
	&\overset{y=\Sigma^{-\frac{1}{2}}x}{=}& (2\pi)^{-\frac{n}{2}} e^{im^T\xi} \int_{\mathbb{R}^{n}}e^{i (\Sigma^{\frac{1}{2}}y) \xi}\,e^{-\frac{1}{2}|y|^2}\, \mathop{dy}\\
	&=& (2\pi)^{-\frac{n}{2}} e^{im^T\xi}  \int_{\mathbb{R}^{n}}e^{i y^T (\Sigma^{\frac{1}{2}}\xi)}\,e^{-\frac{1}{2}|y|^2}\, \mathop{dy}\\
    %&=& (2\pi)^{-\frac{n}{2}} e^{i\xi m} \int_{\mathbb{R}^{n}}e^{i y- (\Sigma^{\frac{1}{2}}\xi)}\,e^{-\frac{1}{2}|y|^2}\, \mathop{dy}\\
	&\overset{\text{Fourier transformation}}{=}& e^{im^T\xi}  e^{-\frac{1}{2}|\Sigma^{\frac{1}{2}}\xi|^2}\\
	&=& e^{im^T\xi}  e^{-\frac{1}{2}\xi^T\Sigma\xi}
  \end{eqnarray*}
  In terms of the uniqueness theorem of characteristic function (in \cite{bauer}, p.199, Satz 23.4), then we can deduce (\ref{sec:dsy}) is density function of $X$.
\end{proof}

%A normal distributed normal random variable can be characterized by its expected value and variance respectively mean vector and covariance vector because of the characteristic function.
%\begin{theorem}
%  Let $X$ be a $\mathbb{R}^{n}$-valued normal distributed random vector with independent, normal distributed components. Then $X$ has a joint normal distribution.
%\end{theorem}

%\begin{proof}
  
%\end{proof}

%\begin{corollary}
%  Let $X$ be a $\mathbb{R}^{n}$-valued normal distributed random vector with normal distributed components. If the covariance matrix of $X$ is positive definit and symmtric, then $X$ has a joint normal distribution.
%  \label{sec:mulnor}
%\end{corollary}

\begin{theorem}
  A linear combination of independent normal distributed random variable (or vector) is Gaussian.
\end{theorem}

\begin{proof}
  We suppose $X_1, \cdots, X_m$ are independent random vectors  on $\mathbb{R}^n$ and $c_1, \cdots, c_m \in \mathbb{R}$. Let have a look at the chracteristic function of it,
  \begin{eqnarray*}
	\mathrm{E}e^{i\xi^T\sum_{j=1}^m(c_jX_j)} &\overset{independent}{=}&\prod_{j=1}^{m} \mathrm{E}e^{i\xi^T(c_jX_j)}\\
	&=& \prod_{j=1}^m \exp\brkt{i\xi^T\mathrm{E}[c_jX_j]-\frac{1}{2}\xi^T\mathrm{Var}[c_jX_j]\xi}\\
	&=&  \exp\brkt{i\xi^T\mathrm{E}[\sum_{j=1}^{m}c_jX_j]-\frac{1}{2}\xi^T\mathrm\sum_{j=1}^{m}{Var}[c_jX_j]\xi}\\
	&\overset{independent}{=}&  \exp\brkt{i\xi^T\mathrm{E}[\sum_{j=1}^{m}c_jX_j]-\frac{1}{2}\xi^T\mathrm{Var}[\sum_{j=1}^{m}c_jX_j]\xi},
  \end{eqnarray*}
  which is a form of characteristc function of normal distribution. That means $\sum_{j=1}^m c_jX_j$ is Gaussian. 
\end{proof}

\begin{example}[Bivariate Normal Distribution]
  Cf.\cite{severini}, p.241, Example 8.6. Suppose $S_1, S_2$ are independent random variables and have standard normal distributions. $\left(
    \begin{array}{c}
      S_1 \\
      S_2
    \end{array}
  \right)$  has standard normal joint distribution since they are independent. We define
  \begin{eqnarray}
	\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)
	&=& 
	\left(
    \begin{array}{l}
	  \sigma_1,\hspace{3em} 0 \\
	  \sigma_2 \rho, \sigma_2(1-\rho^2)^{\frac{1}{2}}
    \end{array}
  \right) \cdot
	\left(
    \begin{array}{c}
      S_1 \\
      S_2
    \end{array}
  \right)  +
  \left(
    \begin{array}{c}
      \mu_1 \\
      \mu_2
    \end{array}
  \right)
  \label{sec:bi},
  \end{eqnarray}
  where $\mu_1, \mu_2, \sigma_1, \sigma_2 \in \mathbb{R}, -1 \le \rho \le 1$. Again, $Y_1, Y_2$ are Gaussian and the joint distribution  $\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)$ is also Gaussian. We set $\mathrm{E}[Y_1] = \mu_1, \mathrm{E}[Y_2] = \mu_2 $ for short. Since $S_1, S_2$ are independent,
	\begin{eqnarray*}
	  \mathrm{Var}[Y_1] &=& \mathrm{Var}[\sigma_1 S_1]\\
						 &=& \sigma_1^2 ,\\
						 \mathrm{Var}[Y_2] &=& \mathrm{Var}[\sigma_2\rho S_1] + \mathrm{Var}[\sigma_2 (1-\rho^2)^{\frac{1}{2}} S_2]\\
						 &=& \sigma_2^2 \rho^2 + \sigma_2^2(1 - \rho^2)\\
						 &=& \sigma_2^2,\\
						 \mathrm{Cov}[Y_1, Y_2] &=& \mathrm{E}[(Y_1 - \mathrm{E}[Y_1])(Y_2 - \mathrm{E}[Y_2])]\\
						 &=& \mathrm{E}[Y_1Y_2 - \mu_1Y_2 - \mu_2Y_1 + \mu_1\mu_2]\\
						 &=& \mathrm{E}[(\sigma_1 S_1 + \mu_1)(\sigma_2\rho S_1 + \sigma_2(1-\rho^2)^{\frac{1}{2}}S_2 + \mu_2)] - \mu_1\mu_2\\
						 &=& \sigma_1\sigma_2\underbrace{\mathrm{E}[S_1^2]}_{=1}\rho + \mu_1\sigma_2\rho\underbrace{\mathrm{E}[S_1]}_{=0} +
						 \sigma_1\sigma_2(1-\rho^2)^{\frac{1}{2}}\underbrace{\mathrm{E}[S_1S_2]}_{=\mathrm{E}[S_1]\mathrm{E}[S_2]=0} \\
						 &+& \mu_1\sigma_2(1-\rho^2)^{\frac{1}{2}}\underbrace{\mathrm{E}[S_2]}_{=0} + \sigma_1\underbrace{\mathrm{E}[S_1]}_{=0}\mu_2 + \mu_1\mu_2 - \mu_1\mu_2\\
						 &=& \rho\sigma_1\sigma_2,
	\end{eqnarray*}
	that means the corrlation of $Y_1, Y_2$ is $\rho$.
	Because of the equation (\ref{sec:dsy}), the joint density function
	\begin{eqnarray*}
	  f_{Y_1, Y_2}(y_1, y_2) &=& (2\pi)^{-1} (\det(\Sigma))^{-\frac{1}{2}} \exp\brkt{(y_1 - \mu_1) \Sigma^{-1} (y_2 - \mu_2)},
	\end{eqnarray*}
	where $\Sigma =\left(
    \begin{array}{l}
	  \sigma_1^2, \hspace{3em}0 \\
	  \sigma_2^2\rho^2, \sigma_2^2(1-\rho^2)
    \end{array}
  \right)
 $\\
 Indeed, 
 $$
 	\det(\Sigma) = (1-\rho^2)\sigma_1^2\sigma_2^2
 $$ and 
 $$
 \Sigma^{-1} = \frac{
   \left(
    \begin{array}{l}
	  \sigma_2^2(1-\rho^2), \hspace{1em}0 \\
	  -\sigma_2^2\rho,\hspace{3em}\sigma_1^2
    \end{array}
  \right)}
  {\displaystyle (1-\rho^2)\sigma_1^2\sigma_2^2}.
  $$
 Namely,
 \begin{equation}
   f_{Y_1, Y_2}(y_1, y_2) = \frac{1}{2\pi(1 - \rho^2)^{\frac{1}{2}}\sigma_1\sigma_2}\exp\brkt{-\frac{1}{2(1-\rho^2)}(z_1^2 - 2\rho z_1 z_2 + z_2^2)}
   \label{sec:jdt}
 \end{equation}
 where $z_1 = \frac{y_1-\mu_1}{\sigma_1}, z_2=\frac{y_2-\mu_2}{\sigma_2}$.
\end{example}

\begin{corollary}
  Let $Y_1, Y_2$ be $\mathbb{R}$-valued random variables and $\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)$  has a joint normal distribution, then the conditional expected value of $Y_2$ given $Y_1$
    $$
	\mathrm{E}[Y_2| Y_1=y_1] = \mathrm{E}[Y_2] + \rho (y_1 - \mathrm{E}[Y_1])\frac{\sigma_2}{\sigma1},
	$$
	and the conditional variance of $Y_2$ given $Y_2$
	$$
		\mathrm{Var}[Y_2| Y_1 = y_1] = \sigma_1^2 (1 - \rho^2).
	$$
	Where $\sigma_1, \sigma_2$ are standard deviations of $Y_1, Y_2$ and $\rho$ is the correlation of $Y_1, Y_2$.
	\label{sec:condi}
\end{corollary}

\begin{proof}
  Recall the equation (\ref{sec:jdt}), we can specify the joint density function if $\sigma_1, \sigma_2, \rho$ are known. As result of this,
  $\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)$ has a form of the equation (\ref{sec:bi}).
  Suppose $S_1, S_2$ are independent standard normal distributed random variables. Now we have
  \begin{eqnarray*}
	S_1 &\sim& \frac{(Y_1 - \mathrm{E}[Y_1])}{\sigma_1} \\
	Y_2 &\sim& \sigma_2\rho S_1 + \sigma_2(1-\rho^2)^{\frac{1}{2}} S_2 + \mathrm{E}[Y_2],
  \end{eqnarray*}
  more precisely,
  $$
  Y_2 \sim \sigma_2\rho \frac{(Y_1 - \mathrm{E}[Y_1])}{\sigma_1}  + \sigma_2(1-\rho^2)^{\frac{1}{2}} S_2 + \mathrm{E}[Y_2].
  $$
  Take expectation of both sides, 
  \begin{equation*}
	\mathrm{E}[Y_2|Y_1=y_1] = \sigma_2\rho \frac{(y_1 - \mathrm{E}[Y_1])}{\sigma_1} + \mathrm{E}[Y_2].
  \end{equation*}
  Now consider
  \begin{eqnarray*}
	\mathrm{Var}[Y_2|Y_1=y_1] &=&  \mathrm{E}[(Y_2 - \mu_{Y_2|Y_1})^2|Y_1=y_1]\\
							  &=& \int_{-\infty}^{\infty}(y_2 - \mu_{Y_2|Y_1})^2f_{Y_2|Y_1}(y_2, y_1)\,\mathop{dy_2}\\
							  &=& \int_{-\infty}^{\infty}\sqbr{y_2 - \mu_2 - \frac{\rho\sigma_2}{\sigma_1}(y_1-\mu_1)}^2f_{Y_2|Y_1}(y_2, y_1)\,\mathop{dy_2},
  \end{eqnarray*}
 After multiplying both sides by the density function of $Y_1$ and integrating it by $y_1$, we have
\begin{eqnarray*}
 &\,&\int_{-\infty}^{\infty} \mathrm{Var}[Y_2|Y_1=y_1] f_{Y_1}(y_1) \mathop{dy_1} \\
 &=&\int_{-\infty}^{\infty}\,\int_{-\infty}^{\infty} \sqbr{y_2 - \mu_2 
	- \frac{\rho\sigma_2}{\sigma_1}(y_1-\mu_1)}^2\underbrace{f_{Y_2|Y_1}(y_2, y_1)\,f_{Y_1}(y_1)}_{f_{Y_1, Y_2}(y_1, y_2)} \mathop{dy_2}\,\mathop{dy_1}\\
	&\iff&\\
	&\,&\mathrm{Var}[Y_2|Y_1=y_1] \underbrace{\int_{-\infty}^{\infty}  f_{Y_1}(y_1)}_{1} \mathop{dy_1} \\
	&=& \mathrm{E}\sqbr{(Y_2 - \mu_2) - (\frac{\rho\sigma_2}{\sigma_1})(Y_1 - \mu_1)}^2 
\end{eqnarray*}

multiplying right side out, we see
\begin{eqnarray*}
  \mathrm{Var}[Y_2|Y_1=y_1] &=&\underbrace{\mathrm{E}[(Y_2 - \mu_2)^2]}_{\sigma_2^2} - 2\frac{\rho\sigma_2}{\sigma_1}\underbrace{\mathrm{E}[(Y_1 -\mu_1)(Y_2 - \mu_2)]}_{\rho\sigma_1\sigma_2}\\
  &+& \frac{\rho^2\sigma_2^2}{\sigma_1^2}\underbrace{\mathrm{E}[(Y_1-\mu_1)^2]}_{\sigma_1^2}\\
  &=& \sigma_2^2 - 2\rho^2\sigma^2 + \rho^2\sigma_2^2\\
  &=& \sigma_2^2 - \rho^2\sigma_2^2.
\end{eqnarray*} 
\end{proof}

\begin{theorem}
  Let $X$ be a Gaussian random variable, then
  \begin{equation}
	\mathrm{E}[\exp(\beta X)] = \exp(\beta\mu + \frac{1}{2}\beta^2\sigma^2).
	\label{sec:expgau}
  \end{equation}
  Where $\mu$ and $\sigma$ are $\mathrm{E}[X]$ and $\mathrm{Var}[X]$ respectively.
\end{theorem}
\begin{proof}
\begin{eqnarray*}
  &&\mathrm{E}[\exp(\beta X)] \\
&=& (2\pi \sigma^2)^{-\frac{1}{2}}\int_\mathbb{R} \exp(\beta x) \exp\brkt{-\frac{(x-\mu)^2}{2\sigma^2}} \mathop{dx}\\
&=& (2\pi \sigma^2)^{-\frac{1}{2}}\int_\mathbb{R} \exp(\beta x) \exp\brkt{-\frac{(x^2- 2x\mu+ \mu^2)}{2\sigma^2}} \mathop{dx}\\
&=& (2\pi \sigma^2)^{-\frac{1}{2}}\int_\mathbb{R} \exp\brkt{-\frac{x^2- 2(\beta\sigma^2+\mu)x+ \mu^2}{2\sigma^2}} \mathop{dx}\\
&=& (2\pi \sigma^2)^{-\frac{1}{2}}\int_\mathbb{R} \exp\brkt{-\frac{x^2- 2(\beta\sigma^2+\mu)x+ (\beta\sigma^2+\mu)^2 -  (\beta\sigma^2+\mu)^2 + \mu^2}{2\sigma^2}} \mathop{dx}\\
&=& (2\pi \sigma^2)^{-\frac{1}{2}}\int_\mathbb{R} \exp\brkt{-\frac{(x- (\beta\sigma^2+\mu))^2+ \mu^2 - (\beta\sigma^2+\mu)^2}{2\sigma^2}} \mathop{dx}\\
&=& \exp\brkt{\frac{(\beta\sigma^2+\mu)^2 - \mu^2}{2\sigma^2}} \underbrace{(2\pi \sigma^2)^{-\frac{1}{2}}\int_\mathbb{R} \exp\brkt{-\frac{(x- (\beta\sigma^2+\mu))^2}{2\sigma^2}}\mathop{dx}}_{1}\\
&=& \exp\brkt{\frac{\beta^2\sigma^4 + 2\mu\beta\sigma^2}{2\sigma^2}}\\
&=& \exp(\mu\beta + \frac{1}{2}\beta^2\sigma^2)
\end{eqnarray*}
\end{proof}
\subsection{Brownian Motion}
The Brownian motion was first introduced by Bachelier in 1900 in his PhD thesis. We now give the common definition of it.
\begin{definition}
Let $(B_t)_{t\ge0}$ be a $\mathbb{R}^{n}$-valued stochastic process. $(B_t)$ is called \emph{Brownian motion} if it satisfies the following conditions:
\begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
  \item $B_0 = 0 $ a.s. .
  \item $(B_{t_1} - B_{t_0}),\dots,(B_{t_n} - B_{t_{n_1}})$ are independet for $0=t_0<t_1<\dots<t_n$ and $n \in \mathbb{N}$.
  \item $B_t - B_s \sim B_{t-s}$, for $0 \le s \le t < \infty$.
  \item $B_t - B_s \sim \mathcal{N}(0, t-s)^{\otimes n}$.
  \item $B_t$ is continuous in $t$ a.s. .
\end{enumerate}
\end{definition}
A usual saying for $(ii)$ and $(iii)$ is the Brownian motion has independent, stationary increments. In (iv), $\mathrm{N}$ represent a random variable which has a normal distribution. $B_t$ is normal distributed due to (ii). It is clear that the increments of Brownian motion is stationary.

\begin{proposition}
  Let $(B_t)$ be $\mathbb{R}$-valued Brownian motion. Then the covariance of $B_m, B_n$ for $m, n \ge 0$ is $m \wedge n $.
\end{proposition}
\begin{proof}
  Without loss of generality, we assume that $m \ge n$, then
  \begin{eqnarray*}
	\mathrm{E}[B_mB_n] &=& \mathrm{E}[(B_m - B_n)B_n] + \mathrm{E}[B_n^2]\\
	&=& \mathrm{E}[B_m - B_n]\mathrm{E}[B_n] + n\\
	&=& n .
  \end{eqnarray*}
  \label{sec:cor}
\end{proof}

\begin{proposition}
  Let $(B_t)$ be  $\mathbb{R}$-valued Brownian motion. Then $B_{cm} \sim c^{\frac{1}{2}}B_m$.
\end{proposition}
\begin{proof}
  Because $B_m$ is normal distributed for any $m > 0$, we then get
  \begin{eqnarray*}
	\mathrm{E} [e^{i\xi B_{cm}}] &=& e^{-\frac{1}{2}cm\xi^2}\\
	&=& e^{-\frac{1}{2}(c(m)^{\frac{1}{2}}\xi)^2}\\
	&=& \mathrm{E} [e^{i\xi c^{\frac{1}{2}}B_m}] .
  \end{eqnarray*}
\end{proof}

\begin{theorem}
  A $\mathbb{R}$-valued Brownian motion is a Gaussian process.
\end{theorem}
\begin{proof}
  The following idea using the independence of increments to prove the claim come from \cite{shilling}.
  We choose $0=t_0<t_1<\dots<t_n$, for $n \in \mathbb{N}$. Define
  $V = (B_{t_1},\dots,B_{t_n})^T$,  $K = (B_{t_1}-B_{t_0},\dots, B_{t_n}-B_{t_{n-1}})^T$ and 
  $A = 
  \begin{pmatrix}
	1      & 0      & \cdots & 0\\
	1      & 1      & \cdots & 0\\
	\vdots & \vdots & \ddots & \vdots \\
	1      & 1      & \cdots & 1
  \end{pmatrix}
	$.
  Let us see the characteristic function of $V$,
  \begin{eqnarray*}
	&&\mathrm{E} [e^{i\xi^T V}]\\
	&=& \mathrm{E} [e^{i\xi^T AK}]\\ 
	&=& \mathrm{E} [e^{iA^T\xi K}]\\
	&=& \mathrm{E} [\exp(i (\xi^{(1)}+\dots+\xi^{(n)}, \xi^{(2)}+\dots+\xi^{(n)}, \cdots,\xi^{(n)})] \\
	&\cdot& (B_{t_1}-B_{t_0}, B_{t_2}-B_{t_1},\dots,B_{t_n}-B_{t_{n-1}})^T)\\
	&\overset{ind.increments}{=}& \prod_{j=1}^n \mathrm{E} [\exp(i(\xi^{(j)+\dots+\xi^{(n)}})(B_{t_j}-B_{t_{t-1}}))]\\
	&\overset{stat.increments}{=}& \prod_{j=1}^n \exp(-\frac{1}{2}(t_j - t_{j-1})(\xi^{(j)}+\dots+\xi^{(n)})^2) \\
	&=& \exp\left(-\frac{1}{2}\sum_{j=1}^n (t_j - t_{j-1})(\xi^{(j)}+\dots+\xi^{(n)})^2\right)\\
    &=& \exp\left(-\frac{1}{2}\left(\sum_{j=1}^n t_j(\xi^{(j)}+\dots+\xi^{(n)})^2 - \sum_{j=1}^n t_{j-1}(\xi^{(j)}+\dots+\xi^{(n)})^2\right)\right)\\
	&=& \exp\left(-\frac{1}{2}\left(\sum_{j=1}^{n-1} t_j((\xi^{(j)}+\dots+\xi^{(n)})^2 - (\xi^{(j+1)}+\dots+\xi^{(n)})^2) + t_n(\xi^{(n)})^2\right)\right)\\
	&=& \exp\left(-\frac{1}{2}\left(\sum_{j=1}^{n-1} t_j\xi^{(j)}(\xi^{(j)}+2\xi^{(j+1)}+\dots+2\xi^{(n)}) + t_n(\xi^{(n)})^2\right)\right)\\
	&=& \exp\left(-\frac{1}{2}\left(\sum_{j,h=1}^n(t_j\wedge t_h)\xi^{(j)}\xi^{(h)}\right)\right).
  \end{eqnarray*}
  Recall with Proposition \ref{sec:cor}, $(t_j\wedge t_h)_{t,h=1,\dots,n}$ is the covariance matrix of $V$ and therefore it is symmetric and positive definit. The mean vector of it is zero, then we have been proved that the characteristic function is a form of some normal distributed random vector, i.e., $V$ is Gaussian.
\end{proof}

Schilling gave in his lecture \cite{shilling} the relationship between a one-dimensional Brownian motion and a n-dimensional Brownian motion.
In fact, $(B_t^{(l)})_{l=1,\dots,n}$ is Brownian motion if and only if $B_t^{(l)}$ is Brownian motion and all of the component are independent. Using this independence and the theorem of fubini in the characteristic function for high dimensional Brownian motion we can say a n-dimensional Brownian motion is also a Gaussian process.

\begin{definition}
  Let $(X_t)_{t\in T}$  be a stochastic process. $(Y_t)_{t\in T}$ is defined on the same probability space as $(X_t)_{t\in T}$ and said to be \emph{modification} of $(X_t)_{t\in T}$, if
  \begin{equation*}
	\mathcal{P}[X_t = Y_t] = 1 \hspace{1em} \forall \hspace{1em} t\in T.
  \end{equation*}
\end{definition}

\begin{theorem}[Kolmogorov Chentsov]
   Let $(X_t)_{t \ge 0}$ be a stochastic process on $\mathbb{R}^{n}$ such that
  \begin{equation*}
	\mathrm[|X_j - X_k|^\alpha] \le c |j - k|^{1+\beta} \hspace{1em}\forall \hspace{1em} j ,k \ge 0 \hspace{1em} \text{and} \hspace{1em} j\neq k,
  \end{equation*}
  for $\alpha, \beta > 0, c < \infty$. Then $(X_t)_t$ has a modification $(Y_t)_t$ with continuous sample path such that 
  \begin{equation*}
	\mathrm{E}[(\frac{|Y_j - Y_k|}{|j - k|^\gamma})^\alpha] < \infty
  \end{equation*}
  for all $\gamma \in (0, \frac{\beta}{\alpha})$.
  \label{sec:kolch}
\end{theorem}
\begin{proof}
  See \cite{loeve}, p.519.
\end{proof}

\begin{lemma}
   Let $(B_t)_{t\ge 0}$ be Brownian motion. Then
   \begin{equation*}
	 \mathrm{E}[B_t^{2k}] = (2k - 1)!! t^{2k}
   \end{equation*}
   for $k \in \mathbb{N}_0$.
   \label{sec:le1}
\end{lemma}

\begin{proof}
  Cf.\cite{shilling}. Taking expectation of $B_t^{2k}$, we get
  \begin{eqnarray*}
	\mathrm{E}[B_t^{2k}] &=& \frac{1}{\sqrt{2\pi t}}\int_{-\infty}^{\infty} x^{2k} e^{-\frac{x^2}{2t}}\,\mathop{dx}\\
	&\overset{x=\sqrt{2ty}}{=}& \frac{2^kt^k}{\sqrt{\pi}} \int_0^{\infty} y^{k-\frac{1}{2}} e^{-y}\,\mathop{dy}\\
	&=& \frac{2^kt^k}{\sqrt{\pi}} \int_0^{\infty} y^{k+\frac{1}{2}-1} e^{-y}\,\mathop{dy}\\
	&=& \frac{2^kt^k}{\sqrt{\pi}} \Gamma(k + \frac{1}{2})\\
	&=& \frac{2^kt^k}{\sqrt{\pi}} \Gamma(\frac{1}{2})\prod_{j=1}^k(j-\frac{1}{2}) \\
	&=& 2^k t^k\prod_{j=1}^k(\frac{2j-1}{2}) \\
	&=& (2k - 1)!!\cdot t^k
  \end{eqnarray*}
\end{proof}

\begin{corollary}
  Let $(B_t)_{t\ge 0}$ be Brownian motion. Then $B_t$ is $\gamma$-H\"older continuous on a compact set of time almost surely for all $\gamma < \frac{1}{2}$.
\end{corollary}

\begin{proof}
  Because of Lemma \ref{sec:le1}, we have
  \begin{eqnarray*}
	\mathrm{E}[(B_t-B_s)^{2k}] &=& \mathrm{E}[B_{t-s}^{2k}]\\
	&=& (2k - 1)!! \cdot |t-s|^{k}.
  \end{eqnarray*}
  In terms of the Theorem of Kolmogorov Chenstov, $B_t$ is $\gamma$-H\"older continuous a.s. for $\gamma \in (0, \frac{k}{2k})$.
\end{proof}

\newpage

%-------- 3. section -----------%
\section{Stable Measures and Stable Integrals}
In order to represent a integration form of fractional Brownian motion, we deal with the stable integral in this section. In fact, fractional Brownian motion is a Gaussian process with zero mean. To show Gaussian properties of it, we define it by a 
stable integral which can imaged as stochastic process of stable variables on time.

\subsection{Stable Variables}
%%We consider now the one-dimensional Brownian motion. In this section we need some notations, which are defined as followings
%\begin{equation*}
%\Delta^{[0,T]} = \left\{t_1,\dots,t_n|0=t_0<\dots<t_n=T\right\} 
%\end{equation*}
%$$
%|\Delta^{[0,T]}| = \max_{t_j \in \Delta^{[0,T]}}|t_j - t_{j-1}|
%$$.

%\begin{lemma}
%  Let $B_t$ be a Brownian motion. Then
%  $$
%  \sum_{t_j \in \Delta^{[0,T]}} |B_{t_j} - B_{t_{j-1}}|^2 \xmapsto[L^2(\mathcal{P})]{|\Delta^{[0,T]}|\rightarrow 0} T  
%  $$
%\end{lemma}

\begin{definition}
  Let $X$ be a random variable. $X$ is said to have a stable distribution, if there exist $0 < \gamma \le 2, \delta \ge 0, -1 \ge \kappa \ge 1, \theta \in \mathbb{R}$ such that its characteristic function can be described as following
  \begin{equation}
	\mathrm{E} [\exp i\xi X] =  \begin{cases} \exp\{i \xi \theta - |\delta\xi|^\gamma(1-i\kappa\cdot sgn(\xi)\tan \frac{\gamma\pi}{2})\},\hspace{1em}  \text{if}\hspace{1em} \gamma \in (0, 1) \cup (1, 2], \\
	    \exp\{i \xi \theta - |\delta\xi|(1+i\frac{2}{\pi}\kappa\cdot sgn(\xi)\ln |\xi|)\},\hspace{1em} \text{if}\hspace{1em} \gamma = 1.
	  \end{cases}
	\label{sec:stbl}
  \end{equation}
 Where
$$
 sgn(x) = \begin{cases} 1,\hspace{1em} \text{if}\hspace{1em} x > 1\\
   0,\hspace{1em} \text{if}\hspace{1em} x = 0 \\
   -1,\hspace{1em}\text{if}\hspace{1em} x < 0.
 \end{cases}
  $$
\end{definition}

Notice, we write $\Lambda(\gamma, \kappa, \theta, \delta)$ as one random variable whose characteristic function equals (\ref{sec:stbl}).

\begin{theorem}
  $X$ is Gaussian if and only $X \sim \Lambda(\gamma, \kappa, \theta, \delta)$ with $\gamma = 2$.
\end{theorem}
\begin{proof}
  In one hand, if $X$ is Gaussian, indeed, $\gamma$ must equal $2$. On the other hand, if $\gamma = 2$, then $i\kappa\cdot sgn(\xi)\tan \frac{\gamma\pi}{2}$ vanishes since $\tan(pi) = 0$. Therefore, $X$ is Gaussian because $\mathrm{E} [\exp i\xi X]=\exp\{i \xi \theta - |\delta\xi|^2\}$.
\end{proof}
Remark, if $\gamma=2$, then $\kappa$ is irrelevant in Definition. We specific $\kappa = 0$ without loss of generality. For instance, $B_t \sim \Lambda(2, 0, 0, \frac{\sqrt{t}}{2})$ when $(B_t)_t$ is Brownian motion.

\begin{definition}
  A random variable $X$ is said to be \emph{symmetric} if $X$ and $-X$ have the same distribution.
\end{definition}

\begin{proposition}
  Let $X$ be have a stable distribution. $X$ is  \emph{symmetric} if and only if $X \sim \Lambda(\gamma, 0, 0, \delta)$. I.e. its characteristic function has the form
  \begin{equation}
	\mathrm{E}[\exp\{i \xi X\}] = \exp\{-|\delta\xi|^\gamma\}
  \end{equation}
\end{proposition}
\begin{proof}
  The Definition of symmetricity implies
  \begin{eqnarray*}
	&&\exp\{i \xi \theta - |\delta\xi|^\gamma(1-i\kappa\cdot sgn(\xi)\tan \frac{\gamma\pi}{2})\}\\
	&=& \mathrm{E}[i\xi X]\\
	&=& \mathrm{E}[i\xi (-X)]\\
	&=& \mathrm{E}[i (-\xi) X]\\
	&=& \exp\{i (-\xi) \theta - |\delta\xi|^\gamma(1-i\kappa\cdot sgn(-\xi)\tan \frac{\gamma\pi}{2})\},
  \end{eqnarray*}
  this requires $\theta=\kappa=0$.
\end{proof}

\begin{corollary}
  Let $(B_t)_t$ be Brownian motion, then $B_t$ has a symmetric stable distribution.
\end{corollary}
\begin{proof}
  It is clear due to the previous Proposition.
\end{proof}


\subsection{Stable Random Measures}
In this subsection we suppose $(\mathrm{D}, \mathscr{D})$ and $(\mathrm{E}, \mathscr{E})$ are probability spaces, $\kappa(\cdot) : \Omega \rightarrow [-1, 1]$ is a measurable function.
\begin{definition}
	Let $\nu$ be a measure such that
	\begin{equation}
	  \nu : \mathscr{D} \rightarrow \mathscr{E}.\nonumber
	\end{equation}
	$\nu$ is said to be \emph{independently scattered}, if $\nu[D_1], \dots, \nu[D_n]$ are independent for any $D_1,\dots, D_n$ \emph{disjoint} $\in \mathscr{D}$ .
\end{definition}

For the next definition we need a notation
\begin{equation}
  \mathscr{G} = \{D \in \mathscr{D} : \mu[D] < \infty,\, \mu : \mathscr{D} \rightarrow \mathscr{E}\}.
\end{equation}
\begin{definition}
  Let $\nu$ be an independent cattered and $\sigma$-additive set function such that
\begin{equation*}
  \nu: \mathscr{G}  \rightarrow \mathrm{L}^{\infty}(\Omega, \mathscr{A}, \mathcal{P}). 
\end{equation*}
$\nu$ is said to be \emph{stable random measure} on $(D, \mathscr{D})$ with control measure $\mu$, degree $\gamma$ and skewness intensity $\kappa(\cdot)$ if 
\begin{equation}
  \nu[F] \sim \Lambda\brkt{\gamma, \frac{\int_F \kappa(x)\, \mu[\mathop{dx}]}{\mu[F]}, 0, (\mu[F])^{\frac{1}{\gamma}}}
\end{equation}
for $F \in \mathscr{D}$.
\end{definition}
Samorodnitsky and Taqqu show the existence of stable measures, see \cite{samorodnitsky}, pp.119$\sim$120.

\begin{example}
  Suppose $[0, T]$ is a index set and $0=t_0, t_1,\dots, t_k \in [0, T]$ for $k\in \mathbb{N}$. We show the mapping $\nu : \mathscr{B}([0, T]) \rightarrow \mathscr{B}(\mathbb{R})$, where $\nu[A_j](\omega):= B_{t_{j+1}}(\omega) - B_{t_{j}}(\omega), A_j=[t_{j}, t_{j+1})$.\\
	Firstly, we show $\nu$ is independently scattered and $\sigma$-additive. We take $\{A_j\}$ such that $\cup_{j=1}^{\infty}A_j = [0, T]$. $\{\nu[A_k]\}_{k=1}^{\infty}$ has independent elements since $B_{t_1} - B_{t_0}, \dots,  B_{t_{j+1}}- B_{t_{j}} $ are independent.\\
	Secondly, 
	\begin{eqnarray*}
	  \nu[\brkt{\cup_{j=1}^{\infty}A_j}] &=& B_T - B_1\\
	  &=& \sum_{j=1}^{\infty} (B_{t_{j+1}} - B_{t_j})\\
	  &=& \sum_{j=1}^{\infty} \nu[A_j].
	\end{eqnarray*}
	Finally, 
	\begin{eqnarray*}
	  \mathrm{E}[\exp(i\xi\nu[A_j])] &=& \mathrm{E}[\exp(i\xi(B_{t_{j+1}} - B_{t_{j}})]\\
	  &=& \exp(-\frac{(t_{j+1}-t_j)\xi^2}{2})
	\end{eqnarray*}
	Comparing with (\ref{sec:stbl}), we deduce the control measure must be $\frac{|\cdot|}{2}$. In fact, $\nu[A_j] \sim \Lambda(2, 0, 0, \frac{|t_{j+1} - t_j|}{2})$.
  \label{sec:ex2}
\end{example}

\subsection{Stable Integrals}
Samorodnitsky and Taqqu defined an Integral with respect to stable measure as stochastic process in \cite{samorodnitsky}. The stable Integral is given as 
\begin{equation}
  \int_F f(x)\, \nu(\mathop{dx}).
  \label{sec:stbint}
\end{equation}
Where $f : F \rightarrow \mathbb{R}$ is a measurable function such that 
\begin{equation}
\begin{cases} \int_F |f(x)|^\gamma \mu(\mathop{dx}) < \infty,\hspace{1em} \text{if} \hspace{1em} \gamma \in (0, 1) \cup (1, 2],\\
	\int_F |\kappa(x) f(x) \ln|f(x)||\mu(\mathop{dx}) < \infty,\hspace{1em} \text{if} \hspace{1em} \gamma = 1,
  \end{cases}
\end{equation}
, $\gamma, \mu, \kappa$ are, respectively, degree, control measure and skewness intensity of the stable measure $\nu$.

Some properties of the stable function are given by Samorodnitsky and Taqqu.

\begin{proposition}
  Let $J(f)$ be a stable integral as form of (\ref{sec:stbint}). Then 
  \begin{equation*}
	J(f) \sim \Lambda(\gamma, \kappa, \theta, \delta)
  \end{equation*}
for the degree, control measure, skewness intensity, respectively, 
\begin{eqnarray*}
\gamma &\in& (0, 2],\\
\kappa &=& \frac{\int_F \kappa(x) |f(x)|^\gamma\cdot sgn(f(x)) \mu(\mathop{dx})}{\int_F|f(x)|^\gamma\mu(\mathop{dx})},\\
\theta &=&
\begin{cases}
0 , \hspace{1em} \text{if} \hspace{1em} \gamma \in (0, 1) \cup (1, 2],\\
  -\frac{2}{\pi}\int_F \kappa(x) f(x) \ln|f(x)|\mu(\mathop{dx}), \hspace{1em} \text{if} \hspace{1em} \gamma = 1,
\end{cases}\\
\delta &=& \brkt{\int_F |f(x)^\gamma|\mu(\mathop{dx})}^{\frac{1}{\gamma}}, 
\end{eqnarray*}
of the stable measure $\nu$.
\label{sec:stbint2}
\end{proposition}
\begin{proof}
  See.\cite{samorodnitsky}, p.124, Proposition 3.4.1 .
\end{proof}
\begin{proposition}
  The stable integral is linear, in fact,
\begin{equation}
  J(c_1f_1 + c_2f_2) \overset{a.s.}{=}c_1J(f_1) + c_2J(f_2)
  \label{sec:stblin}
\end{equation}
for any $f_1, f_2$ integrable with respect to some stable measure and real numbers $c_1, c_2$. 
\end{proposition}
\begin{proof}
  See \cite{samorodnitsky}, p.117, Property 3.2.3 .
\end{proof}
\newpage
%---------- 3. section ------------%
\section{Fractional Brownian Motion}
\setcounter{equation}{0}
The fractional Brownian motion(FBM) was defined by Kolmogorov primitively. After that Mandelbrot and Van Ness has present the work in detail. This section is concerned with the definition and some properties of it.

\subsection{Definition of Fractional Brownian Motion}
Mandelbrot and Van Ness \cite{mandelbrot} gave a integration presentation of the definion of FBM.
\begin{definition}
  Let $(U_H(t))_{t\in \mathbb{R}}$ be a $\mathbb{R}$-valued stochatstic process and $H$ be such that $0<H<1$. $(U_H(t))$ is said to be \emph{fractional Brownian motion} if 
  \begin{eqnarray}
	U_H(t) - U_H(s) &=& \frac{1}{\Gamma(H+\frac{1}{2})}\brkt{\int_{\mathbb{R}} \mathbbm{1}_{\{t\ge u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u\le s\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}}
	\label{sec:frtbm}
  \end{eqnarray}
  for $t\ge s, t, s \in \mathbb{R}$. Where $(B_u)$ is defined as two-sides Brownian motion, the integral is in sense of stable integral as in previous section. $H$ is called Hurst exponent or Hurst index of FBM.
\end{definition}

As usual, we set $U_H(0) = 0$, then equation (\ref{sec:frtbm}) is equivalent to
	 \begin{eqnarray}
	   U_H(t) &=& \frac{1}{(\Gamma(H+\frac{1}{2}))^2}\brkt{\int_{\mathbb{R}} \mathbbm{1}_{\{t\ge u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u\le 0\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}}.
	\label{sec:fbm}
  \end{eqnarray}

%  We take (\ref{sec:fbm}) as definion of FBM now.
\begin{lemma}
  The equation (\ref{sec:fbm}) is well-defined, $U_H(t)$ has stable distribution and 
  \begin{equation*}
	U_H(t) \sim \Lambda(2, 0, 0, \frac{1}{\Gamma(H+\frac{1}{2})}(\int_{\mathbb{R}} |f(u)^2|\, \mathop{\frac{du}{2}})^\frac{1}{2}),
  \end{equation*}
  Where $f(x)$ is the integrand of integral in (\ref{sec:fbm}).
  \label{sec:l2}
\end{lemma}
\begin{proof}
  Firstly, $B_t$ is Gaussian and symmetric stable measure with zero mean and $\frac{|\cdot|}{2}$ as the control measure of it shown in Example \ref{sec:ex2}. \\
  Secondly, the well-definition was refered to by Samorodnitksy and Taqqu in \cite{samorodnitsky}, p.321, Proposition 7.2.6 for not only $H= \frac{1}{2}$, but also $H\neq \frac{1}{2}$. Which satisfies, in other words, the condition $\int_{-\infty}^{\infty}f^2(u)\,\mathop{\frac{du}{2}} < \infty$.\\
  Finally, in terms of Proposition \ref{sec:stbint2}, we get the claim.
\end{proof}
Remark that, if we take $H=\frac{1}{2}$ choosing a restriction of the integrand on $\mathbb{R}_+$, $(U_2(t))_{t\ge 0}$ is a Brownian motion. 	
\begin{theorem}
  Let $(U_H(t))_t$ be a FBM. Then $U_H(t) \sim \mathcal{N}(0, \frac{1}{\Gamma(H+\frac{1}{2})^2}(\int_{\mathbb{R}} |f(u)^2|\, \mathop{du}))$.
  \label{sec:the1}
\end{theorem}
\begin{proof}
  In terms of Lemma \ref{sec:l2}, $\mathrm{E}[i\xi U_H(t)] = \exp\{-\xi^2 \frac{1}{2\Gamma(H+\frac{1}{2})^2}(\int_{\mathbb{R}} |f(u)^2|\, \mathop{du})\}$. The rest is clear thanks to the form of characteristic function of a Gaussian random variable.
\end{proof}
\begin{lemma}
  Let $(U_H(t))_t$ be a FBM. Then $U_H(t)$ has an expected value $0$ and variance $t^{2H}\, \mathrm{E} U^2_H(1)$ for any $t \in \mathbb{R}$.
  \label{sec:fbmp1}
\end{lemma}
\begin{proof}
  It is clear that $U_H$ is Gaussian with zero mean due to Lemma \ref{sec:l2}. We suppose that $t \ge s \ge 0,  c(H)=\frac{1}{(\Gamma(H+\frac{1}{2}))^2}$.
  \begin{eqnarray}
	&&\mathrm{E}[(U_H(t) - U_H(s))^2]\nonumber\\
	&=& c(H)\mathrm{E}[\brkt{\int_{\mathbb{R}} [\mathbbm{1}_{\{t\ge u\}}\cdot(t-u)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{s\ge u\}}\cdot(s-u)^{H-\frac{1}{2}} \,\mathop{U_H(u)}}^2]\nonumber\\
	&\overset{\text{Theorem \ref{sec:the1}}}{=}& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{[\mathbbm{1}_{\{t\ge u\}}\cdot(t-u)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{s\ge u\}}\cdot(s-u)^{H-\frac{1}{2}}}^2 \,\mathop{du}]\nonumber\\
	&=& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{\mathbbm{1}_{\{t-s \ge u\}}\cdot(t-s-u)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{0\ge u\}}\cdot(-u)^{H-\frac{1}{2}}}^2 \,\mathop{du}]\nonumber\\
	&\overset{m=t-s}{=}& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{\mathbbm{1}_{\{m \ge u\}}\cdot(m-u)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{0\ge u\}}\cdot(-u)^{H-\frac{1}{2}}}^2 \,\mathop{du}]\nonumber\\
	&\overset{u=ml}{=}& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{\mathbbm{1}_{\{m \ge ml\}}\cdot(m-ml)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{0\ge ml\}}\cdot(-ml)^{H-\frac{1}{2}}}^2 m\cdot\,\mathop{dl}]\nonumber\\
	&=& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{\mathbbm{1}_{\{1 \ge l \}}\cdot(1-l)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{0\ge l\}}\cdot(-l)^{H-\frac{1}{2}}}^2 \cdot m^{2H-1}\cdot m\,\mathop{dl}]\nonumber\\
	&=& c(H)m^{2H}\mathrm{E}[U_H(1)^2]\nonumber\\
	&=& c(H)(t-s)^{2H}\mathrm{E}[U_H(1)^2]
	\label{sec:eqn1}
  \end{eqnarray}
  Using the same calculation, we get
  \begin{equation}
	\mathrm{E}[(U_H(t)^2] = c(H)t^{2H}\mathrm{E}[U_H(1)^2].
	\label{sec:eqn2}
  \end{equation}
%  $c(H), t^{2H}, \mathrm{E}[U_H(1)^2]$ are nonegative, then
%  $$\mathrm{E}[(U_H(t)] = c(H)^{\frac{1}{2}}t^{H}\mathrm{E}[U_H(1)].$$

%  Because of (\ref{sec:eqn1}),
%  $$
%	\mathrm{E}[(U_H(2) - U_H(1))^2] = c(H)\mathrm{E}[U_H(1)^2].
%  $$
%  And again
%  $$
%  \mathrm{E}[(U_H(2) - U_H(1))] = c(H)^{\frac{1}{2}}\mathrm{E}[U_H(1)]
%  $$
%Conserquently,
%\begin{eqnarray}
%	0 &=& \mathrm{E}[(U_H(2)(U_H(2)-2U_H(1)))]\nonumber\\
%	&\overset{Cauchy-Schwartz}{\le}& (\mathrm{E}[U_H(2)^2]\cdot\mathrm{E}[(U_H(2)-2U_H(1))^2])^{\frac{1}{2}}\nonumber
% \end{eqnarray}
% WLOG, let $\mathrm{E}[U_H(2)^2] \neq 0$, otherwise $\mathrm{E}[U_H(1)] = 0$ due to . Then we get 
%  $$
%  \mathrm{E}[(U_H(2)] = 2\mathrm{E}[(U_H(1)]
%  $$
%  \begin{eqnarray}
%	(1+c(H)^{\frac{1}{2}})\mathrm{E}[((U_H(1))] &=& \mathrm{E}[((U_H(2))] \nonumber\\
%	&=&  2\mathrm{E}[((U_H(1))]\nonumber
%  \end{eqnarray}
%  that means $\mathrm{E}[((U_H(1))] = 0$. Due to (\ref{sec:eqn2}), $\mathrm{E}[(U_H(t)] = 0$ for $t\ge 0$.
%  Therefore, (\ref{sec:eqn2}) is variance of $U_H(t)$.
  (\ref{sec:eqn2}) is variance of $U_H(t)$ due to $\mathrm{E}[(U_H(t)] = 0$.
\end{proof}
To normalize the variance, a definition of standard FBM is given.

\begin{definition}
  A stochastic process $(U_H(t))_{t}$ is said to be a \emph{standrad fractional Brownian motion}(sFBM) if
  \begin{equation}
U_H(t) = \hat{c}(H) \int_{\mathbb{R}} \mathbbm{1}_{\{t\ge u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u\le 0\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}.
\label{sec:eqn3}
\end{equation}
Where $\hat{c}(H) = \frac{1}{\mathrm{E}[U_H(1)^2]} $.
\end{definition}
We consider from now on sFBM as FBM.

\begin{theorem}
  Let  $(U_H(t))_{t}$ be a FBM. The Covariance function of $U_H(t), U_H(s)$ is $ \frac{1}{2}(t^{2H} + s^{2H} - |t-s|^{2H})$ for $t, s \in \mathbb{R}$.
\end{theorem}

\begin{proof}
  Cf.\cite{mandelbrot}, Theorem 5.3 .
  \begin{eqnarray}
	\mathrm{Cov}[U_H(t), U_H(s)] &=& \mathrm{E}[U_H(t)U_H(s)] \nonumber\\
	&=& \frac{1}{2}\brkt{\mathrm{E}[U_H(t)^2] + \mathrm{E}[U_H(s)^2] - \mathrm{E}[(U_H(t) - U_H(s))^2]} \nonumber\\
	&\overset{(\ref{sec:eqn2})}{=}& \frac{1}{2}(t^{2H} + s^{2H} - |t-s|^{2H})
	\label{sec:eqn4}
  \end{eqnarray}
\end{proof}

\begin{theorem}
  $(U_H(t))_{t}$ is Gaussian process.
\end{theorem}

\begin{proof}
  We just need to prove that for an arbitrary finite linear combination of values of time is Gaussian. We take $t_1, \dots, t_k \in T, c_1, \dots, c_k \in \mathbb{R}$, and the stable integral $J(f) $ is a linear functional with $\gamma=2, \kappa=0, \theta=0, \delta= (\frac{1}{2}\int_{-\infty}^{\infty}f^2(u)\,\mathop{du})^{\frac{1}{2}}$ due to Corollary \ref{sec:stblin}. Suppose $f_1,\dots, f_k$ are integrands of the form of stable integral of $U_H(t_1), \dots, U_H(t_k)$. \\
  Consider now, according to the Minkowski inequality,
  \begin{eqnarray*}
	\int_{-\infty}^{\infty}(\sum_{j=1}^k c_jf_j)^2\,\mathop{du} &\le& \sum_{j=1}^k \underbrace{\int_{-\infty}^{\infty}(c_jf_j)^2\,\mathop{du}}_{<\infty}\\
	&<& \infty.
  \end{eqnarray*}
Moreover,
  \begin{eqnarray*}
	\sum_{j=1}^k c_jU_H(t_j) &=& \sum_{j=1}^k c_jJ(f_j)\\
	&=& J(\sum_{j=1}^k c_jf_j)\\
	&\sim& \Lambda(2, 0, 0, (\frac{1}{2}\int_{-\infty}^{\infty}(\sum_{j=1}^k c_jf_j)^2\,\mathop{du})^{\frac{1}{2}})
  \end{eqnarray*}
  is Gaussian and the rest is clear.
\end{proof}

\begin{corollary}
  Let $(U_H(t))_{t}$ be a FBM, then $(U_H(t))_{t}$ has stationary and H-self similar increments . 
\end{corollary}
\begin{proof}
 Assume that $s \ge u $. Because the joint distribution of $(U_H(s), U_H(u))^T$ is Gaussian, $(1, -1) \cdot (U_H(s), U_H(u))^T $ is Gaussian. In other words,  $U_H(s) - U_H(u) \sim \mathcal{N}(0, (s-u)^{2H})$ which is only dependent on $(s-u)$ and $(U_H(t))$ has therefore stationary increments.\\
   $(U_H(t))$ has zero mean and $\mathrm{Var[U_H(s)]} = s^{2H}\mathrm{Var}[U_H(1)]$ we get $U_H(s) \sim s^HU_H(1)$ due to it is Gaussian.
   To show FBM has H-self similar increments, we have to prove\\ $(U_H(zt_1), U_H(zt_2),\dots, U_H(zt_n))) \sim (z^HU_H(t_1), z^HU_H(t_2),\dots, z^HU_H(t_n))$ for any $z > 0$. Obviously, the former and the latter of the term are Gaussian and $\mathrm{Var}[U_H(zt_i), U_H(zt_j)] = \mathrm{Var}[z^HU_H(t_i), z^HU_H(t_j)] = \frac{1}{2}z^{2H}(t_i^{2H} + t_j^{2H} - |t_i-t_j|^{2H})$. Thus they have the same covariance matrix and zero mean in their characteristic function. Using uniqueness theorem we can prove the claim. 
\end{proof}

\subsection{Regularity}
\begin{theorem}[Kolmogorov Chentsov]
  FBM has almost surely continuous sample path.  
\end{theorem}

\begin{proof}
  Cf.\cite{mandelbrot} Proposition 4.1 . Let $(U_H(t))_{t}$ be FBM with Hurst index $H$. Fix $\alpha$ such that $1 < \alpha H$. Let look at the expectation of $(U_H(t) - U_H(s))^\alpha$ using the same calculation in (\ref{sec:eqn1})
  \begin{eqnarray}
	\mathrm{E} [(U_H(t) - U_H(s))^\alpha] &=& |t-s|^{\alpha H} \cdot \underbrace{\mathrm{E}\brkt{\int_{\mathbb{R}} \mathbbm{1}_{\{1\ge u\}}\cdot (1-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u\le 0\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}}^\alpha}_{c(\alpha,H)}\nonumber\\
	&=& c(\alpha,H) \cdot |t-s|^{\alpha H}.
	\label{sec:eqn5}
  \end{eqnarray}
  We choose $\beta = \alpha H -1$ and $\gamma \in (0, H-\frac{1}{\alpha})$ then the rest follows from Theorem \ref{sec:kolch} .
\end{proof}

Remark, $U_H(t)$ is, in fact, $\gamma$-H\"older continuous with $\gamma < H$ almost surely.

\begin{theorem}
  The sample path of FBM is almost surely not differentiable.
\end{theorem}

\begin{proof}
  Cf. \cite{mandelbrot} Proposition 4.2 . Fix $\omega \in \Omega$, we assume $c > 0, t_j \rightarrow s$.
\begin{eqnarray}
 && \mathcal{P}[\limsup\limits_{t\rightarrow s} |\frac{U_H(t) - U_H(s)}{t-s}| > c]\nonumber\\
 &=& \mathcal{P}[\lim\limits_{j\rightarrow\infty}\sup\limits_{t_j\neq s} |\frac{U_H(t_j) - U_H(s)}{t_j-s}| > c]\nonumber\\
 \label{sec:mass}
 \end{eqnarray}
 Since continuity of measures from above, then
 \begin{eqnarray*}
 \text{(\ref{sec:mass})} &=& \lim\limits_{j\rightarrow\infty} \mathcal{P}[\sup\limits_{t_j\neq s} |\frac{U_H(t_j) - U_H(s)}{t_j-s}| > c]\\
 &\ge& \lim\limits_{j\rightarrow\infty} \mathcal{P}[|\frac{U_H(t_j) - U_H(s)}{t_j-s}| > c]\\
 &=& \lim\limits_{j\rightarrow\infty} \mathcal{P}[\frac{|(t_j-s)^{H}U_H(1)}{t_j - s}| > c]\\
 &=& \lim\limits_{j\rightarrow\infty} \mathcal{P}[|(t_j-s)^{H-1}U_H(1)| > c]\\
 &=& \lim\limits_{j\rightarrow\infty} \mathcal{P}[|U_H(1)| > \underbrace{|t_j - s|^{1-H}}_{\overset{j\rightarrow\infty}{\longrightarrow}\, 0}c]\\
 &\overset{j\rightarrow \infty}{\longrightarrow}& 1 
\end{eqnarray*}
\end{proof}

\begin{theorem}
  Let $(U_H(k))_{k}$ be FBM. The conditional expectation of $U_H(s)$  given $U_H(t)=x$ is
\begin{equation*}
  \frac{|\frac{s}{t}|^{2H} + 1 - |\frac{s}{t} - 1|^{2H}}{2} \cdot x
\end{equation*}
for all $ s < t$.
\end{theorem}
\begin{proof}
  Cf. \cite{mandelbrot} Theorem 5.3. Taking conditional expectation of $U_H(s)$ given $U_H(t)$,
  \begin{eqnarray*}
  &&\mathrm{E}[U_H(s)|U_H(t)]\\
  &\overset{\text{Corollary \ref{sec:condi}}}{=}& \mu_s + \rho_{s,t} (\frac{\sigma_s}{\sigma_t}U_H(t) - \mu_t)\\
  &=& \rho\frac{\sigma_s}{\sigma_t}U_H(t)\\
  &=& \frac{\rho\cdot\sigma_s\sigma_t\cdot U_H(t)}{\sigma_t^2}\\
  &=& \frac{\mathrm{E}[U_H(s)U_H(t)]}{\mathrm{E}[U_H^2(t)]} \cdot U_H(t)\\
  &\overset{\text{(\ref{sec:eqn4})}}{=}& \frac{s^{2H} + t^{2H} - |s-t|^{2H}}{2\mathrm{E}[U_H^2(t)]}\cdot U_H(t)\\
  &=&  \frac{s^{2H} + t^{2H} - |s-t|^{2H}}{2t^{2H}}\cdot U_H(t)\\
  &=& \frac{|\frac{s}{t}|^{2H} + 1 - |\frac{s}{t} - 1|^{2H}}{2} \cdot U_H(t)
  \end{eqnarray*}
\end{proof}

\subsection{Fractional Brownian Noise}
\begin{definition}
  Let $(U_H(t))_t$ be a FBM. The \emph{fractional Brownian noise} is a sequence $(S_k)_k$ forms
\begin{eqnarray*}
  S_H(k) &=& U_H(k+1) - U_H(k)
\end{eqnarray*}
for $k \in \mathbb{R}$.
\end{definition}

\begin{proposition}
  Fractional Brownian noise is stationary and its autocovariance is 
  \begin{eqnarray}
	\varsigma_{S_H}(\tau) &=&  \frac{1}{2} (|\tau + 1|^{2H} - 2|\tau|^{2H} + |\tau-1|^{2H})
  \label{sec:auto}
  \end{eqnarray}
  for $\tau \in \mathbb{R}$.
\end{proposition}

\begin{proof}
  Cf. \cite{nourdin}, p.333, Proposition 7.2.9 . The first part of the claim is clear due to FBM has stationary increments.\\
  In terms of definition of fractional Brownian noise we have
  \begin{eqnarray*}
	\varsigma_{S_H}(\tau) &=& \mathrm{E}[S_H(0)S_H(\tau)] \\
	&=& \mathrm{E}[(U_H(1) - U_H(0))(U_H(\tau+1)-U_H(\tau))]\\
	&=& \mathrm{E}[(U_H(1) - U_H(0))(U_H(\tau+1)-U_H(\tau))]\\
    &=& \mathrm{E}[U_H(1)(U_H(\tau+1)-U_H(\tau))]\\
	&=& \mathrm{E}[U_H(1)U_H(\tau+1)] - \mathrm{E}[U_H(1)U_H(\tau)]\\
	&=& \varsigma_{U_H}(1, \tau+1) - \varsigma_{U_H}(1, \tau)\\
	&\overset{\text{(\ref{sec:eqn4})}}{=}& \frac{1}{2} (1 + |1+\tau|^{2H} - \tau^{2H}) - \frac{1}{2}(1 + \tau^{2H} - |1 - \tau|^{2H})\\
	&=& \frac{1}{2}(|1+\tau|^{2H} - 2|\tau|^{2H} + |1-\tau|^{2H})
  \end{eqnarray*}
  for $\tau \in \mathbb{R}$.
\end{proof}


\begin{definition}
  A stationary stochastic process $(X_t)_t$ is said to have \emph{long memory} if its autocovariance $\varsigma_X(\tau)$ tend to $0$  with so slowly such that
  $ \sum_{\tau = -\infty} ^{\infty} \varsigma_X(\tau)$ diverges for $\tau \in \mathbb{Z}$.
\end{definition}

\begin{theorem}
  The fractional Brownian noise has long memory.
  \label{sec:lmemory}
\end{theorem}
\begin{proof}
  Cf. \cite{nourdin}, p.335, Proposition 7.2.10 .\\
  Without loss of generality, we suppose $\tau \in \mathbb{Z}^+_0$. For $\tau\in\mathbb{Z}^-$, we deal with it in a similar way and $\tau=0$ is clear. Following (\ref{sec:auto}), we have
  \begin{eqnarray*}
  &&\varsigma(\tau) \\
  &=& \frac{1}{2} \tau^{2H-2}\{ \tau^2[(1+\frac{1}{\tau})^{2H} - 2 + (1-\frac{1}{\tau})^{2H}] \} \\
  &=& \frac{1}{2} \tau^{2H-2}\{ \frac{(1+\frac{1}{\tau})^{2H} - 1 } {\frac{1}{\tau^2}} -  \frac{1 - (1-\frac{1}{\tau})^{2H}} {\frac{1}{\tau^2}} \}
\end{eqnarray*}

We deal with the former of the content in $\{\,\}$ with L'H\^opital's rule as $\tau$ tend to infinity.
\begin{eqnarray*}
&\lim\limits_{\tau\rightarrow\infty}&\frac{(1+\frac{1}{\tau})^{2H} - 1 } {\frac{1}{\tau^2}}\\
&=& \lim\limits_{\tau\rightarrow\infty} \frac{2H(1+\frac{1}{\tau})^{2H-1}(-\frac{1}{\tau^2})}{-\frac{4}{\tau^3}} \\
&=&   \lim\limits_{\tau\rightarrow\infty}\frac{H(1+\frac{1}{\tau})^{2H-1}}{\frac{1}{\tau}}.
\end{eqnarray*}

We calculate the Latter of the content in $\{\,\}$ in a similar way. Then
\begin{eqnarray*}
&\lim\limits_{\tau\rightarrow\infty}&\varsigma(\tau)\\
&=&\lim\limits_{\tau\rightarrow\infty} \tau^{2H-2} \frac{1}{2}\{ \frac{H(1 +\frac{1}{\tau})^{2H-1}}{\frac{1}{\tau}} - \frac{H(1-\frac{1}{\tau})^{2H-1}}{\frac{1}{\tau}}\}\\
&\overset{\text{L'H\^opital}}{=}&\lim\limits_{\tau\rightarrow\infty}  \frac{1}{2} \tau^{2H-2}\{\frac{H(2H-1)(1+\frac{1}{\tau})^{2H-2}(-\frac{1}{\tau^2})}{-\frac{1}{\tau^2}} - \frac{H(2H-1)(1-\frac{1}{\tau})^{2H-2}\frac{1}{\tau^2}}{-\frac{1}{\tau^2}}\}\\
&=& \lim\limits_{\tau\rightarrow\infty} \frac{1}{2} \tau^{2H-2}\{H(2H-1)(1+\frac{1}{\tau})^{2H-2} + H(2H-1)(1-\frac{1}{\tau})^{2H-2}\}\\
&=& \lim\limits_{\tau\rightarrow\infty}\frac{1}{2} \tau^{2H-2} 2H(2H-1)\\
&=& \lim\limits_{\tau\rightarrow\infty} H(2H-1) \tau^{2H-2}
\end{eqnarray*}


%In fact, for $\tau$ is large,
%\begin{eqnarray*}
%  \frac{\varsigma(\tau+1)}{\varsigma(\tau)} &=& (\frac{\tau+1}{\tau})^{2H-2}
%\end{eqnarray*}

Convergence of $\sum\limits_{\tau=-n}^{n} \varsigma(\tau)$ as $n \rightarrow \infty$ requires $2H-2 < -1$, namely, $H < \frac{1}{2}$.
Otherwise, if $\frac{1}{2} < H < 1, \sum\limits_{\tau=-\infty}^{\infty} \varsigma(\tau) $ diverges. With $\lim\limits_{\tau\rightarrow \infty}\varsigma(\tau)=0$, we achieve the claim. 
\end{proof}

\begin{corollary}
  Let $S_H$ be fractional Brownian noise , $\tau\in\mathbb{Z}$ and $\varsigma_{S_H}(\cdot)$ its autocovariance. Then $\sum_{\tau\in\mathbb{Z}}\varsigma^2_{S_H}(\tau)<\infty$ if and only if $H < \frac{3}{4}$.
\end{corollary}
\begin{proof}
  Cf. \cite{nourdin}, p.72, Lemma 6.3. In Theorem \ref{sec:lmemory}, we have $\varsigma_{S_H}^2(\tau) \propto H^2(2H-1)^2\tau^{4H-4} $. The sum of it is finite if and only if, according to the same reason as in Theorem \ref{sec:lmemory}, $\tau^{4H-4}<-1$. That means $H < \frac{3}{4}$.
\end{proof}
\subsection{FBM is not Semimartingale for $H\neq \frac{1}{2}$}
Let look at our Integration representation for FBM, in the case of FBM with Hurst index $\frac{1}{2}$, it must be an ordinary Brownian motion. Except for this, we will show FBM is not a seimimartingal.
\begin{definition}
  The \emph{Hermite polynomials} form as following
  \begin{equation}
	\eta_n(u) = (-1)^n e^{\frac{u^2}{2}} (\frac{\partial^n }{\partial u^n}e^{-\frac{u^2}{2}}),
	\label{sec:hermite}
  \end{equation}
  for $u\in \mathbb{R}, n\in \mathbb{N}_0$. 
\end{definition}
\begin{proposition}
  Let $(\eta_n)_{n\in \mathbb{N}}$ be a family of Hermite polynomials, $W$ a standard Gaussian variable, $f, g: \mathbb{R}\rightarrow \mathbb{R}$ continuously differentiable. It has then following properties 
  \begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
	\item $\eta_{n+1}(u)=(n+1)\eta_n(u)$ and $\eta_{n+2}(u)=u\cdot\eta_{n+1}(u) - (n+1)\eta_n(u)$ for all $n\in\mathbb{N}_0, u\in\mathbb{R}$.
%	\item The family $\{\frac{1}{\sqrt{n!}}\eta_n\}_{n\in \mathbb{N}}$ is an orthonormal basis of $\mathcal{L}^2(\mathbb{R}, \frac{1}{\sqrt{2\pi}}e^{-\frac{u^2}{2}}\, du)$
	\item Let $W, V$ be standard Gaussian distributed such that $(W, V)$ have a disjoint Gaussian distribution. Then
	  \begin{equation*}
		\int_{\Omega} \eta_j(W)\cdot\eta_k(V) \mathop{\mathcal{P}} = \begin{cases} j!\,(\mathrm{E}[WV])^j & \mbox{if } j=k,\\
		  0 &\text{otherwise}.
		\end{cases}
	  \end{equation*}
	\item Let $W$ be standard Gaussian distributed, then
	  \begin{equation*}
		\frac{1}{j!} \int_\Omega \eta_j(W) \eta_k(W) \mathop{\mathcal{P}} = \begin{cases} 1 & \mbox{if } j=k,\\
		  0 & \text{otherwise}.
		\end{cases}
	  \end{equation*}
\end{enumerate}
Remark that, $(iii)$ means, in fact, $\{\frac{1}{\sqrt{j!}}\cdot\eta_j\}_{j=0}^{\infty}$ is an orthonormal basis in $\mathcal{L}^2(\mathbb{R}, \mathscr{B}(\mathbb{R}), e^{-\frac{x^2}{2}}\mathop{dx})$.
		\label{sec:herpro}
\end{proposition}
\begin{proof}
  See \cite{nourdin} p.3, Propostion 1.3.
\end{proof}

\begin{lemma}
  Let $(U_H(t))_{t}$ be a FBM, $W$ standrad Gaussian variable, and $f: \mathbb{R}\rightarrow \mathbb{R}$, Borel-measurable function such that $\mathrm{E}[f^2(G)] < \infty$. Then, 
  \begin{equation*}
	\frac{1}{n}\sum\limits_{j=1}^n f(U_H(j)-U_H(j-1)) \overset{\text{in }\mathcal{L}^2(\mathcal{P})}{\rightarrow} \mathrm{E}[f(W)],
  \end{equation*}
  as $n$ tend to $\infty$. In particular,
  \begin{equation}
	\sum_{j=1}^n |U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^\beta \overset{\text{in }\mathcal{L}^2(\mathcal{P})}{\rightarrow} \begin{cases}
	  0 &\mbox{if } \beta > \frac{1}{H}\\
	  \infty &\mbox{if } \beta < \frac{1}{H}
	\end{cases}
	\label{sec:semilemma}
  \end{equation}
  as $n$ tend to $\infty$.
\end{lemma}
\begin{proof}
  C.f. \cite{nourdin}, p.17, Theorem 2.1 .\\
  Firstly, because $\mathrm{E}[f^2(W)] < \infty$ one has $f \in \mathcal{L}^2(\mathbb{R}, \mathscr{B}(\mathbb{R}), e^{-\frac{x^2}{2}}\mathop{dx})$. In terms of Proposition \ref{sec:herpro}(iii), taking expectation
  \begin{equation*}
	\mathrm{E}[f(x)] = \mathrm{E}[\sum\limits_{j=0}^{\infty} \frac{a_jH_j(x)}{\sqrt{j!}}], 
  \end{equation*}
  for $x \in \mathbb{R}$.
  Notice $H_0=1$ due to (\ref{sec:hermite}). Setting $x=W$, Equating coefficients leads to $a_0 = \mathrm{E}[f(W)]$. Moreover,
  \begin{eqnarray*}
	&& \mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n f(U_H(j) - U_H(j-1)) - \mathrm{E}[f(W)]\}^2]\\
	&=& \mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n (f(U_H(j) - U_H(j-1)) - \mathrm{E}[f(W)])\}^2] \\
	&=& \mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n (\sum\limits_{k=0}^\infty \frac{a_k}{\sqrt{k!}} H_k(U_H(j) - U_H(j-1))) - \mathrm{E}[f(W)]\}^2] \\
	&=& \mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n (\sum\limits_{k=1}^\infty \frac{a_k}{\sqrt{k!}} H_k(U_H(j) - U_H(j-1)))\}^2] \\
  \end{eqnarray*}
  Consider now 
  \begin{equation*}
	\mathrm{E}[f^2(W)] < \infty,
  \end{equation*}
  which requires $\sum\limits_{k=1}^\infty (a_k)^2 < \infty$. Then
  \begin{eqnarray*}
	&& \mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n f(U_H(j) - U_H(j-1)) - \mathrm{E}[f(W)]\}^2]\\
	&=& \frac{1}{n^2} \mathrm{E}[\sum\limits_{k=1}^\infty \frac{a_k^2}{k!}(\sum\limits_{j=1}^n H_k(U_H(j) - U_H(j-1)))^2] \\
	&=& \frac{1}{n^2} \sum\limits_{k=1}^\infty \frac{a_k^2}{k!}\sum\limits_{j=1, m=1}^n \mathrm{E}[H_k(U_H(j) - U_H(j-1))  H_k(U_H(m) - U_H(m-1)]\\
	&\overset{\text{Proposition \ref{sec:herpro}(ii)}}{=}&  \frac{1}{n^2} \sum\limits_{k=1}^\infty a_k^2\sum\limits_{j=1, m=1}^n (\mathrm{E}[(U_H(j) - U_H(j-1)) (U_H(m) - U_H(m-1))])^k \\
	&=& \frac{1}{n^2} \sum\limits_{k=1}^\infty a_k^2\sum\limits_{j=1, m=1}^n (\mathrm{E}[S_H(j-1) S_H(m-1)])^k\\
	&=& \frac{1}{n^2} \sum\limits_{k=1}^\infty a_k^2\sum\limits_{j=1, m=1}^n (\varsigma_{S_H}(j-m))^k
  \end{eqnarray*}
	And
	\begin{eqnarray*}
	  |\varsigma_{S_H}(k)| &=& |\varsigma_{S_H}(|k|)|\\
	  &=& \mathrm{E}[(U_H(1) - U_H(0))(U_H(|x|+1)-U_H(|x|))]\\
	  &\overset{\text{Cauchy Schwartz}}{\le}& \underbrace{\sqrt{\mathrm{E}[U_H(1)^2]}}_{= 1}\cdot \underbrace{\sqrt{\mathrm{E}[U_H(|x|+1) - U_H(|x|)]^2}}_{= 1}\\
	  &=& 1
	\end{eqnarray*}
	Consequently, $(\varsigma_{S_H}(j-m))^k \le |\varsigma_{S_H}(j-m)|$. In fact,
	\begin{eqnarray*}
	  && \mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n f(U_H(j) - U_H(j-1)) - \mathrm{E}[f(W)]\}^2]\\
	  &\le& \frac{1}{n^2} \underbrace{\sum\limits_{k=1}^\infty a_k^2}_{=:\alpha < \infty}\sum\limits_{j=1, m=1}^n |\varsigma_{S_H}(j-m)|\\
	  &=& \frac{\alpha}{n^2} \sum\limits_{j=1, m=1}^n |\varsigma_{S_H}(j-m)|\\
	  &=& \frac{\alpha}{n^2} 2 \cdot \sum\limits_{j=1}^n  \sum\limits_{m<j} |\varsigma_{S_H}(j-m)|\\
	  &\le& \frac{\alpha}{n^2} 2n \sum\limits_{k=1}^{n-1} |\varsigma_{S_H}(k)|\\
	  &=& \frac{2\alpha}{n}  \sum\limits_{k=0}^{n-1} |\varsigma_{S_H}(k)|,
	\end{eqnarray*}
	As in the proof in Theorem \ref{sec:lmemory}, $\sum\limits_{k=0}^{n-1} |\varsigma_{S_H}(k)| \propto H(2H-1)\sum\limits_{j=1}^{n-1}j^{2H-2} \propto H(2H-1)n\cdot n^{2H-2} \propto n^{2H-1}$ and $\frac{2\alpha}{n} \sum\limits_{k=0}^{n-1} |\varsigma_{S_H}(k)| \propto n^{2H-2}$ as k goes to infinity. This leads  $\mathrm{E}[\{\frac{1}{n}\sum\limits_{j=1}^n f(U_H(j) - U_H(j-1)) - \mathrm{E}[f(W)]\}^2]=0$ due to $n^{2H-2}\rightarrow 0$ for $0<H<1$ as $n \rightarrow \infty$.\\
	Secondly, we apply previous result for (\ref{sec:semilemma}), In fact,
	\begin{eqnarray*}
		&&\sum_{j=1}^n |U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^\beta\\
		&=& \frac{1}{n^{\beta H}} \sum_{j}^n|U_H(j) - U_H(j-1)|^\beta\\
		&=& \frac{1}{n^{\beta H-1}} \frac{1}{n}\sum_{j}^n|U_H(j) - U_H(j-1)|^\beta\\
		&\longrightarrow& n^{1-\beta H}\mathrm{E}[|W|^\beta]
	  \end{eqnarray*}
	  Due to $\mathrm{E}[|W|^\beta] < \infty$, (\ref{sec:semilemma}) holds as well as $n\rightarrow \infty$.
\end{proof}
\begin{theorem}
  FBM is not a semimartingale for $H\neq \frac{1}{2}$.
\end{theorem}
\begin{proof}
  Without loss of generality, we set the Time index $T=[0, 1]$. Choosing $\beta=2$ in (\ref{sec:semilemma}), we suppose $U_H(t)$ were a semimartingale.\\
  Case $H < \frac{1}{2}$. Then $\sum_{j=1}^n |U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^2=\infty$ contradicts that semimartingale has finite quadratic variation.\\
  Case $H > \frac{1}{2}$. $\sum_{j=1}^n |U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^2=0$. On the one hand, according to Doob-Meyer decomposition, $U_H(t) = M(t) + A(t)$, where $M(t)$ is a local martingal and $A(t)$ is local finite variation process. Due to $A(t)$ is local finite, we have $0=<U_H, U_H> = <M, M> $, where $<\cdot,\cdot>$ is denoted for quadratic variation. Consequently, $M(t)$ is zero process due to Cauchy Schwarz inequality. In other words, $U_H(t)=A(t)$ which has finite variation. On the ohter Hand, choosing $1<\gamma<\frac{1}{H}$, then $\sum_{j=1}^n |U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^\gamma \rightarrow \infty$. Precisily, 
  \begin{eqnarray*}
	 &&\infty \leftarrow \sum_{j=1}^n |U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^\gamma\\
	 &\hspace{2em}&\le \underbrace{\sup\limits_{1\le j\le n}|U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|^{\gamma-1}}_{\overset{(\gamma-1)-\text{H\"older}}{\rightarrow}0} \cdot \sum_{j=1}^n|U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|,
  \end{eqnarray*}
  this leads to $\sum_{j=1}^n|U_H(\frac{j}{n}) - U_H(\frac{j-1}{n})|\rightarrow \infty$ contradicts mentioned above that $U_H(t)$ has finite variation.\\
  Given all that, FBM is not a semimartingal for $H\neq \frac{1}{2}$.
\end{proof}

%\subsection{Integration with respect to FBM}
%\begin{definition}
%  Let $(F(t))_{t \in T}$ and $(G(t))_{t \in T}$ be two stochastic processes with continuous path. The \emph{symmetric integral} of $F$ with respecct to $G$ is defined as following
%  \begin{equation*}
%	(\textbf{symmetric})\int_0^T F(u) \mathop{dG(u)} = \frac{1}{2}\lim\limits_{n\rightarrow\infty}\sum_{j=0}^{n-1} (F_{\frac{(j+1)T}{n}} + F_{\frac{jT}{n}}) (G_{\frac{(j+1)T}{n}} - G_{\frac{jT}{n}}).
%  \end{equation*}
%\end{definition}
%\begin{theorem}
%  (Cf.\cite{chenu} and \cite{gradin})Let $U_H$ be a FBM. If the Hurst index $H>\frac{1}{6}$ and $f\in \mathcal{C}^{\infty}(\mathbb{R})$ with polynomial growth for all its derivatives, then $(\textbf{symmetric})\int_0^T f'(U_H(u))\,\mathop{dU_H(u)}$ exist. Moreover ons has
%  \begin{equation}
%	f(U_H(T)) - f(U_H(0)) = (\textbf{symmetric})\int_0^T f'(U_H(u)) \mathop{dU_H(u)}
%	\label{sec:intfbm}
%  \end{equation}
%  for $T \ge 0$.
%\end{theorem}

%\begin{proof}
%  See \cite{nourdin}, p.32, Theorem 3.5 .
% \end{proof}


\newpage
%---------- 4. section ------------%
\section{Fractional Ornstein Uhlenbeck Process Model}
\setcounter{equation}{0}
\newpage

%---------- 5. Anwendung in die finanzmathe bzw. in der volatility process -------%
\section{Application in Financial Mathematics}
\setcounter{equation}{0}
\subsection{Fractional Black-Scholes Model}
\begin{lemma}
  
\end{lemma}

\begin{theorem}
  Let $(S_t)_{t\in[0, T]}$ be a stochastic process such that
  \begin{eqnarray}
	S_t = \exp\brkt{a(t) + \sigma U_H(t)},
	\label{sec:fbs}
  \end{eqnarray}
  where $U_H(t)$ is FBM and $a: [0, T]\rightarrow \mathbb{R}$ a measurable and bounded on its domain. If there exist
  \begin{eqnarray*}
  \xi^1 = f_0\mathbbm{1}_{\{0\}}+\sum_{k=1}^{n-1} f_k \mathbbm{1}_{(\tau_k, \tau_{k+1}]}
  \end{eqnarray*}
  where $\{f_k\}_0^n$ is family of real $\mathcal{F}^{U_H}_k $-measurable function with $\mathcal{F}^{U_H}_k $ Filtration generated of $\{U_H(k)\}$ , $0 < \tau_1 < \cdots <\tau_n \le T$ are stopping times respect to $\mathcal{F}^{U_H}_t $ respectively,  with $\tau_{k+1} - \tau_k\ge m$ and $\mathcal{P}[f_k\neq 0]>0$, then
  \begin{eqnarray*}
	\mathcal{P}[(\xi^1 \cdot S)_T < 0] > 0.
	\label{sec:claim}
  \end{eqnarray*}
\end{theorem}

\begin{proof}
 % We prove the $\mahtcal{\mu^w} [(\xi^1 \codt S)_T < 0] > 0$ to obtain the claim. \\
  Cf.\cite{chridito}, p.18, Theorem 4.3 . We have
    \begin{eqnarray*}	
	  (\xi^1 \cdot S)_T = \sum_{k=1}^{n-1} f_k(e^{U_H(\tau_{k+1})} - e^{U_H(\tau_k)}).
	\end{eqnarray*}
  Assume ${\mathcal{P}}[(\xi^1 \cdot S)_T < 0] = 0$, then there exist 
  $$
  l = \min \{ j: \mathcal{P}[f_j \neq 0] > 0,\hspace{1em} \mathcal{P}[\sum_{k=1}^{j} f_k (e^{U_H(\tau_{k+1})} - e^{U_H(\tau_k)})\ge 0]=1 \}
  $$
  This leads to
  \begin{eqnarray*}
	\mathcal{P} \left[\brkt{\sum_{k=1}^{l-1} f_k (e^{U_H(\tau_{k+1})} - e^{U_H(\tau_k)}} \le 0\right] = 1 
  \end{eqnarray*}
  Ignoring constant term, we define 
  $$
  U_H(t)(\omega) = \int_{\mathbb{R}} \mathbbm{1}_{\{t\ge u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u\le 0\}} (-u)^{H-\frac{1}{2}} \, \mathop{d\omega(u)} .
  $$
  where $\omega(u) := B_u(\omega)$ for all $\omega \in \Omega^w$.
  We give the filtration $(\mathcal{F}_t^{\Omega^w})$ denoting by 
  $$
  \mathcal{F}_t^{\Omega^w} := \sigma(\{\{w \in \Omega^w : \omega(u) \in \mathbb{R}\} : -\infty < u \le t, c\in\mathbb{R}\}).
  $$
  Then $\tau_k$ is also stopping time of $\mathcal{F}_t^{\Omega^w}$ due to
  $$
  \mathcal{F}^{U_H}_t \subset \mathcal{F}_t^{\Omega^w}.	t \in \mathbb{R}
  $$
 For $\omega \in \Omega^w$, we split it into two parts as follows
 \begin{eqnarray*}
   \psi_{\omega}(u) := \omega(u)\mathbbm{1}_{(-\infty, \tau_l(\omega)}, s\in \mathbb{R} \\
   \phi_{\omega}(u) := \omega(\tau_l(\omega) + u) - \omega(\tau_l(\omega)), s\ge 0.
 \end{eqnarray*}
Corresponding to each, we define
\begin{eqnarray*}
  \Omega^1 := \{\psi_{\omega} \in \mathcal{C}(\mathbb{R}) : \omega \in \Omega^w\}\\
  \Omega^2 := \{\phi_{\omega} \in \mathcal{C}([0, \infty)) : \omega \in \Omega^w\}
\end{eqnarray*}
And for the smallst $\sigma$-algebra of all subsets, respectively, of $\Omega^1, \Omega^2$ denoted by $\mathscr{B}^1, \mathscr{B}^2$.
Notice that
\begin{eqnarray*}
  \{\tau_l \le t\} \cap \{\psi_{\omega} \in \Omega^w\} &=& \{{\omega \in \Omega^w : \omega(u) \in \mathbb{R}\} : -\infty < u \le t\}\\&\in& \mathcal{F}^{\Omega^w}_t,
\end{eqnarray*}
therefore is $\psi_{\omega}$ a $\mathcal{F}^{\Omega^w}_{\tau_l}$- measurable mapping. We replace the $\omega$ by $\psi_{\omega}, \phi_{\omega}$ calculating the value process
\begin{eqnarray*}
  \sum_{k=1}^{l-1} f_k (e^{U_H(\tau_{k+1})} - e^{U_H(\tau_{k})}) + f_l(e^{U_H(\tau_l+h)}) (\omega) = J_h(\psi_{\omega}, \phi_{\omega})
\end{eqnarray*}
where $J_h$ defined as
\begin{eqnarray*}
  &&J_h(\psi, \phi) : = \brkt{\sum_{k=1}^{l-1} f_k\brkt{e^{U_H(\tau_{k+1})} - e^{U_H(\tau_{k})}}} (\psi) \\
  &+& f_l\brkt{\exp\{\int_{-\infty}^{\tau_l(\psi)}(\tau_l(\psi)+h+u)^{H-\frac{1}{2}}-(\tau_l(\psi)-u)^{H-\frac{1}{2}}\mathop{d\psi(s)}\}}\\
  &\cdot& f_l \brkt{\exp\{\int_0^{h}(h-u)^{H-\frac{1}{2}}\mathop{d \phi(u)}\} - 1}
\end{eqnarray*}
for $\psi\in\Omega^1, \phi\in\Omega^2$.
\end{proof}
\newpage

%---------- x. conclusion ---------%
\section{Conclusion}

\newpage

%---------- reference -------------%
\addcontentsline{toc}{section}{References}
\fancyhead[LO, RE]{}
\begin{thebibliography}{99}
	\bibitem{bauer} \textsc{Bauer,~H.} (2002). Wahrscheinlichkeitstheorie(5th. durchges. und verb. Aufl.). Berlin: W. de Gruyter
	\bibitem{chenu} \textsc{Cheridito,~P., \& Nualart,~D.} (2005). Stochastic Integral of Divergence Type with Respect to Fractional Brownian Motion with Hurst Parameter $H \in(0, \frac{1}{2})$. Ann. Inst. H. Poincar\'e Probab. Statist. 41(6), pp.1049-1081 
	\bibitem{chridito} \textsc{Cheridito,~P.} (2003). Arbitrage in Fractional Brownian Motion Models. Finance Stoch. 7. no. 4, 533-553
	\bibitem{gradin} \textsc{Gradinaru~M., Nourdin~I., Russo~F., Vallois~P.} (2005). m-Order Integrals and Generalized Ito's Formula; The Case of a Fractional Brownian Motion with Any Hurst Index. Ann. Inst. H. Poincar\'e Probab. Statist. 41(4), pp.781-806 
	\bibitem{loeve} \textsc{L\`oeve,~M.} (1960). Probability Theory (2nd ed.). N.J.: D. Van Nostrand Princeton.
	\bibitem{samorodnitsky} \textsc{Samorodnitsky,~G., \& Taqqu, ~M.} (1994). Stable Non-Gaussian Random Processes: Stochastic Models with Infinite Variance. New York: Chapman \& Hall.
	\bibitem{severini}\textsc{Severini,~T.} (2005). Elements of Distribution Theory. New York, NY: Cambridge University Press.
	\bibitem{mandelbrot} \textsc{Mandelbrot, ~B.B., \& Van Ness, ~J.W.} (1968). Fractional Brownian Motions, Fractional Noises and Applications. SIAM Review 10 (4), 422–437.
	\bibitem{nourdin} \textsc{Nourdin, I.} (2012). Selected Aspects of Fractional Brownian Motion. Milano: Springer Milan.
	\bibitem{shilling} (2012, October 1). Stochastic Processes. Lecture conducted from \textsc{Schilling, R.}, Dresden.
\end{thebibliography}
\newpage
\end{document}


  \begin{equation}
