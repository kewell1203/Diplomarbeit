%        File: Diplomarbeit.tex
%     Created: Fri Jan 02 04:00 PM 2015 C
% Last Change: Mon Jan 19 12:00 AM 2015 C
%      Author: Eduard Zhu
%       Email: Kewell1203@gmail.com

\documentclass[a4paper, twoside, 11pt]{article}

%------------------------------%
\synctex=1
%----------- preamble ---------%
%----------- packages ---------%
\usepackage[body={15cm, 23cm}, top=4.5cm, left=4cm]{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage[perpage, symbol]{footmisc}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{bbm}

%----------- pagestyle setting ----------%
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\renewcommand{\headrulewidth}{.4pt}
\renewcommand{\footrulewidth}{.4pt}
\fancyhead[RO]{\leftmark}
\fancyhead[LE]{\rightmark}
\fancyfoot[LE, RO]{\large \thepage}

%---------- new commands ---------%
\theoremstyle{definition}
\newtheorem{definition}{\scshape Definition}[section]
\newtheorem{theorem}[definition]{\scshape Theorem}
\newtheorem{lemma}[definition]{\scshape Lemma}
\newtheorem{proposition}[definition]{\scshape Proposition}
\newtheorem{corollary}[definition]{\scshape Corollary}
\newtheorem{example}[definition]{\scshape Example}
\renewcommand{\proofname}{\upshape\bfseries Proof.}
\renewcommand{\theequation}{\thesection.\arabic{equation}}


%---------- definitions of math -----------%
% R, N
\def\RR{$\mathcal{R}$}
\def\NN{$\mathcal{N}$}
\def\AA{$\mathscr{A}$\ }
% complement
\newcommand{\compl}[1]{{#1}^{c}}
% sigma algebra
\def\sa{$\sigma$- Algebra\ } 
% prob. space
\def\bs{$(\Omega, \mathscr{A}, \mathcal{P})$\ } 
\def\bsigma{\mathscr{B}\brkt{\mathbb{R}^{n}}}
\newcommand{\sqbr}[1]{\left[ {#1} \right]}
\newcommand{\brkt}[1]{\left({#1} \right)}

  %---------- global variables setting -----%
  \setlength{\parindent}{0em}
  \setlength{\parskip}{1.5ex plus .5ex minus .5ex}
  \renewcommand{\baselinestretch}{1.3}
  \setfnsymbol{wiley}
  \renewenvironment{abstract}{
	\begin{center}
		  \Large
		  \textbf{Abstract}
		  \hspace{2em}
	\end{center}				
  }{}

  %---------- beginning of document --------%
  \begin{document}

  %---------- title page -----------%
  \input{./title.tex}
  \newpage

  %---------- abstract -------------%
  \thispagestyle{empty}
  \begin{abstract}
	blahblah
  \end{abstract}
\newpage

%---------- contents -------------%
\thispagestyle{empty}
\mbox{}
\newpage
\fancyhead[LO, RE]{}
\fancyfoot[LE, RO]{}
\tableofcontents
\newpage
\thispagestyle{empty}
\mbox{}
\newpage

%---------- 1. introduction -------%
\fancyhead[RO]{\leftmark}
\fancyhead[LE]{\rightmark}
\fancyfoot[LE, RO]{\large \thepage}
\setcounter{section}{0}
\setcounter{page}{1}
\section{Introduction}

\newpage

%---------- 2. section ------------%
\section{Gaussian Process and Brownian Motion }
In this section we start off by looking at some general concepts of probability spaces and stochastic processes. Of this, a most important case we then discribe, is Gaussian process. It bring us to introduce the Brownian Motion as a fine example.

%---------- 2.1. subsection -------%
\subsection{Probability Space and Stochastic Process }
\begin{definition}
  Let \AA be a collection of subsets of a set $\Omega$. \AA is then a \emph{$\sigma$- Algebra} on $\Omega$ if it satisfies the following conditions:
  \begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
	\item $\Omega \in $ \AA.
	\item For any set $F \in \mathscr{A}$, its complement $\compl{F} \in$ \AA.
	\item If a serie $\{F_n\}_{n \in \mathbb{N}} \subseteq \mathscr{A}$, then $\cup_{n \in \mathbb{N}}F_n \in $ \AA.
  \end{enumerate}
\end{definition}

\begin{definition}
  A mapping $\mathcal{P}$ is said to be a \emph{probability measure} from $\mathscr{A}$ to $\bsigma$, if $\mathcal{P}\sqbr{\sum_{n=1}^{\infty} F_n} = \sum_{n=1}^{\infty} \mathcal{P}\sqbr{F_n}$ for any $\{F_n\}_{n \in \mathbb{N}}$ disjoint in $\mathscr{A}$ satisfying $\sum_{n=1}^{\infty}F_n \in \mathscr{A}$. 
\end{definition}

\begin{definition}
  A \emph{probability space} is defined as a triple \bs of a set $\Omega$, a \sa \AA  of $\Omega$ and a measure $\mathcal{P}$ from $\mathscr{A}$ to $\bsigma$.
\end{definition}

The $\sigma$- Algebra generated of all open sets on $\mathbb{R}^{n}$ is called the \emph{Borel $\sigma$- Algebra} which we denote as usual by $\mathscr{B}\left(\mathbb{R}^{n}\right)$. Let $\mu$ be a probability measure on $\mathbb{R}^{n}$. Indeed, $\brkt{\mathbb{R}^{n}, \mathscr{B}\brkt{\mathbb{R}^{n}}, \mu}$ is a special case that probability space on $\mathbb{R}^{n}$. A function $f$ mapping from $\brkt{\mathcal{D}, \mathscr{D}, \mu}$ into $\brkt{\mathcal{E}, \mathscr{E}, \nu}$ is \emph{measurable} if its collection of the inverse image of $\mathscr{E}$ is a subset of $\mathscr{D}$. A \emph{random variable} is a $\mathbb{R}^{n}$-valued measurable function on some probability space. Let $\mathcal{P}$ represent a probability measure, recall that in probability theory, for $B \in \bsigma$ we call $\mathcal{P}\sqbr{\left\{X \in B\right\}}$ the \emph{distribution} of $X$. We write also $\mathcal{P}_X \sqbr{\cdot}$ or $\mathcal{P}\sqbr{X}$ for convenience of the notation above.

\begin{definition}
  Let $\brkt{\Omega, \mathscr{A}, \mathcal{P}}$ be a probability space. A $n$-dimensional \emph{stochastic process} $\brkt{X_t}$ is a family of random variable such that $X_t\brkt{\omega} : \Omega \longrightarrow  \mathbb{R}^{n},  \forall t \in T$, where $T$ denotes the set of Index of Time.    
\end{definition}

\begin{definition}
  A stochastic process $\brkt{X_t}_{t \in T}$ is said to be \emph{stationary}, if the joint distribution 
\[
  \mathcal{P}\sqbr{X_{t_1},\dots,X_{t_n}} = \mathcal{P}\sqbr{X_{t_1+\tau},\dots,X_{t_n+\tau}} 
\]
for $t_1, \dots, t_n$ and $t_1+\tau,\dots,t_n+\tau \in T$. 
\label{sec:stn}
\end{definition}

Remark that, definition \ref{sec:stn} means the distribution of a stationary process is independent of a shift of time.

%---------- 2.2. subsection -------%
\subsection{Normal Distribution and  Gaussian Process}
\begin{definition}[1-dimensional normal distribution]
  A $\mathbb{R}$-valued random variable $X$ is said to be \emph{standard normal distributed}, if its distribution can be discribed as
  \[
	\mathcal{P}\sqbr{X \le x} = \int_{-\infty}^{x} (2\pi)^{-\frac{1}{2}}e^{-\frac{u^2}{2}}\,\mathop{du}  
  \]
  for $x \in \mathbb{R}$.
\end{definition}

\begin{definition}
  A $\mathbb{R}$-valued random variable $X$ is said to be \emph{normal distributed} with a \emph{mean} $\mu$ and a \emph{variance} $\sigma^2$, if
\[
  (X-\mu) / \sigma
\]
is standard normal distributed.
\end{definition}

We use a notation $X \sim Y$, which means $X$ and $Y$ have the same distribution. In similar way it is denoted by $X \sim (2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}\mathop{dx} $, if it is standard normal distributed. In order to identifing the behaviour of a normal distributed random variable we recall the characteristic function in probability theory, see\cite{bauer}. 

\begin{proposition}
  Let $X$ be a $\mathbb{R}$-valued standard normal distributed random variable. The characteristic function of $X$
\begin{equation}
  \Psi_X(\xi) := \int_\mathbb{R} e^{ix\xi}\mathcal{P}\sqbr{X \in \mathop{dx}} = e^{-\frac{\xi^2}{2}}
  \label{sec:cht}
\end{equation}
for $\xi \in \mathbb{R}$.
\end{proposition}
\begin{proof}
  According to the definion of characteristic function
  \begin{equation*}
	\Psi_X(\xi) = \int_\mathbb{R} (2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}e^{ix\xi}\,\mathop{dx},
  \end{equation*}
take differentiating both sides of the equation by $\xi$, then
\begin{eqnarray*}
\Psi_X'(\xi) &=& \int_\mathbb{R}(2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}e^{ix\xi}ix\,\mathop{dx}\\
             &=& (-i)\cdot\int_\mathbb{R} (2\pi)^{-\frac{1}{2}}(\frac{d}{dx}e^{-\frac{x^2}{2}})e^{ix\xi}\,\mathop{dx}\\
			 &\overset{part.int.}{=}& -\int_\mathbb{R}(2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}e^{ix\xi}\xi\,\mathop{dx}\\
			 &=& -\xi\Psi_X(\xi).
\end{eqnarray*}
Obviously, 
$\Psi(\xi) = \Psi(0)e^{-\frac{\xi^2}{2}}$ is the solution of the partial differential equation above, and $\Psi(0)$ is equal to $1$.
\end{proof}

In particular, the characteristic function of a normal distributed random variable with a mean $\mu$ and a variance $\sigma^2$, which denoted by $\Psi_{X_{\mu,\sigma^2}}(\xi)$, is $e^{i\mu\xi-\frac{1}{2}(\sigma\xi)^2}$. To achieve this result, we just need to substitute $x$ by $(x-\mu)/\sigma$ in the calculation before. 

\begin{definition}
  Let $X$ be a $\mathbb{R}^{n}$-valued random variable. $X$ is said to be \emph{normal distributed}, if for any $d \in \mathbb{R}^{n}$ such that $d^TX$ is normal distributed on $\mathbb{R}$.
\end{definition}
%Note that, $<d,X>$ is defined as scalar product on $\mathbb{R}^{n}$ that means $\sum_{j=1}^{n}\,d_j\cdot X_i$. 
\begin{proposition}
  Let $X$ be a $\mathbb{R}^{n}$-valued normal distributed. Then there exist $m \in \mathbb{R}^{n}$ and a positive definite symmetric matrix $\Sigma \in \mathbb{R}^{n\times n}$ such that,
  \begin{equation}
	\mathrm{E}\,e^{i\xi^TX} = e^{i\xi^Tm - \frac{1}{2}\xi^T \Sigma \xi}
	\label{sec:mcf}
  \end{equation}
  For $\xi \in \mathbb{R}^{n}$. Furthermore, the density function of $X$ is
\begin{equation}
  (2\pi)^{-\frac{d}{2}}\, (\det\Sigma) ^{-\frac{1}{2}}\,e^{-\frac{1}{2}(x-m)^T\Sigma^{-1}(x-m)}\,\mathop{dx}.
  \label{sec:dsy}
\end{equation}
\end{proposition}

Remark, the equation (\ref{sec:mcf}) can also be as definition of characteristic function of a n-dimensional normal distributed random variable. I.e., any normal distributed random variable can be characterized by form of the equation (\ref{sec:mcf}).

\begin{proof}
  Since $X$ normal distributed on $\mathbb{R}^{n}$, then $\xi^T X$ is normal distributed on $\mathbb{R}$. Due to the proposition 2.8 there is
  \begin{eqnarray*}
	\mathrm{E} e^{i\xi^T X} &=& \mathrm{E} e^{i\cdot 1 \cdot \xi^T X}\\
	                        &=& e^{i\mathrm{E}\sqbr{\xi^T X} -\frac{1}{2}\mathrm{Var}\sqbr{\xi^T X}}\\
							&=& e^{i\xi^T\mathrm{E}\sqbr{X} - \frac{1}{2}\xi^T \mathrm{Var}\sqbr{X} \xi}.
  \end{eqnarray*}
  According to the uniqueness theorem of characteristic function (Satz 23.4 in \cite{bauer}), then we can deduce the density function of the equation (\ref{sec:dsy}).
\end{proof}

A normal distributed normal random variable can be characterized by its mean and variance respectively mean vector and covariance vector because of the characteristic function.

\begin{corollary}
  A linear combination of independent normal distributed random variables has normal distribution.
\end{corollary}

\begin{proof}
  In general case, we suppose $Y_1, \cdots, Y_m$ are independent random variables on $\mathbb{R}^n$, for $c_1, \cdots, c_m \in \mathbb{R}$. Let have a look at the chracteristic function of it,
  \begin{eqnarray*}
	\mathrm{E}e^{i\xi^T\sum_{j=1}^m(c_jX_j)} &\overset{independent}{=}&\prod_{j=1}^{m} \mathrm{E}e^{i\xi^T(c_jX_j)}\\
	&=& \prod_{j=1}^m \exp\brkt{i\xi^T\mathrm{E}[c_jX_j]-\frac{1}{2}\xi^T\mathrm{Var}[c_jX_j]\xi}\\
	&=&  \exp\brkt{i\xi^T\mathrm{E}[\sum_{j=1}^{m}c_jX_j]-\frac{1}{2}\xi^T\mathrm\sum_{j=1}^{m}{Var}[c_jX_j]\xi}\\
	&\overset{independent}{=}&  \exp\brkt{i\xi^T\mathrm{E}[\sum_{j=1}^{m}c_jX_j]-\frac{1}{2}\xi^T\mathrm{Var}[\sum_{j=1}^{m}c_jX_j]\xi},
  \end{eqnarray*}
  which is a form of characteristc function of normal distribution. That means $\sum_{j=1}^m c_jX_j$ is normal distributed. 
\end{proof}

\begin{example}[Bivariate Normal Distribution]
  Suppose $S_1, S_2$ are independent random variables on $\mathbb{R}$ and have standard normal distributions. $\left(
    \begin{array}{c}
      S_1 \\
      S_2
    \end{array}
  \right)$  has standard normal joint distribution since they are independent. We define
  \begin{eqnarray}
	\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)
	&=& 
	\left(
    \begin{array}{l}
	  \sigma_1,\hspace{3em} 0 \\
	  \sigma_2 \rho, \sigma_2(1-\rho^2)^{\frac{1}{2}}
    \end{array}
  \right) \cdot
	\left(
    \begin{array}{c}
      S_1 \\
      S_2
    \end{array}
  \right)  +
  \left(
    \begin{array}{c}
      \mu_1 \\
      \mu_2
    \end{array}
  \right)
  \label{sec:bi},
  \end{eqnarray}
  where $\mu_1, \mu_2, \sigma_1, \sigma_2 \in \mathbb{R}, -1 \le \rho \le 1$. Again, $Y_1, Y_2$ are normal distributed and the joint distribution  $\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)$ is normal. We set $\mathrm{E}[Y_1] = \mu_1, \mathrm{E}[Y_2] = \mu_2 $ for short. Since $S_1, S_2$ are independent,
	\begin{eqnarray*}
	  \mathrm{Var}[Y_1] &=& \mathrm{Var}[\sigma_1 S_1]\\
						 &=& \sigma_1^2 ,\\
						 \mathrm{Var}[Y_2] &=& \mathrm{Var}[\sigma_2\rho S_1] + \mathrm{Var}[\sigma_2 (1-\rho^2)^{\frac{1}{2}} S_2]\\
						 &=& \sigma_2^2 \rho^2 + \sigma_2^2(1 - \rho^2)\\
						 &=& \sigma_2^2,\\
						 \mathrm{Cov}[Y_1, Y_2] &=& \mathrm{E}[(Y_1 - \mathrm{E}[Y_1])(Y_2 - \mathrm{E}[Y_2])]\\
						 &=& \mathrm{E}[Y_1Y_2 - \mu_1Y_2 - \mu_2Y_1 + \mu_1\mu_2]\\
						 &=& \mathrm{E}[(\sigma_1 S_1 + \mu_1)(\sigma_2\rho S_1 + \sigma_2(1-\rho^2)^{\frac{1}{2}}S_2 + \mu_2)] - \mu_1\mu_2\\
						 &=& \sigma_1\sigma_2\underbrace{\mathrm{E}[S_1^2]}_{=1}\rho + \mu_1\sigma_2\rho\underbrace{\mathrm{E}[S_1]}_{=0} +
						 \sigma_1\sigma_2(1-\rho^2)^{\frac{1}{2}}\underbrace{\mathrm{E}[S_1S_2]}_{=\mathrm{E}[S_1]\mathrm{E}[S_2]=0} \\
						 &+& \mu_1\sigma_2(1-\rho^2)^{\frac{1}{2}}\underbrace{\mathrm{E}[S_2]}_{=0} + \sigma_1\underbrace{\mathrm{E}[S_1]}_{=0}\mu_2 + \mu_1\mu_2 - \mu_1\mu_2\\
						 &=& \rho\sigma_1\sigma_2,
	\end{eqnarray*}
	that means the corrlation of $Y_1, Y_2$ is $\rho$.
	Because of the equation (\ref{sec:dsy}), the joint density function
	\begin{eqnarray*}
	  f_{Y_1, Y_2}(y_1, y_2) &=& (2\pi)^{-1} (\det(\Sigma))^{-\frac{1}{2}} \exp\brkt{(y_1 - \mu_1) \Sigma^{-1} (y_2 - \mu_2)},
	\end{eqnarray*}
	where $\Sigma =\left(
    \begin{array}{l}
	  \sigma_1^2, \hspace{3em}0 \\
	  \sigma_2^2\rho^2, \sigma_2^2(1-\rho^2)
    \end{array}
  \right)
 $\\
 Indeed, 
 $$
 	\det(\Sigma) = (1-\rho^2)\sigma_1^2\sigma_2^2
 $$ and 
 $$
 \Sigma^{-1} = \frac{
   \left(
    \begin{array}{l}
	  \sigma_2^2(1-\rho^2), \hspace{1em}0 \\
	  -\sigma_2^2\rho,\hspace{3em}\sigma_1^2
    \end{array}
  \right)}
  {\displaystyle (1-\rho^2)\sigma_1^2\sigma_2^2}.
  $$
 Namely,
 \begin{equation}
   f_{Y_1, Y_2}(y_1, y_2) = \frac{1}{2\pi(1 - \rho^2)^{\frac{1}{2}}\sigma_1\sigma_2}\exp\brkt{-\frac{1}{2(1-\rho^2)}(z_1^2 - 2\rho z_1 z_2 + z_2^2)}
   \label{sec:jdt}
 \end{equation}
 where $z_1 = \frac{y_1-\mu_1}{\sigma_1}, z_2=\frac{y_2-\mu_2}{\sigma_2}$.
\end{example}

\begin{corollary}
  Let $Y_1, Y_2$ be $\mathbb{R}$-valued normal distributed random variables and $\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)$  has a joint normal distribution, then the conditional mean of $Y_2$ given $Y_1$
    $$
	\mathrm{E}[Y_2| Y_1=y_1] = \mathrm{E}[Y_2] + \rho (y_1 - \mathrm{E}[Y_1])\frac{\sigma_2}{\sigma1},
	$$
	and the conditional variance of $Y_2$ given $Y_2$
	$$
		\mathrm{Var}[Y_2| Y_1 = y_1] = \sigma_1^2 (1 - \rho^2).
	$$
	Where $\sigma_1, \sigma_2$ are standard deviations of $Y_1, Y_2$ and $\rho$ is the correlation of $Y_1, Y_2$.
\end{corollary}

\begin{proof}
  Recall the equation (\ref{sec:jdt}), we can specify the joint density function if $\sigma_1, \sigma_2, \rho$ are known. As result of this,
  $\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)$ has a form of the equation (\ref{sec:bi}).
  Suppose $S_1, S_2$ are independent standard normal distributed random variables. Now we have
  \begin{eqnarray*}
	S_1 &\sim& \frac{(Y_1 - \mathrm{E}[Y_1])}{\sigma_1} \\
	Y_2 &\sim& \sigma_2\rho S_1 + \sigma_2(1-\rho^2)^{\frac{1}{2}} S_2 + \mathrm{E}[Y_2],
  \end{eqnarray*}
  more precisely,
  $$
  Y_2 \sim \sigma_2\rho \frac{(Y_1 - \mathrm{E}[Y_1])}{\sigma_1}  + \sigma_2(1-\rho^2)^{\frac{1}{2}} S_2 + \mathrm{E}[Y_2].
  $$
  Take expectation of both sides, 
  \begin{equation*}
	\mathrm{E}[Y_2|Y_1=y_1] = \sigma_2\rho \frac{(y_1 - \mathrm{E}[Y_1])}{\sigma_1} + \mathrm{E}[Y_2].
  \end{equation*}
  Now consider
  \begin{eqnarray*}
	\mathrm{Var}[Y_2|Y_1=y_1] &=&  \mathrm{E}[(Y_2 - \mu_{Y_2|Y_1})^2|Y_1=y_1]\\
							  &=& \int_{-\infty}^{\infty}(y_2 - \mu_{Y_2|Y_1})^2f_{Y_2|Y_1}(y_2, y_1)\,\mathop{dy_2}\\
							  &=& \int_{-\infty}^{\infty}\sqbr{y_2 - \mu_2 - \frac{\rho\sigma_2}{\sigma_1}(y_1-\mu_1)}^2f_{Y_2|Y_1}(y_2, y_1)\,\mathop{dy_2},
  \end{eqnarray*}
  multiply both sides by the density function of $Y_1$ and integral it over by $y_1$, we have
\begin{eqnarray*}
 &\,&\int_{-\infty}^{\infty} \mathrm{Var}[Y_2|Y_1=y_1] f_{Y_1}(y_1) \mathop{dy_1} \\
 &=&\int_{-\infty}^{\infty}\,\int_{-\infty}^{\infty} \sqbr{y_2 - \mu_2 
	- \frac{\rho\sigma_2}{\sigma_1}(y_1-\mu_1)}^2\underbrace{f_{Y_2|Y_1}(y_2, y_1)\,f_{Y_1}(y_1)}_{f_{Y_1, Y_2}(y_1, y_2)} \mathop{dy_2}\,\mathop{dy_1}\\
	&\iff&\\
	&\,&\mathrm{Var}[Y_2|Y_1=y_1] \underbrace{\int_{-\infty}^{\infty}  f_{Y_1}(y_1)}_{1} \mathop{dy_1} \\
	&=& \mathrm{E}\sqbr{(Y_2 - \mu_2) - (\frac{\rho\sigma_2}{\sigma_1})(Y_1 - \mu_1)}^2 
\end{eqnarray*}

% ausmultipizieren
Also
\begin{eqnarray*}
  \mathrm{Var}[Y_2|Y_1=y_1] &=&\underbrace{\mathrm{E}[(Y_2 - \mu_2)^2]}_{\sigma_2^2} - 2\frac{\rho\sigma_2}{\sigma_1}\underbrace{\mathrm{E}[(Y_1 -\mu_1)(Y_2 - \mu_2)]}_{\rho\sigma_1\sigma_2}\\
  &+& \frac{\rho^2\sigma_2^2}{\sigma_1^2}\underbrace{\mathrm{E}[(Y_1-\mu_1)^2]}_{\sigma_1^2}\\
  &=& \sigma_2^2 - 2\rho^2\sigma^2 + \rho^2\sigma_2^2\\
  &=& \sigma_2^2 - \rho^2\sigma_2^2.
\end{eqnarray*} 
\end{proof}

\begin{definition}
  Let $(X_t)_{t \in T}$ be a $\mathbb{R}^{n}$-valued stochastic process. $(X_t)$ is said to be a \emph{Gaussian process} if $X_{t_1},\dots, X_{t_n}$ has a joint normal distribution for any  $t_1 \dots t_n \in T$ and $n \in \mathbb{N}$. 
\end{definition}
The definition immediately shows for every $X_t$ in Gaussian process has a normal distribution. Therefore the prior corollary is applicable to a Gaussian process.


\subsection{Brownian Motion}
The Brownian motion was first introduced by Bachelier in 1900 in his PhD thesis. We now give the common definition of it.
\begin{definition}
Let $(B_t)_{t\ge0}$ be a $\mathbb{R}^{n}$-valued stochastic process. $(B_t)$ is called \emph{Brownian motion} if it satisfies the following conditions:
\begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
  \item $B_0 = 0 $ a.s. .
  \item $(B_{t_1} - B_{t_0}),\dots,(B_{t_n} - B_{t_{n_1}})$ are independet for $0=t_0<t_1<\dots<t_n$ and $n \in \mathbb{N}$.
  \item $B_t - B_s \sim B_{t-s}$, for $0 \le s \le t < \infty$.
  \item $B_t - B_s \sim \mathcal{N}(0, t-s)^{\otimes n}$.
  \item $B_t$ is continuous in $t$ a.s. .
\end{enumerate}
\end{definition}
A usual saying for $(ii)$ and $(iii)$ is the Brownian motion has independent, stationary increments. In (iv), $\mathrm{N}$ represent a random variable which has a normal distribution. $B_t$ is normal distributed due to (ii). It is clear that the increments of Brownian motion is stationary.

\begin{proposition}
  Let $(B_t)$ be a one-dimensional Brownian motion. Then the covarice of $B_m, B_n$ for $m, n \ge 0$ is $m \wedge n $.
\end{proposition}

\begin{proof}
  WLOG, we assume that $m \ge n$, then
  \begin{eqnarray*}
	\mathrm{E}[B_mB_n] &=& \mathrm{E}[(B_m - B_n)B_n] + \mathrm{E}[B_n^2]\\
	&=& \mathrm{E}[B_m - B_n]\mathrm{E}[B_n] + n\\
	&=& n .
  \end{eqnarray*}
  \label{sec:cor}
\end{proof}

\begin{proposition}
  Let $(B_t)$ be a one-dimensional Brownian motion. Then $B_{cm} \sim c^{\frac{1}{2}}B_m$.
\end{proposition}

\begin{proof}
  Because $B_m$ is normal distributed for any $m > 0$, we then get
  \begin{eqnarray*}
	\mathrm{E} [e^{i\xi B_{cm}}] &=& e^{-\frac{1}{2}cm\xi^2}\\
	&=& e^{-\frac{1}{2}(c(m)^{\frac{1}{2}}\xi)^2}\\
	&=& \mathrm{E} [e^{i\xi c^{\frac{1}{2}}B_m}] .
  \end{eqnarray*}
\end{proof}

\begin{theorem}
  A one-dimensional Brownian motion is a Gaussian process.
\end{theorem}

\begin{proof}
  The following idea using the independence of increments to prove the claim come from \cite{shilling}.
  We choose $0=t_0<t_1<\dots<t_n$, for $n \in \mathbb{N}$. Define
  $V = (B_{t_1},\dots,B_{t_n})^T$,  $K = (B_{t_1}-B_{t_0},\dots, B_{t_n}-B_{t_{n-1}})^T$ and 
  $A = 
  \begin{pmatrix}
	1      & 0      & \cdots & 0\\
	1      & 1      & \cdots & 0\\
	\vdots & \vdots & \ddots & \vdots \\
	1      & 1      & \cdots & 1
  \end{pmatrix}
	$.
  Let us see the characteristic function of $V$,
  \begin{eqnarray*}
	\mathrm{E} [e^{i\xi^T V}] &=& \mathrm{E} [e^{i\xi^T AK}]\\ 
	&=& \mathrm{E} [e^{iA^T\xi K}]\\
	&=& \mathrm{E} [\exp(i (\xi^{(1)}+\dots+\xi^{(n)}, \xi^{(2)}+\dots+\xi^{(n)}, \cdots,\xi^{(n)})] \\
	&\cdot& (B_{t_1}-B_{t_0}, B_{t_2}-B_{t_1},\dots,B_{t_n}-B_{t_{n-1}})^T)\\
	&\overset{ind.increments}{=}& \prod_{j=1}^n \mathrm{E} [\exp(i(\xi^{(j)+\dots+\xi^{(n)}})(B_{t_j}-B_{t_{t-1}}))]\\
	&\overset{stat.increments}{=}& \prod_{j=1}^n \exp(-\frac{1}{2}(t_j - t_{j-1})(\xi^{(j)}+\dots+\xi^{(n)})^2) \\
	&=& \exp\left(-\frac{1}{2}\sum_{j=1}^n (t_j - t_{j-1})(\xi^{(j)}+\dots+\xi^{(n)})^2\right)\\
    &=& \exp\left(-\frac{1}{2}\left(\sum_{j=1}^n t_j(\xi^{(j)}+\dots+\xi^{(n)})^2 - \sum_{j=1}^n t_{j-1}(\xi^{(j)}+\dots+\xi^{(n)})^2\right)\right)\\
	&=& \exp\left(-\frac{1}{2}\left(\sum_{j=1}^{n-1} t_j((\xi^{(j)}+\dots+\xi^{(n)})^2 - (\xi^{(j+1)}+\dots+\xi^{(n)})^2) + t_n(\xi^{(n)})^2\right)\right)\\
	&=& \exp\left(-\frac{1}{2}\left(\sum_{j=1}^{n-1} t_j\xi^{(j)}(\xi^{(j)}+2\xi^{(j+1)}+\dots+2\xi^{(n)}) + t_n(\xi^{(n)})^2\right)\right)\\
	&=& \exp\left(-\frac{1}{2}\left(\sum_{j,h=1}^n(t_j\wedge t_h)\xi^{(j)}\xi^{(h)}\right)\right).
  \end{eqnarray*}
  Recall with proposition \ref{sec:cor}, $(t_j\wedge t_h)_{t,h=1,\dots,n}$ is the covariance matrix of $V$. The mean vector of it is zero, then we have been proved that the characteristic function is a form of some normal distributed random vector, i.e., $V$ is normal distributed.
\end{proof}

Shilling gave in his lecture \cite{shilling} the relationship between a one-dimensional Brownian motion and a n-dimensional Brownian motion.
$(B_t^{(l)})_{l=1,\dots,n}$ is Brownian motion if and only if $B_t^{(l)}$ is Brownian motion and all of the component are independent. Using this independence and te theorem of fubini in the characteristic function for high-dimensional Brownian motion we can say a n-dimensional Brownian motion is also a Gaussian process.

\newpage

%-------- 3. section
\section{Regularity for Brownian Motion and It\'o Integral}
\subsection{Theorem of Kolmogorov Chentsov and L\'evy Modulus of Continuity}
We consider now the one-dimensional Brownian motion. In this section we need some notations, which are defined as followings
\begin{equation*}
\Delta^{[0,T]} = \left\{t_1,\dots,t_n|0=t_0<\dots<t_n=T\right\} 
\end{equation*}
$$
|\Delta^{[0,T]}| = \max_{t_j \in \Delta^{[0,T]}}|t_j - t_{j-1}|
$$.

\begin{lemma}
  Let $B_t$ be a Brownian motion. Then
  $$
  \sum_{t_j \in \Delta^{[0,T]}} |B_{t_j} - B_{t_{j-1}}|^2 \xmapsto[L^2(\mathcal{P})]{|\Delta^{[0,T]}|\rightarrow 0} T  
  $$
\end{lemma}

\newpage
%---------- 3. section ------------%
\section{Fractional Brownian Motion}
\setcounter{equation}{0}
The fractional Brownian motion(FBM) was defined by Kolmogorov primitively. After that Mandelbrot and Van Ness has present the work in detail. This section is concerned with the xxxxx of it.

\subsection{Definition of fractional Brownian motion}
Mandelbrot and Van Ness gave a integration presentation of the definion of FBM.
\begin{definition}
  Let $(U_t)_t$ be a $\mathbb{R}$-valued stochatstic process an $H$ be such that $1<H<0$. $(U_t)$ is said to be \emph{fractional Brownian motion} if 
  \begin{eqnarray}
	U_t - U_s &=& \int_{\mathbb{R}} \mathbbm{1}_{\{t>u\}}\, (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u<0\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}\nonumber\\
			&-& \int_{\mathbb{R}} \mathbbm{1}_{\{s>u\}}\, (s-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u<0\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}
	\label{<++>}
  \end{eqnarray}<++>
\end{definition}<++>

\newpage

%---------- 4. section ------------%
\section{Fractional Ornstein Uhlenbeck Process Model}
\setcounter{equation}{0}

\newpage

%---------- 5. Anwendung in die finanzmathe bzw. in der volatility process -------%
\section{Application in Financial Mathematics}

\newpage

%---------- x. conclusion ---------%
\section{Conclusion}

\newpage

%---------- reference -------------%
\addcontentsline{toc}{section}{References}
\fancyhead[LO, RE]{}
\begin{thebibliography}{99}
	\bibitem{bauer} \textsc{Bauer,~H.} (2002). Wahrscheinlichkeitstheorie(5th. durchges. und verb. Aufl.). Berlin: W. de Gruyter
	\bibitem{shilling} (2012, October 1). Stochastic Processes. Lecture conducted from \textsc{Shilling, R.}, Dresden.

\end{thebibliography}
\newpage
\end{document}


