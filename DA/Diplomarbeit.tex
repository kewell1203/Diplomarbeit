%        File: Diplomarbeit.tex
%     Created: Fri Jan 02 04:00 PM 2015 C
% Last Change: Thu Jan 29 12:00 AM 2015 C
%      Author: Eduard Zhu
%       Email: Kewell1203@gmail.com

\documentclass[a4paper, twoside, 11pt]{article}

%------------------------------%
\synctex=1
%----------- preamble ---------%
%----------- packages ---------%
\usepackage[body={15cm, 23cm}, top=4.5cm, left=4cm]{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage[perpage, symbol]{footmisc}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{bbm}

%----------- pagestyle setting ----------%
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\renewcommand{\headrulewidth}{.4pt}
\renewcommand{\footrulewidth}{.4pt}
\fancyhead[RO]{\leftmark}
\fancyhead[LE]{\rightmark}
\fancyfoot[LE, RO]{\large \thepage}

%---------- new commands ---------%
\theoremstyle{definition}
\newtheorem{definition}{\scshape Definition}[section]
\newtheorem{theorem}[definition]{\scshape Theorem}
\newtheorem{lemma}[definition]{\scshape Lemma}
\newtheorem{proposition}[definition]{\scshape Proposition}
\newtheorem{corollary}[definition]{\scshape Corollary}
\newtheorem{example}[definition]{\scshape Example}
\renewcommand{\proofname}{\upshape\bfseries Proof.}
\renewcommand{\theequation}{\thesection.\arabic{equation}}


%---------- definitions of math -----------%
% R, N
\def\RR{$\mathcal{R}$}
\def\NN{$\mathcal{N}$}
\def\AA{$\mathscr{A}$\ }
% complement
\newcommand{\compl}[1]{{#1}^{c}}
% sigma algebra
\def\sa{$\sigma$- Algebra\ } 
% prob. space
\def\bs{$(\Omega, \mathscr{A}, \mathcal{P})$\ } 
\def\bsigma{\mathscr{B}\brkt{\mathbb{R}^{n}}}
\newcommand{\sqbr}[1]{\left[ {#1} \right]}
\newcommand{\brkt}[1]{\left({#1} \right)}

  %---------- global variables setting -----%
  \setlength{\parindent}{0em}
  \setlength{\parskip}{1.5ex plus .5ex minus .5ex}
  \renewcommand{\baselinestretch}{1.3}
  \setfnsymbol{wiley}
  \renewenvironment{abstract}{
	\begin{center}
		  \Large
		  \textbf{Abstract}
		  \hspace{2em}
	\end{center}				
  }{}

  %---------- beginning of document --------%
  \begin{document}

  %---------- title page -----------%
  \input{./title.tex}
  \newpage

  %---------- abstract -------------%
  \thispagestyle{empty}
  \begin{abstract}
	blahblah
  \end{abstract}
\newpage

%---------- contents -------------%
\thispagestyle{empty}
\mbox{}
\newpage
\fancyhead[LO, RE]{}
\fancyfoot[LE, RO]{}
\tableofcontents
\newpage
\thispagestyle{empty}
\mbox{}
\newpage

%---------- 1. introduction -------%
\fancyhead[RO]{\leftmark}
\fancyhead[LE]{\rightmark}
\fancyfoot[LE, RO]{\large \thepage}
\setcounter{section}{0}
\setcounter{page}{1}
\section{Introduction}

\newpage

%---------- 2. section ------------%
\section{Gaussian Process and Brownian Motion}
In this section we start off by looking at some general concepts of probability spaces and stochastic processes. Of this, a most important case we then discribe is Gaussian process. Within the framework of Gaussian process, one could spcefic, respectively, a stationary or independent behaviour of increments of it. This lead us to introduce the Brownian motion as a fine example.

%---------- 2.1. subsection -------%
\subsection{Probability Space and Stochastic Process }
\begin{definition}
  Let \AA be a collection of subsets of a set $\Omega$. \AA is said to be a \emph{$\sigma$- Algebra} on $\Omega$, if it satisfies the following conditions:
  \begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
	\item $\Omega \in $ \AA.
	\item For any set $F \in \mathscr{A}$, its complement $\compl{F} \in$ \AA.
	\item If a serie $\{F_n\}_{n \in \mathbb{N}} \subseteq \mathscr{A}$, then $\cup_{n \in \mathbb{N}}F_n \in $ \AA.
  \end{enumerate}
\end{definition}

\begin{definition}
  A mapping $\mathcal{P}$ is said to be a \emph{probability measure} from $\mathscr{A}$ to $\bsigma$, if $\mathcal{P}\sqbr{\sum_{n=1}^{\infty} F_n} = \sum_{n=1}^{\infty} \mathcal{P}\sqbr{F_n}$ for any $\{F_n\}_{n \in \mathbb{N}}$ disjoint in $\mathscr{A}$ satisfying $\sum_{n=1}^{\infty}F_n \in \mathscr{A}$. 
\end{definition}

\begin{definition}
  A \emph{probability space} is defined as a triple \bs of a set $\Omega$, a \sa \AA  of $\Omega$ and a measure $\mathcal{P}$ from $\mathscr{A}$ to $\bsigma$.
\end{definition}

The $\sigma$- Algebra generated of all open sets on $\mathbb{R}^{n}$ is called the \emph{Borel $\sigma$- Algebra} which we denote as usual by $\mathscr{B}\left(\mathbb{R}^{n}\right)$. Let $\mu$ be a probability measure on $\mathbb{R}^{n}$. Indeed, $\brkt{\mathbb{R}^{n}, \mathscr{B}\brkt{\mathbb{R}^{n}}, \mu}$ is a special case that probability space on $\mathbb{R}^{n}$. A function $f$ mapping from $\brkt{\mathcal{D}, \mathscr{D}, \mu}$ into $\brkt{\mathcal{E}, \mathscr{E}, \nu}$ is \emph{measurable}, if its collection of the inverse image of $\mathscr{E}$ is a subset of $\mathscr{D}$. A \emph{random variable} is a $\mathbb{R}^{n}$-valued measurable function on some probability space. Let $\mathcal{P}$ represent a probability measure, recall that in probability theory, for $B \in \bsigma$ we call $\mathcal{P}\sqbr{\left\{X \in B\right\}}$ the \emph{distribution} of $X$. We write also $\mathcal{P}_X \sqbr{\cdot}$ or $\mathcal{P}\sqbr{X}$ for convenience for those notations .

\begin{definition}
  Let $\brkt{\Omega, \mathscr{A}, \mathcal{P}}$ be a probability space. A $n$-dimensional \emph{stochastic process} $\brkt{X_t}_{t\in T}$ is a family of random variable such that $X_t\brkt{\omega} : \Omega \longrightarrow  \mathbb{R}^{n},  \forall t \in T$, where $T$ denotes the set of Index of Time.    
\end{definition}

Some basically definitions, which are needed in following sections, are given.
\begin{definition}
  A stochastic process $\brkt{X_t}_{t \in T}$ is said to be \emph{stationary}, if the joint distribution 
\[
  \mathcal{P}\sqbr{X_{t_1},\dots,X_{t_n}} = \mathcal{P}\sqbr{X_{t_1+\tau},\dots,X_{t_n+\tau}} 
\]
for $t_1, \dots, t_n$ and $t_1+\tau,\dots,t_n+\tau \in T$. 
\label{sec:stn}
\end{definition}

Remark that, Definition \ref{sec:stn} means the distribution of a stationary process is independent of a shift of time.

\begin{definition}
  A stochastic process $(X_t)_t$ is said to be \emph{ergodic} if the moving average of $X_t$ over $T$ tend to infinity, in fact,
  \begin{equation}
	\frac{1}{T}\int_0^T\,X_t\,\mathop{dt} \longrightarrow \infty\nonumber\\.
  \end{equation}
	\label{sec:erg}
\end{definition}

\begin{definition}
  A stochastic process $(X_t)_t$ is said to have \emph{long memory} if there exist  $\alpha \in (0, \frac{1}{2})$ such that
  \begin{equation}
	\tau ^{1 - 2\alpha} \varsigma_X(\tau) \overset{\tau\rightarrow \infty}{\longrightarrow} \infty
	\label{sec:lm1}
  \end{equation}
   or 
  \begin{equation}
	|\lambda|^{2\alpha}g_X(\lambda) \overset{\lambda\rightarrow 0}{\longrightarrow} 0 .
	\label{sec:lm2}
  \end{equation}
 
\end{definition}

%---------- 2.2. subsection -------%
\subsection{Normal Distribution and  Gaussian Process}
\begin{definition}[1-dimensional normal distribution]
  A $\mathbb{R}$-valued random variable $X$ is said to be \emph{standard normal distributed} or \emph{standard Gaussian}, if its distribution can be discribed as
  \begin{equation}
	\mathcal{P}\sqbr{X \le x} = \int_{-\infty}^{x} (2\pi)^{-\frac{1}{2}}e^{-\frac{u^2}{2}}\,\mathop{du}  
	\label{sec:21}
  \end{equation}
  for $x \in \mathbb{R}$.
\end{definition}
The integrand of (\ref{sec:21}) is also called \emph{density function} of a Gaussian random variable.

\begin{definition}
  A $\mathbb{R}$-valued random variable $X$ is said to be \emph{normal distributed} or \emph{Gaussian} with a \emph{expected value} $\mu$ and a \emph{variance} $\sigma^2$, if
\[
  (X-\mu) / \sigma
\]
is standard Gaussian.
\end{definition}

We use a notation $X \sim Y$ represents $X$ equals $Y$ \emph{in distribution}. In similar way it is denoted by $X \sim (2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}\mathop{dx} $, if $X$ is standard Gaussian. In order to veifying the behaviour of a normal distributed random variable we recall the characteristic function in probability theory, see\cite{bauer}. 

\begin{proposition}
  Let $X$ be a $\mathbb{R}$-valued standard normal distributed random variable. The characteristic function of $X$
\begin{equation}
  \Psi_X(\xi) := \int_\mathbb{R} e^{ix\xi}\mathcal{P}\sqbr{X \in \mathop{dx}} = e^{-\frac{\xi^2}{2}}
  \label{sec:cht}
\end{equation}
for $\xi \in \mathbb{R}$.
\end{proposition}
\begin{proof}
  According to the definion of characteristic function we replace probability measure by (\ref{sec:21})
  \begin{equation*}
	\Psi_X(\xi) = \int_\mathbb{R} (2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}e^{ix\xi}\,\mathop{dx},
  \end{equation*}
take differentiating both sides of the equation by $\xi$, then
\begin{eqnarray*}
\Psi_X'(\xi) &=& \int_\mathbb{R}(2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}e^{ix\xi}ix\,\mathop{dx}\\
             &=& (-i)\cdot\int_\mathbb{R} (2\pi)^{-\frac{1}{2}}(\frac{d}{dx}e^{-\frac{x^2}{2}})e^{ix\xi}\,\mathop{dx}\\
			 &\overset{part.int.}{=}& -\int_\mathbb{R}(2\pi)^{-\frac{1}{2}}e^{-\frac{x^2}{2}}e^{ix\xi}\xi\,\mathop{dx}\\
			 &=& -\xi\Psi_X(\xi).
\end{eqnarray*}
Obviously, 
$\Psi(\xi) = \Psi(0)e^{-\frac{\xi^2}{2}}$ is the solution of the partial differential equation, and $\Psi(0)$ equals $1$.
\end{proof}

In particular, the characteristic function of a normal distributed random variable with a expected value $\mu$ and a variance $\sigma^2$, which denoted by $\Psi_{X_{\mu,\sigma^2}}(\xi)$, is $e^{i\mu\xi-\frac{1}{2}(\sigma\xi)^2}$. To achieve this result, we just need to substitute $x$ by $(x-\mu)/\sigma$ in the previous calculation. 

\begin{definition}
  Let $X$ be a $\mathbb{R}^{n}$-valued random vector. $X$ is said to be \emph{normal distributed} or \emph{Gaussian}, if for any $d \in \mathbb{R}^{n}$ such that $d^TX$ is Gaussian on $\mathbb{R}$.
\end{definition}
%Note that, $<d,X>$ is defined as scalar product on $\mathbb{R}^{n}$ that means $\sum_{j=1}^{n}\,d_j\cdot X_i$. 
\begin{proposition}
  Let $X$ be a $\mathbb{R}^{n}$-valued normal distributed ramdon vector. Then there exist $m \in \mathbb{R}^{n}$ and a positive definite symmetric matrix $\Sigma \in \mathbb{R}^{n\times n}$ such that,
  \begin{equation}
	\mathrm{E}\,e^{i\xi^TX} = e^{i\xi^Tm - \frac{1}{2}\xi^T \Sigma \xi}
	\label{sec:mcf}
  \end{equation}
  For $\xi \in \mathbb{R}^{n}$. Furthermore, the density function of $X$ is
\begin{equation}
  (2\pi)^{-\frac{d}{2}}\, (\det\Sigma) ^{-\frac{1}{2}}\,e^{-\frac{1}{2}(x-m)^T\Sigma^{-1}(x-m)}\,\mathop{dx}.
  \label{sec:dsy}
\end{equation}
\end{proposition}

Remark, the equation (\ref{sec:mcf}) can also be as definition of characteristic function of a n-dimensional normal distributed random variable. I.e., any normal distributed random variable can be characterized by form of the equation (\ref{sec:mcf}).

\begin{proof}
  Since $X$ normal distributed on $\mathbb{R}^{n}$, then $\xi^T X$ is normal distributed on $\mathbb{R}$. Due to the Proposition 2.8 there is
  \begin{eqnarray*}
	\mathrm{E} e^{i\xi^T X} &=& \mathrm{E} e^{i\cdot 1 \cdot \xi^T X}\\
	                        &=& e^{i\mathrm{E}\sqbr{\xi^T X} -\frac{1}{2}\mathrm{Var}\sqbr{\xi^T X}}\\
							&=& e^{i\xi^T\mathrm{E}\sqbr{X} - \frac{1}{2}\xi^T \mathrm{Var}\sqbr{X} \xi}.
  \end{eqnarray*}
  According to the uniqueness theorem of characteristic function (Satz 23.4 in \cite{bauer}), then we can deduce the density function of the equation (\ref{sec:dsy}).
\end{proof}

\begin{corollary}
  Let $(X_t)_t$ be a stochastic process. If 
  \begin{equation}
	\sum_j^n c_{t_j} X_{t_j}
  \end{equation}
  is Gaussian for any $n \in \mathbb{N}, c_{t_j} \in \mathbb{R}$, then $(X_t)_t$ has joint normal distribution.
\end{corollary}
\begin{proof}
  
\end{proof}


A normal distributed normal random variable can be characterized by its expected value and variance respectively mean vector and covariance vector because of the characteristic function.

\begin{theorem}
  Let $X$ be a $\mathbb{R}^{n}$-valued normal distributed random vector with independent, normal distributed components. Then $X$ has a joint normal distribution.
\end{theorem}

\begin{proof}
  
\end{proof}

%\begin{corollary}
%  Let $X$ be a $\mathbb{R}^{n}$-valued normal distributed random vector with normal distributed components. If the covariance matrix of $X$ is positive definit and symmtric, then $X$ has a joint normal distribution.
%  \label{sec:mulnor}
%\end{corollary}


\begin{corollary}
  A linear combination of independent normal distributed random variables or random vector has normal distribution respectively.
\end{corollary}

\begin{proof}
  We suppose $X_1, \cdots, X_m$ are independent random vectors  on $\mathbb{R}^n$ and $c_1, \cdots, c_m \in \mathbb{R}$. Let have a look at the chracteristic function of it,
  \begin{eqnarray*}
	\mathrm{E}e^{i\xi^T\sum_{j=1}^m(c_jX_j)} &\overset{independent}{=}&\prod_{j=1}^{m} \mathrm{E}e^{i\xi^T(c_jX_j)}\\
	&=& \prod_{j=1}^m \exp\brkt{i\xi^T\mathrm{E}[c_jX_j]-\frac{1}{2}\xi^T\mathrm{Var}[c_jX_j]\xi}\\
	&=&  \exp\brkt{i\xi^T\mathrm{E}[\sum_{j=1}^{m}c_jX_j]-\frac{1}{2}\xi^T\mathrm\sum_{j=1}^{m}{Var}[c_jX_j]\xi}\\
	&\overset{independent}{=}&  \exp\brkt{i\xi^T\mathrm{E}[\sum_{j=1}^{m}c_jX_j]-\frac{1}{2}\xi^T\mathrm{Var}[\sum_{j=1}^{m}c_jX_j]\xi},
  \end{eqnarray*}
  which is a form of characteristc function of normal distribution. That means $\sum_{j=1}^m c_jX_j$ is Gaussian. 
\end{proof}

\begin{example}[Bivariate Normal Distribution]
  Suppose $S_1, S_2$ are independent random variables and have standard normal distributions. $\left(
    \begin{array}{c}
      S_1 \\
      S_2
    \end{array}
  \right)$  has standard normal joint distribution since they are independent. We define
  \begin{eqnarray}
	\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)
	&=& 
	\left(
    \begin{array}{l}
	  \sigma_1,\hspace{3em} 0 \\
	  \sigma_2 \rho, \sigma_2(1-\rho^2)^{\frac{1}{2}}
    \end{array}
  \right) \cdot
	\left(
    \begin{array}{c}
      S_1 \\
      S_2
    \end{array}
  \right)  +
  \left(
    \begin{array}{c}
      \mu_1 \\
      \mu_2
    \end{array}
  \right)
  \label{sec:bi},
  \end{eqnarray}
  where $\mu_1, \mu_2, \sigma_1, \sigma_2 \in \mathbb{R}, -1 \le \rho \le 1$. Again, $Y_1, Y_2$ are Gaussian and the joint distribution  $\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)$ is also Gaussian. We set $\mathrm{E}[Y_1] = \mu_1, \mathrm{E}[Y_2] = \mu_2 $ for short. Since $S_1, S_2$ are independent,
	\begin{eqnarray*}
	  \mathrm{Var}[Y_1] &=& \mathrm{Var}[\sigma_1 S_1]\\
						 &=& \sigma_1^2 ,\\
						 \mathrm{Var}[Y_2] &=& \mathrm{Var}[\sigma_2\rho S_1] + \mathrm{Var}[\sigma_2 (1-\rho^2)^{\frac{1}{2}} S_2]\\
						 &=& \sigma_2^2 \rho^2 + \sigma_2^2(1 - \rho^2)\\
						 &=& \sigma_2^2,\\
						 \mathrm{Cov}[Y_1, Y_2] &=& \mathrm{E}[(Y_1 - \mathrm{E}[Y_1])(Y_2 - \mathrm{E}[Y_2])]\\
						 &=& \mathrm{E}[Y_1Y_2 - \mu_1Y_2 - \mu_2Y_1 + \mu_1\mu_2]\\
						 &=& \mathrm{E}[(\sigma_1 S_1 + \mu_1)(\sigma_2\rho S_1 + \sigma_2(1-\rho^2)^{\frac{1}{2}}S_2 + \mu_2)] - \mu_1\mu_2\\
						 &=& \sigma_1\sigma_2\underbrace{\mathrm{E}[S_1^2]}_{=1}\rho + \mu_1\sigma_2\rho\underbrace{\mathrm{E}[S_1]}_{=0} +
						 \sigma_1\sigma_2(1-\rho^2)^{\frac{1}{2}}\underbrace{\mathrm{E}[S_1S_2]}_{=\mathrm{E}[S_1]\mathrm{E}[S_2]=0} \\
						 &+& \mu_1\sigma_2(1-\rho^2)^{\frac{1}{2}}\underbrace{\mathrm{E}[S_2]}_{=0} + \sigma_1\underbrace{\mathrm{E}[S_1]}_{=0}\mu_2 + \mu_1\mu_2 - \mu_1\mu_2\\
						 &=& \rho\sigma_1\sigma_2,
	\end{eqnarray*}
	that means the corrlation of $Y_1, Y_2$ is $\rho$.
	Because of the equation (\ref{sec:dsy}), the joint density function
	\begin{eqnarray*}
	  f_{Y_1, Y_2}(y_1, y_2) &=& (2\pi)^{-1} (\det(\Sigma))^{-\frac{1}{2}} \exp\brkt{(y_1 - \mu_1) \Sigma^{-1} (y_2 - \mu_2)},
	\end{eqnarray*}
	where $\Sigma =\left(
    \begin{array}{l}
	  \sigma_1^2, \hspace{3em}0 \\
	  \sigma_2^2\rho^2, \sigma_2^2(1-\rho^2)
    \end{array}
  \right)
 $\\
 Indeed, 
 $$
 	\det(\Sigma) = (1-\rho^2)\sigma_1^2\sigma_2^2
 $$ and 
 $$
 \Sigma^{-1} = \frac{
   \left(
    \begin{array}{l}
	  \sigma_2^2(1-\rho^2), \hspace{1em}0 \\
	  -\sigma_2^2\rho,\hspace{3em}\sigma_1^2
    \end{array}
  \right)}
  {\displaystyle (1-\rho^2)\sigma_1^2\sigma_2^2}.
  $$
 Namely,
 \begin{equation}
   f_{Y_1, Y_2}(y_1, y_2) = \frac{1}{2\pi(1 - \rho^2)^{\frac{1}{2}}\sigma_1\sigma_2}\exp\brkt{-\frac{1}{2(1-\rho^2)}(z_1^2 - 2\rho z_1 z_2 + z_2^2)}
   \label{sec:jdt}
 \end{equation}
 where $z_1 = \frac{y_1-\mu_1}{\sigma_1}, z_2=\frac{y_2-\mu_2}{\sigma_2}$.
\end{example}

\begin{corollary}
  Let $Y_1, Y_2$ be $\mathbb{R}$-valued normal distributed random variables and $\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)$  has a joint normal distribution, then the conditional expected value of $Y_2$ given $Y_1$
    $$
	\mathrm{E}[Y_2| Y_1=y_1] = \mathrm{E}[Y_2] + \rho (y_1 - \mathrm{E}[Y_1])\frac{\sigma_2}{\sigma1},
	$$
	and the conditional variance of $Y_2$ given $Y_2$
	$$
		\mathrm{Var}[Y_2| Y_1 = y_1] = \sigma_1^2 (1 - \rho^2).
	$$
	Where $\sigma_1, \sigma_2$ are standard deviations of $Y_1, Y_2$ and $\rho$ is the correlation of $Y_1, Y_2$.
\end{corollary}

\begin{proof}
  Recall the equation (\ref{sec:jdt}), we can specify the joint density function if $\sigma_1, \sigma_2, \rho$ are known. As result of this,
  $\left(
    \begin{array}{c}
      Y_1 \\
      Y_2
    \end{array}
	\right)$ has a form of the equation (\ref{sec:bi}).
  Suppose $S_1, S_2$ are independent standard normal distributed random variables. Now we have
  \begin{eqnarray*}
	S_1 &\sim& \frac{(Y_1 - \mathrm{E}[Y_1])}{\sigma_1} \\
	Y_2 &\sim& \sigma_2\rho S_1 + \sigma_2(1-\rho^2)^{\frac{1}{2}} S_2 + \mathrm{E}[Y_2],
  \end{eqnarray*}
  more precisely,
  $$
  Y_2 \sim \sigma_2\rho \frac{(Y_1 - \mathrm{E}[Y_1])}{\sigma_1}  + \sigma_2(1-\rho^2)^{\frac{1}{2}} S_2 + \mathrm{E}[Y_2].
  $$
  Take expectation of both sides, 
  \begin{equation*}
	\mathrm{E}[Y_2|Y_1=y_1] = \sigma_2\rho \frac{(y_1 - \mathrm{E}[Y_1])}{\sigma_1} + \mathrm{E}[Y_2].
  \end{equation*}
  Now consider
  \begin{eqnarray*}
	\mathrm{Var}[Y_2|Y_1=y_1] &=&  \mathrm{E}[(Y_2 - \mu_{Y_2|Y_1})^2|Y_1=y_1]\\
							  &=& \int_{-\infty}^{\infty}(y_2 - \mu_{Y_2|Y_1})^2f_{Y_2|Y_1}(y_2, y_1)\,\mathop{dy_2}\\
							  &=& \int_{-\infty}^{\infty}\sqbr{y_2 - \mu_2 - \frac{\rho\sigma_2}{\sigma_1}(y_1-\mu_1)}^2f_{Y_2|Y_1}(y_2, y_1)\,\mathop{dy_2},
  \end{eqnarray*}
 After multiplying both sides by the density function of $Y_1$ and integrating it by $y_1$, we have
\begin{eqnarray*}
 &\,&\int_{-\infty}^{\infty} \mathrm{Var}[Y_2|Y_1=y_1] f_{Y_1}(y_1) \mathop{dy_1} \\
 &=&\int_{-\infty}^{\infty}\,\int_{-\infty}^{\infty} \sqbr{y_2 - \mu_2 
	- \frac{\rho\sigma_2}{\sigma_1}(y_1-\mu_1)}^2\underbrace{f_{Y_2|Y_1}(y_2, y_1)\,f_{Y_1}(y_1)}_{f_{Y_1, Y_2}(y_1, y_2)} \mathop{dy_2}\,\mathop{dy_1}\\
	&\iff&\\
	&\,&\mathrm{Var}[Y_2|Y_1=y_1] \underbrace{\int_{-\infty}^{\infty}  f_{Y_1}(y_1)}_{1} \mathop{dy_1} \\
	&=& \mathrm{E}\sqbr{(Y_2 - \mu_2) - (\frac{\rho\sigma_2}{\sigma_1})(Y_1 - \mu_1)}^2 
\end{eqnarray*}

multiplying right side out, we see
\begin{eqnarray*}
  \mathrm{Var}[Y_2|Y_1=y_1] &=&\underbrace{\mathrm{E}[(Y_2 - \mu_2)^2]}_{\sigma_2^2} - 2\frac{\rho\sigma_2}{\sigma_1}\underbrace{\mathrm{E}[(Y_1 -\mu_1)(Y_2 - \mu_2)]}_{\rho\sigma_1\sigma_2}\\
  &+& \frac{\rho^2\sigma_2^2}{\sigma_1^2}\underbrace{\mathrm{E}[(Y_1-\mu_1)^2]}_{\sigma_1^2}\\
  &=& \sigma_2^2 - 2\rho^2\sigma^2 + \rho^2\sigma_2^2\\
  &=& \sigma_2^2 - \rho^2\sigma_2^2.
\end{eqnarray*} 
\end{proof}

\begin{theorem}
  Let $X$ be a Gaussian random variable, then
  \begin{equation}
	\mathrm{E}[\exp(\beta X)] = \exp(\beta\mathrm{E}[X] + \frac{1}{2}\beta^2\mathrm{Var}[X]).
	\label{sec:expgau}
  \end{equation}
\end{theorem}

\begin{proof}
  
\end{proof}

\begin{definition}
  Let $(X_t)_{t \in T}$ be a $\mathbb{R}^{n}$-valued stochastic process. $(X_t)$ is said to be a \emph{Gaussian process} if $X_{t_1},\dots, X_{t_n}$ has a joint normal distribution for any  $t_1 \dots t_n \in T$ and $n \in \mathbb{N}$. 
\end{definition}
The definition immediately shows for every $X_t$ in Gaussian process has a normal distribution. Therefore, the previous Corollary is applicable to  Gaussian processes.


\subsection{Brownian Motion}
The Brownian motion was first introduced by Bachelier in 1900 in his PhD thesis. We now give the common definition of it.
\begin{definition}
Let $(B_t)_{t\ge0}$ be a $\mathbb{R}^{n}$-valued stochastic process. $(B_t)$ is called \emph{Brownian motion} if it satisfies the following conditions:
\begin{enumerate}[topsep=0pt, itemsep=-1ex, partopsep=1ex, parsep=1ex, label=(\roman*)]
  \item $B_0 = 0 $ a.s. .
  \item $(B_{t_1} - B_{t_0}),\dots,(B_{t_n} - B_{t_{n_1}})$ are independet for $0=t_0<t_1<\dots<t_n$ and $n \in \mathbb{N}$.
  \item $B_t - B_s \sim B_{t-s}$, for $0 \le s \le t < \infty$.
  \item $B_t - B_s \sim \mathcal{N}(0, t-s)^{\otimes n}$.
  \item $B_t$ is continuous in $t$ a.s. .
\end{enumerate}
\end{definition}
A usual saying for $(ii)$ and $(iii)$ is the Brownian motion has independent, stationary increments. In (iv), $\mathrm{N}$ represent a random variable which has a normal distribution. $B_t$ is normal distributed due to (ii). It is clear that the increments of Brownian motion is stationary.

\begin{proposition}
  Let $(B_t)$ be a one-dimensional Brownian motion. Then the covarice of $B_m, B_n$ for $m, n \ge 0$ is $m \wedge n $.
\end{proposition}

\begin{proof}
  Without loss of generality, we assume that $m \ge n$, then
  \begin{eqnarray*}
	\mathrm{E}[B_mB_n] &=& \mathrm{E}[(B_m - B_n)B_n] + \mathrm{E}[B_n^2]\\
	&=& \mathrm{E}[B_m - B_n]\mathrm{E}[B_n] + n\\
	&=& n .
  \end{eqnarray*}
  \label{sec:cor}
\end{proof}

\begin{proposition}
  Let $(B_t)$ be a one-dimensional Brownian motion. Then $B_{cm} \sim c^{\frac{1}{2}}B_m$.
\end{proposition}

\begin{proof}
  Because $B_m$ is normal distributed for any $m > 0$, we then get
  \begin{eqnarray*}
	\mathrm{E} [e^{i\xi B_{cm}}] &=& e^{-\frac{1}{2}cm\xi^2}\\
	&=& e^{-\frac{1}{2}(c(m)^{\frac{1}{2}}\xi)^2}\\
	&=& \mathrm{E} [e^{i\xi c^{\frac{1}{2}}B_m}] .
  \end{eqnarray*}
\end{proof}

\begin{theorem}
  A one-dimensional Brownian motion is a Gaussian process.
\end{theorem}

\begin{proof}
  The following idea using the independence of increments to prove the claim come from \cite{shilling}.
  We choose $0=t_0<t_1<\dots<t_n$, for $n \in \mathbb{N}$. Define
  $V = (B_{t_1},\dots,B_{t_n})^T$,  $K = (B_{t_1}-B_{t_0},\dots, B_{t_n}-B_{t_{n-1}})^T$ and 
  $A = 
  \begin{pmatrix}
	1      & 0      & \cdots & 0\\
	1      & 1      & \cdots & 0\\
	\vdots & \vdots & \ddots & \vdots \\
	1      & 1      & \cdots & 1
  \end{pmatrix}
	$.
  Let us see the characteristic function of $V$,
  \begin{eqnarray*}
	\mathrm{E} [e^{i\xi^T V}] &=& \mathrm{E} [e^{i\xi^T AK}]\\ 
	&=& \mathrm{E} [e^{iA^T\xi K}]\\
	&=& \mathrm{E} [\exp(i (\xi^{(1)}+\dots+\xi^{(n)}, \xi^{(2)}+\dots+\xi^{(n)}, \cdots,\xi^{(n)})] \\
	&\cdot& (B_{t_1}-B_{t_0}, B_{t_2}-B_{t_1},\dots,B_{t_n}-B_{t_{n-1}})^T)\\
	&\overset{ind.increments}{=}& \prod_{j=1}^n \mathrm{E} [\exp(i(\xi^{(j)+\dots+\xi^{(n)}})(B_{t_j}-B_{t_{t-1}}))]\\
	&\overset{stat.increments}{=}& \prod_{j=1}^n \exp(-\frac{1}{2}(t_j - t_{j-1})(\xi^{(j)}+\dots+\xi^{(n)})^2) \\
	&=& \exp\left(-\frac{1}{2}\sum_{j=1}^n (t_j - t_{j-1})(\xi^{(j)}+\dots+\xi^{(n)})^2\right)\\
    &=& \exp\left(-\frac{1}{2}\left(\sum_{j=1}^n t_j(\xi^{(j)}+\dots+\xi^{(n)})^2 - \sum_{j=1}^n t_{j-1}(\xi^{(j)}+\dots+\xi^{(n)})^2\right)\right)\\
	&=& \exp\left(-\frac{1}{2}\left(\sum_{j=1}^{n-1} t_j((\xi^{(j)}+\dots+\xi^{(n)})^2 - (\xi^{(j+1)}+\dots+\xi^{(n)})^2) + t_n(\xi^{(n)})^2\right)\right)\\
	&=& \exp\left(-\frac{1}{2}\left(\sum_{j=1}^{n-1} t_j\xi^{(j)}(\xi^{(j)}+2\xi^{(j+1)}+\dots+2\xi^{(n)}) + t_n(\xi^{(n)})^2\right)\right)\\
	&=& \exp\left(-\frac{1}{2}\left(\sum_{j,h=1}^n(t_j\wedge t_h)\xi^{(j)}\xi^{(h)}\right)\right).
  \end{eqnarray*}
  Recall with proposition \ref{sec:cor}, $(t_j\wedge t_h)_{t,h=1,\dots,n}$ is the covariance matrix of $V$. The mean vector of it is zero, then we have been proved that the characteristic function is a form of some normal distributed random vector, i.e., $V$ is normal distributed.
\end{proof}

Shilling gave in his lecture \cite{shilling} the relationship between a one-dimensional Brownian motion and a n-dimensional Brownian motion.
In fact, $(B_t^{(l)})_{l=1,\dots,n}$ is Brownian motion if and only if $B_t^{(l)}$ is Brownian motion and all of the component are independent. Using this independence and the theorem of fubini in the characteristic function for high dimensional Brownian motion we can say a n-dimensional Brownian motion is also a Gaussian process.

\newpage

%-------- 3. section -----------%
\section{Stable Measure and Stable Integral}
\subsection{Stable Variable}
%%We consider now the one-dimensional Brownian motion. In this section we need some notations, which are defined as followings
%\begin{equation*}
%\Delta^{[0,T]} = \left\{t_1,\dots,t_n|0=t_0<\dots<t_n=T\right\} 
%\end{equation*}
%$$
%|\Delta^{[0,T]}| = \max_{t_j \in \Delta^{[0,T]}}|t_j - t_{j-1}|
%$$.

%\begin{lemma}
%  Let $B_t$ be a Brownian motion. Then
%  $$
%  \sum_{t_j \in \Delta^{[0,T]}} |B_{t_j} - B_{t_{j-1}}|^2 \xmapsto[L^2(\mathcal{P})]{|\Delta^{[0,T]}|\rightarrow 0} T  
%  $$
%\end{lemma}

\begin{definition}
  Let $X$ be a random variable. $X$ is said to be stable distributed, if there exist $0 < \gamma \le 2, \delta \ge 0, -1 \ge \kappa \ge 1, \theta \in \mathbb{R}$ such that its characteristic function can express as following
  \begin{equation}
	\mathrm{E} [\exp i\xi X] =  \begin{cases} \exp\{i \xi \theta - |\delta\xi|^\gamma(1-i\kappa(sgn(\xi)\tan \frac{\gamma\pi}{2}))\},\hspace{1em}  \text{if}\hspace{1em} \gamma \neq 1, \\
	    \exp\{i \xi \theta - |\delta\xi|(1+i\frac{2}{\pi}\kappa(sgn(\xi)\ln |\xi|))\},\hspace{1em} \text{if}\hspace{1em} \gamma = 1.
	  \end{cases}
	\label{sec:stbl}
  \end{equation}
 Where
$$
 sgn(x) = \begin{cases} 1,\hspace{1em} \text{if}\hspace{1em} x > 1\\
   0,\hspace{1em} \text{if}\hspace{1em} x = 0 \\
   -1,\hspace{1em}\text{if}\hspace{1em} x < 0.
 \end{cases}
  $$
\end{definition}

Notice, we write $\Lambda(\gamma, \kappa, \theta, \delta)$ as one random variable whose characteristic function equals (\ref{sec:stbl}).

\begin{theorem}
  $X \sim \Lambda(\gamma, \kappa, \theta, \delta)$, $X$ is Gaussian if and only if $\gamma = 2$. 
\end{theorem}

\begin{proof}
  
\end{proof}

\begin{definition}
  A random variable $X$ is said to be \emph{symmetric} if $X$ and $-X$ have the same distribution.
\end{definition}

\begin{proposition}
  Let $X$ be have stable distribution. $X$ is  \emph{symmetric} if and only if $X \sim \Lambda(\gamma, 0, 0, \delta)$. I.e. its characteristic function has the form
  \begin{equation}
	\mathrm{E}[\exp\{i \xi X\}] = \exp\{-|\delta\xi|^\gamma\}
  \end{equation}
\end{proposition}

\begin{proof}
  
\end{proof}


\subsection{Stable Random Measure}
In this section we suppose $(\mathrm{D}, \mathscr{D}, \mu)$ is a probability space, $\kappa : \Omega \rightarrow [-1, 1]$ is a measurable function.
\begin{definition}
	Let $\nu$ be a measure such that
	\begin{equation}
	  \nu : \mathscr{D} \rightarrow \mathscr{E}.\nonumber
	\end{equation}
	$\nu$ is said to be \emph{independently scattered}, if for any $D_1,\dots, D_n$ disjoint $\in \mathscr{D}$ that $\nu[D_1], \dots, \nu[D_n]$ are independent.
\end{definition}

For the next definition we need notations
\begin{equation}
  \mathscr{G} = \{D \in \mathscr{D} : \mu[D] < \infty \}.
\end{equation}
\begin{definition}
  Let $\nu$ be an independent cattered and $\sigma$-additive set function such that
\begin{equation*}
  \nu: \mathscr{G}  \rightarrow \mathrm{L}^{\infty}(\Omega, \mathscr{A}, \mathcal{P}). 
\end{equation*}
$\nu$ is said to be \emph{stable random measure} on $(D, \mathscr{D})$ with control measure $\mu$, degree $\gamma$ and skewness intensity $\kappa(\cdot)$ if 
\begin{equation}
  \nu[F] \sim \Lambda\brkt{\gamma, \frac{\int_F \kappa(x)\, \mu[\mathop{dx}]}{\mu[F]}, 0, (\mu[F])^{\frac{1}{\gamma}}}
\end{equation}
for $F \in \mathscr{D}$.
\end{definition}

\subsection{Stable Integral}
Samorodnitsky and Taqqu define a Integral with respect to stable measure as stochastic process in \cite{samorodnitsky}. The stable Integral is given as 
\begin{equation}
  \int_F f(x)\, \nu(\mathop{dx}).
  \label{sec:stbint}
\end{equation}
Where $f : F \rightarrow \mathbb{R}$ is a measurable function such that 
\begin{equation}
  \begin{cases} \int_F |f(x)|^\gamma \mu(\mathop{dx}) < \infty,\hspace{1em} \text{if} \hspace{1em} \gamma \in (0, 1) \cup (1, 2),\\
	\int_F |\kappa(x) f(x) \ln|f(x)||\mu(\mathop{dx}) < \infty,\hspace{1em} \text{if} \hspace{1em} \gamma = 1,
  \end{cases}
\end{equation}
, $\gamma, \mu, \kappa$ is, respectively, the degree, the control measure and the skewness intensity of the stable measure $\nu$.

Some propoties of the stable function are given by Samorodnitsky and Taqqu.

\begin{proposition}
  (Cf.\cite{samorodnitsky}, p.124) Let $J(f)$ be a stable integral as form of (\ref{sec:stbint}). Then 
  \begin{equation}
	J(f) \sim \Lambda(\gamma, \kappa, \theta, \delta)
  \end{equation}
for the degree, control measure, skewness intensity, respectively, 
\begin{eqnarray*}
\gamma &\in& (0, 2],\\
\kappa &=& \frac{\int_F \kappa(x) |f(x)|^\gamma\cdot sgn(f(x)) \mu(\mathop{dx})}{\int_F|f(x)|^\gamma\mu(\mathop{dx})},\\
\theta &=&
\begin{cases}
  0 , \hspace{1em} \text{if} \hspace{1em} \gamma \neq 1,\\
  -\frac{2}{\pi}\int_F \kappa(x) f(x) \ln|f(x)|\mu(\mathop{dx})
\end{cases},\\
\delta &=& \brkt{\int_F |f(x)^\gamma|\mu(\mathop{dx})}^{\frac{1}{\gamma}}
\end{eqnarray*}
of the stable measure $\nu$.
\end{proposition}
\begin{proposition}
(Cf.\cite{samorodnitsky}, p.117)  The stable integral is linear, in fact,
\begin{equation}
  J(c_1f_1 + c_2f_2) \overset{a.s.}{=}c_1J(f_1) + c_2J(f_2)
  \label{sec:stblin}
\end{equation}
for any $f_1, f_2$ integrable with respect to the stable measure and real numbers $c_1, c_2$. 
\end{proposition}
\newpage
%---------- 3. section ------------%
\section{Fractional Brownian Motion}
\setcounter{equation}{0}
The fractional Brownian motion(FBM) was defined by Kolmogorov primitively. After that Mandelbrot and Van Ness has present the work in detail. This section is concerned with the definition and some properties of it.

\subsection{Definition of Fractional Brownian Motion}
Mandelbrot and Van Ness \cite{mandelbrot} gave a integration presentation of the definion of FBM.
\begin{definition}
  Let $(U_H(t))_{t\ge 0}$ be a $\mathbb{R}$-valued stochatstic process an $H$ be such that $1<H<0$. $(U_H(t))$ is said to be \emph{fractional Brownian motion} if 
  \begin{eqnarray}
	U_H(t) - U_H(s) &=& \frac{1}{\Gamma(H+\frac{1}{2})}\brkt{\int_{\mathbb{R}} \mathbbm{1}_{\{t\ge u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u\le s\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}}
	\label{sec:frtbm}
  \end{eqnarray}
  for $t\ge s \ge 0$. Where $(B_u)$ is defined in sense of two-sides Brownian motion. The equation (\ref{sec:frtbm}) is well-defined. Sending $u$ to the limit  we will check the integrand is in $\mathcal{L}^2(\mathop{du})$ (cf.\cite{samorodnitsky}, page 321, Proposition 7.2.6).
\end{definition}

Indeed, setting $U_H(0) = 0$, the equation (\ref{sec:frtbm}) is equivalent to
	 \begin{eqnarray}
	   U_H(t) &=& \frac{1}{\Gamma(H+\frac{1}{2})}\brkt{\int_{\mathbb{R}} \mathbbm{1}_{\{t\ge u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u\le 0\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}}.
	\label{sec:fbm}
  \end{eqnarray}

%  We take (\ref{sec:fbm}) as definion of FBM now.
	
\begin{lemma}
  Let $(U_H(t))_{t\ge 0}$ be a FBM. Then $U_H(t)$ has a expected value $0$ and variance $t^{2H}\, \mathrm{E} U^2_H(1)$ for any $t \ge 0$.
  \label{sec:fbmp1}
\end{lemma}

\begin{proof}
  Firstly, we notice the integrand of FBM is deterministic function, hence it is $\sigma(U_H(u))$-measurable for all $u\ge 0$. Since $(B_u)$ is a martingal, FBM is then a martingal transformation with zero mean. \\
  Secondly we suppose that $t \ge s \ge 0,  c(H)=\frac{1}{(\Gamma(H+\frac{1}{2}))^2}$.
  \begin{eqnarray}
	\mathrm{E}[(U_H(t) - U_H(s))^2] &=& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{[\mathbbm{1}_{\{t\ge u\}}\cdot(t-u)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{s\ge u\}}\cdot(s-u)^{H-\frac{1}{2}}}^2 \,\mathop{du}]\nonumber\\
	&=& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{\mathbbm{1}_{\{t-s \ge u\}}\cdot(t-s-u)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{0\ge u\}}\cdot(-u)^{H-\frac{1}{2}}}^2 \,\mathop{du}]\nonumber\\
	&\overset{m=t-s}{=}& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{\mathbbm{1}_{\{m \ge u\}}\cdot(m-u)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{0\ge u\}}\cdot(-u)^{H-\frac{1}{2}}}^2 \,\mathop{du}]\nonumber\\
	&\overset{u=ml}{=}& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{\mathbbm{1}_{\{m \ge ml\}}\cdot(m-ml)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{0\ge ml\}}\cdot(-ml)^{H-\frac{1}{2}}}^2 m\cdot\,\mathop{dl}]\nonumber\\
	&=& c(H)\mathrm{E}[\int_{\mathbb{R}} \brkt{\mathbbm{1}_{\{1 \ge l \}}\cdot(1-l)^{H-\frac{1}{2}}  - \mathbbm{1}_{\{0\ge l\}}\cdot(-l)^{H-\frac{1}{2}}}^2 \cdot m^{2H-1}\cdot m\,\mathop{dl}]\nonumber\\
	&=& c(H)m^{2H}\mathrm{E}[U_H(1)^2]\nonumber\\
	&=& c(H)(t-s)^{2H}\mathrm{E}[U_H(1)^2]
	\label{sec:eqn1}
  \end{eqnarray}
  Using the same calculation, we get
  \begin{equation}
	\mathrm{E}[(U_H(t)^2] = c(H)t^{2H}\mathrm{E}[U_H(1)^2].
	\label{sec:eqn2}
  \end{equation}
%  $c(H), t^{2H}, \mathrm{E}[U_H(1)^2]$ are nonegative, then
%  $$\mathrm{E}[(U_H(t)] = c(H)^{\frac{1}{2}}t^{H}\mathrm{E}[U_H(1)].$$

%  Because of (\ref{sec:eqn1}),
%  $$
%	\mathrm{E}[(U_H(2) - U_H(1))^2] = c(H)\mathrm{E}[U_H(1)^2].
%  $$
%  And again
%  $$
%  \mathrm{E}[(U_H(2) - U_H(1))] = c(H)^{\frac{1}{2}}\mathrm{E}[U_H(1)]
%  $$
%Conserquently,
%\begin{eqnarray}
%	0 &=& \mathrm{E}[(U_H(2)(U_H(2)-2U_H(1)))]\nonumber\\
%	&\overset{Cauchy-Schwartz}{\le}& (\mathrm{E}[U_H(2)^2]\cdot\mathrm{E}[(U_H(2)-2U_H(1))^2])^{\frac{1}{2}}\nonumber
% \end{eqnarray}
% WLOG, let $\mathrm{E}[U_H(2)^2] \neq 0$, otherwise $\mathrm{E}[U_H(1)] = 0$ due to . Then we get 
%  $$
%  \mathrm{E}[(U_H(2)] = 2\mathrm{E}[(U_H(1)]
%  $$
%  \begin{eqnarray}
%	(1+c(H)^{\frac{1}{2}})\mathrm{E}[((U_H(1))] &=& \mathrm{E}[((U_H(2))] \nonumber\\
%	&=&  2\mathrm{E}[((U_H(1))]\nonumber
%  \end{eqnarray}
%  that means $\mathrm{E}[((U_H(1))] = 0$. Due to (\ref{sec:eqn2}), $\mathrm{E}[(U_H(t)] = 0$ for $t\ge 0$.
%  Therefore, (\ref{sec:eqn2}) is variance of $U_H(t)$.
  (\ref{sec:eqn2}) is vairance of $U_H(t)$ due to $\mathrm{E}[(U_H(t)] = 0$.
\end{proof}
To normalize the variance, a definition of standard FBM is given.

\begin{definition}
  A stochastic process $(U_H(t))_{t\ge 0}$ is said to be a \emph{standrad fractional Brownian motion}(sFBM) if
  \begin{equation}
U_H(t) = \hat{c}(H) \int_{\mathbb{R}} \mathbbm{1}_{\{t\ge u\}}\cdot (t-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u\le 0\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}.
\label{sec:eqn3}
\end{equation}
Where $\hat{c}(H) = \frac{1}{\mathrm{E}[U_H(1)^2]} $.
\end{definition}
We consider from now on sFBM as FBM.

\begin{theorem}
 Let  $(U_H(t))_{t\ge 0}$ be a FBM. The Covariance function of $U_H(t), U_H(s)$ is $ \frac{1}{2}(t^{2H} + s^{2H} - |t-s|^{2H})$ for $t, s \ge 0$.
\end{theorem}

\begin{proof}
  \begin{eqnarray}
	\mathrm{Cov}[U_H(t), U_H(s)] &=& \mathrm{E}[U_H(t)U_H(s)] \nonumber\\
	&=& \frac{1}{2}\brkt{\mathrm{E}[U_H(t)^2] + \mathrm{E}[U_H(s)^2] - \mathrm{E}[(U_H(t) - U_H(s))^2]} \nonumber\\
	&\overset{(\ref{sec:eqn2})}{=}& \frac{1}{2}(t^{2H} + s^{2H} - |t-s|^{2H})
	\label{sec:eqn4}
  \end{eqnarray}
\end{proof}

\begin{corollary}
  The FBM is a Gaussian process.
\end{corollary}

\begin{proof}
The covariance matrix is positive-defined due to the previous Theorem. The claim follows directly from Theorem ().
\end{proof}

\begin{corollary}
  Let $(U_H(t))_{t\ge 0}$ be a FBM, then $(U_H(t))_{t\ge 0}$ has stationary and H-self similar increments . 
\end{corollary}
\begin{proof}

 Assume that $s \ge u \ge 0 $. Because the joint distribution of $(U_H(s), U_H(u))^T$ is Gaussian, $(1, -1) \cdot (U_H(s), U_H(u))^T $ is Gaussian. In another word,  $U_H(s) - U_H(u) \sim \mathcal{N}(0, (s-u)^{2H})$ which is only dependent on $(s-u)$ and $(U_H(t))$ has therefore stationary increments.\\
   $(U_H(t))$ has zero mean and $\mathrm{Var[U_H(s)]} = s^{2H}\mathrm{Var}[U_H(1)]$ we get $U_H(s) \sim s^HU_H(1)$ due to it is Gaussian.
   To show FBM has H-similar increments, we have to prove\\ $(U_H(zt_1), U_H(zt_2),\dots, U_H(zt_n))) \sim (z^HU_H(t_1), z^HU_H(t_2),\dots, z^HU_(t_n))$ for any $z > 0$. Obviously, the former and the latter of the term are Gaussian and $\mathrm{Var}[U_H(zt_i), U_H(zt_j)] = \mathrm{Var}[z^HU_H(t_i), z^HU_H(t_j)] = \frac{1}{2}z^{2H}(t_i^{2H} + t_j^{2H} - |t_i-t_j|^{2H})$. Thus they have the same covariance matrix and zero mean. The claim is then proved.
\end{proof}

\subsection{Regularity}
\begin{theorem}[Kolmogorov Chentsov]
  A FBM $(U_H(t))_{t\ge 0}$ has almost surely continuous sample path.  
\end{theorem}

\begin{proof}
  Cf.\cite{mandelbrot}. Fix $\alpha$ such that $1 < \alpha H$. Let look at the expected value of $(U_H(t) - U_H(s))^\alpha$ using same calculation in (\ref{sec:eqn1})
  \begin{eqnarray}
	\mathrm{E} [(U_H(t) - U_H(s))^\alpha] &=& |t-s|^{\alpha H} \cdot \underbrace{\mathrm{E}\brkt{\int_{\mathbb{R}} \mathbbm{1}_{\{1\ge u\}}\cdot (1-u)^{H-\frac{1}{2}} - \mathbbm{1}_{\{u\le 0\}} (-u)^{H-\frac{1}{2}} \, \mathop{dB_u}}^\alpha}_{c(\alpha,H)}\nonumber\\
	&=& c(\alpha,H) \cdot |t-s|^{\alpha H}.
	\label{sec:eqn5}
  \end{eqnarray}
  We choose $\beta = \alpha -1$ and $\gamma \in (0, H-\frac{1}{\alpha})$ then the claim follows from Theorem .
\end{proof}

\begin{theorem}
  A FBM is almost surely not differentiable.
\end{theorem}

\begin{proof}

\end{proof}

\subsection{Fractional Gaussian Noise}
\newpage
%---------- 4. section ------------%
\section{Fractional Ornstein Uhlenbeck Process Model}
\setcounter{equation}{0}

\newpage

%---------- 5. Anwendung in die finanzmathe bzw. in der volatility process -------%
\section{Application in Financial Mathematics}

\newpage

%---------- x. conclusion ---------%
\section{Conclusion}

\newpage

%---------- reference -------------%
\addcontentsline{toc}{section}{References}
\fancyhead[LO, RE]{}
\begin{thebibliography}{99}
	\bibitem{bauer} \textsc{Bauer,~H.} (2002). Wahrscheinlichkeitstheorie(5th. durchges. und verb. Aufl.). Berlin: W. de Gruyter
	\bibitem{samorodnitsky} \textsc{Samorodnitsky, G., \& Taqqu, M.} (1994). Stable non-Gaussian random processes: Stochastic models with infinite variance. New York: Chapman \& Hall.
	\bibitem{mandelbrot} \textsc{Mandelbrot, B.B., \& van Ness, J.W.} (1968). Fractional Brownian motions, fractional noises and applications. SIAM Review 10 (4), 422–437.
	\bibitem{shilling} (2012, October 1). Stochastic Processes. Lecture conducted from \textsc{Shilling, R.}, Dresden.
\end{thebibliography}
\newpage
\end{document}


